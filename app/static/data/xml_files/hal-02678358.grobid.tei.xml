<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">French Contextualized Word-Embeddings with a sip of CaBeRnet: a New French Balanced Reference Corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Murielle</forename><surname>Popa-Fabre</surname></persName>
							<email>murielle.fabre@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH -Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">LLF -Université de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">Javier</forename><surname>Ortiz Suárez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH -Inria</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<addrLine>2 rue Simone Iff</addrLine>
									<postCode>75012</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
							<email>benoit.sagot@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH -Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>De La Clergerie</surname></persName>
							<email>eric.delaclergerie@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">ALMAnaCH -Inria</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">French Contextualized Word-Embeddings with a sip of CaBeRnet: a New French Balanced Reference Corpus</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF5758A96039C6E22DFB0A20BFAC62B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-23T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Balanced French Corpus</term>
					<term>Language Models</term>
					<term>French</term>
					<term>BERT</term>
					<term>ELMo</term>
					<term>Tagging</term>
					<term>Parsing</term>
					<term>NER</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes and compares the impact of different types and size of training corpora on language models like ELMO. By asking the fundamental question of quality versus quantity we evaluate four French corpora for training on parsing scores, POS-tagging and named-entities recognition downstream tasks. The paper studies the relevance of a new corpus, CaBeRnet, featuring a representative range of language usage, including a balanced variety of genres (oral transcriptions, newspapers, popular magazines, technical reports, fiction, academic texts), in oral and written styles. We hypothesize that a linguistically representative and balanced corpora will allow the language model to be more efficient and representative of a given language and therefore yield better evaluation scores on different evaluation sets and tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The question of quality versus the size of training corpora is increasingly gaining attention and interest in the context of the latest developments in neural language models' performance. The longstanding issue of corpora "representativeness" is here addressed, in order to grasp to what extent a linguistically balanced cross-genre language sample is sufficient for a language model to gain in accuracy for contextualized word-embeddings on different NLP tasks. Several increasingly larger corpora are nowadays compiled from the web, i.e. frWAC <ref type="bibr" target="#b4">(Baroni et al., 2009)</ref>, CCNet and OSCAR-fr <ref type="bibr" target="#b34">(Ortiz Suárez et al., 2019)</ref>, but does large size necessarily go along with better performance for language model training? Their alleged lack of representativeness has called for inventive ways of building a French balanced corpus offering new insights into language variation. Following <ref type="bibr">Biber (1993: 244)</ref>, "representativeness refers to the extent to which a sample includes the full range of variability in a population", we adopt a balanced approach in sampling a wide spectrum of features of language use and its cross-genre variability, be it situational (e.g. format, author, addressee, purposes, settings or topics) or linguistic, e.g. linked to distributional parameters like frequencies of word classes and genres. Thereby we contribute two newly built corpora. One purposed to be maximally representative of French language to yield good generalizations from, including a full range of language use variability, the French Balanced Reference Corpus -CaBeRnet. And a second that would yield a domain-specific language model training including both narrative material and oral language use, the French Children Book Test (CBT-fr). Based on the underlying assumption that a linguistically representative corpus would possibly generate wordembeddings, that while being more representative of real language use, would tentatively preform better in downstream tasks. This paper provides an evaluation-based investigation of how a linguistically balanced corpus can yield improvements in the performance of neural language models like ELMo <ref type="bibr" target="#b35">(Peters et al., 2018)</ref> in a given language. Specifically, we ask the contribution of oral language use in corpora, and therefore contrast a more domain-specific and written corpus like Wikipedia-fr with the newly built domain-specific CBT-fr corpus which additionally features oral style dialogues like the ones one can find in youth literature. To test for the effect of corpus size, we further compare to wide ranging corpora characterized by a variety of linguistic phenomena crawled from internet by ortizsuarez ortizsuarez, versus our newly built French Balanced Reference Corpus CaBeRnet, that features a wide and balanced coverage of cross-genre language use, including oral. All in all, our evaluation results confirm the effectiveness of large ElMo-based language models fine-tuned or pre-trained with a balanced and linguistically representative corpus, like CaBeRnetFRanc as opposed to domainspecific ones and extra-large and noisy ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure of the paper</head><p>The paper is organized as follows. Section 2. is dedicated to a descriptive overlook of corpus building and data collection. The construction process of our two newly brewed corpora CBT-fr and CaBeRnet is presented thoroughly in this section that summarises information details that can be found in corpus metadata. The achievement of linguistic balance in CaBeRnet is detailed in section 2.1. Statistics on the distribution of lexical, syntactic and morphological features of the different subparts of the corpus are also presented. In section 3. the focus is give to several quantitative measures to characterize the corpora under analysis : average length of sentences, type-token ratio and morphological richness. The characteristics of CBT-fr and CaBeRnet are compared to the other corpora under analysis (OSCAR-fr, Wiki-fr) are to be found in this section. Section 4. introduces the evaluation methods used to obtain the POS-tagging, NER and dependency Parsing results. Results are presented and discussed in Section 5.. Finally, we conclude in section 6. on the computational and linguistic relevance of fine-tuning obtained through balanced and representative corpora. We conclude by broadening the discussion with a series of future developments to enrich CaBeRnet and further investigate the benefits of smaller and noiseless corpora in neural NLP research. Resources associated to this paper encompass<ref type="foot" target="#foot_0">1</ref> : five version of FrELMo trained on the four corpora presented in this paper and two newly brewed corpora, including a French version of the balanced Corpus of Contemporary American English COCA <ref type="bibr" target="#b11">(Davies, 2008)</ref> and one of the Children Book Test CBT <ref type="bibr" target="#b22">(Hill et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Corpus Building</head><p>CaBeRnet corpus is meant to parallel COCA corpus<ref type="foot" target="#foot_1">2</ref> , which contains more than 560 million words of text (20 million words each year 1990-2017) and is equally divided among spoken, fiction, popular magazines, newspapers, and academic texts <ref type="bibr" target="#b11">(Davies, 2008)</ref>. A second reference guiding our approach and building method is one of the precursor and a classical balanced reference: the BNC <ref type="bibr" target="#b7">(Burnard, 2007)</ref>. In that it aims at covering a wide variety of genres, with the intention to be a representative sample of spoken and written language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Collection -CaBeRnet</head><p>CaBeRnet was obtained by compiling both existing datasets and web text from different sources (see Metadata -Lists), evenly divided (∼120 million words each) into spoken, fiction, magazine, newspaper, academic to achieve genre-balanced between oral and written modality in newspapers or popular written style, technical reports and Wikipedia entries, fiction, literature or academic written production).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">CaBeRnet-Oral Transcriptions</head><p>The oral sub-portion gathers both oral transcriptions (OR-FEO and Rhapsodie<ref type="foot" target="#foot_2">3</ref> ) and Films subtitles (Open Subtitles, www.opensubtitles.org/fr), pruned from diacritics and interlocutors tagging and time stamps. To these transcriptions, the French European Parliament <ref type="bibr">Proceedings (1996</ref><ref type="bibr">Proceedings ( -2011) )</ref> as presented in <ref type="bibr" target="#b25">Koehn (2005)</ref> contribute a sample of more complex oral style with longer sentences and richer vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">CaBeRnet-Popular Press</head><p>The whole sub-portion of Popular Press is gathered from an open data-set from the Est Républicain <ref type="bibr">(1999, 2002 and 2003</ref>), a regional press format<ref type="foot" target="#foot_3">4</ref> .It was selected to match popular style because it is characterized by simplified written press style and a wide range of every-day topics characterizing local regional press.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">CaBeRnet-Fiction &amp; Literature</head><p>The whole sub-portion of Fiction &amp; Literature was compiled from march 2019's Wikisource dump and extracted using WikiExtractor.py, a script that extracts and cleans text from a Wikipedia database dumps, by performing template expansion and preprocessing of template definitions (https://github.com/attardi/ wikiextractor).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">CaBeRnet-News</head><p>The News sub-portion builds upon web crawled elements, including Wikimedia's NewsComments and Wikinews reports from may 2019 Wikimedia dump, collected with a modified version of WikiExtractor.py. Newspaper's content gathered by the <ref type="bibr">Chambers-Rostand Corpus (i.e. Le Monde 2002</ref><ref type="bibr">-2003</ref><ref type="bibr">, La Dépèche 2002</ref><ref type="bibr">-2003</ref><ref type="bibr">, L'Humanité 2002</ref><ref type="bibr">-2003)</ref> and Le Monde diplomatique open-source corpus were assembled to represent a high register written news style from different political and thematic horizons. Several months of French Press Agency reports <ref type="bibr">(AFP, 2007</ref><ref type="bibr">(AFP, -2011</ref><ref type="bibr">(AFP, -2012) )</ref> competed with more simple and telegraphic style the newspaper written sample of the corpus.<ref type="foot" target="#foot_4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5.">CaBeRnet-Academic</head><p>The academic genre was also built from different sources including WikiBooks and Wikipedia dump for their thematic variety of highly specialized written production. ORFEO Corpus offered a small sample of academic writings like PHD dissertations and scientific articles encompassing a wide choice of disciplinary topics, and TALN Corpus<ref type="foot" target="#foot_5">6</ref> was included to represent more concise written style characterizing scientific abstracts and proceedings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Corpora Descriptive Comparison -Method</head><p>Two tokanization methods were used, the first was used for descriptive purposes because it technically allowed to segment and tokenize all corpora including OSCAR 23 billion words.</p><p>Hence, all corpora were entirely segmented into sentences and tokenized using SEM, Segmenteur-Étiqueteur Markovien standalone <ref type="bibr" target="#b14">Dupont (2017)</ref>. All corpora were then randomly shuffled by sentence to then were shuffled sake of select samples of 3 million words, that would allow to compare then in terms of lexical composition (Type-Token Ratio).</p><p>The second tokenization method was run only on the 3 million words samples (see Table <ref type="table" target="#tab_7">5</ref> to automatically tag them into part-of-speech and lemmatize them. For this purpose we used the TreeTagger.<ref type="foot" target="#foot_6">7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Corpora Size and Composition</head><p>Length of sentences is a simple measure to quantify both sentence syntactic complexity and genre. Hence, the average length of a sentences reported in Table <ref type="table" target="#tab_3">3</ref> shows interesting patterns of distributions across genres.  In our effort to evaluate the impact of corpora pre-training on ELMo-based contextualized word-embedding, we introduce here our two terms of comparison, namely the crawled corpus OSCAR-fr and the Wikipedia-fr one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">OSCAR fr</head><p>As it has been shown that pre-trained language models can be significantly improved by using more data <ref type="bibr">(Liu et al., 2019;</ref><ref type="bibr" target="#b41">Raffel et al., 2019)</ref>, we decided to include in our corpus comparison a corpus of French text extracted from Common Crawl<ref type="foot" target="#foot_7">8</ref> . Specifically, we leverage on a recently published corpus, OSCAR <ref type="bibr" target="#b34">(Ortiz Suárez et al., 2019)</ref>, which offers a pre-classified and pre-filtered version of the November 2018 Common Craw snapshot. OSCAR gathers a set of monolingual corpora extracted from Common Crawl, from the plain text WET format, where all HTML tags are removed and all text encodings are converted to UTF-8. It follows a similar approach to <ref type="bibr">(Grave et al., 2018a</ref>) by using a language classification model based on the fastText linear classifier <ref type="bibr" target="#b24">(Joulin et al., 2016;</ref><ref type="bibr" target="#b19">Grave et al., 2017)</ref> pre-trained on Wikipedia, Tatoeba and SETimes, supporting 176 different languages.</p><p>After language classification, a deduplication step is performed without introducing a specialised filtering scheme : paragraphs containing 100 or more UTF-8 encoded characters are kept. This makes OSCAR an example of unfiltered data that is nearly as noisy as to the original Crawled data.<ref type="foot" target="#foot_8">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">FrWIKI</head><p>This corpus collects a selection of pages from Wikipediafr from a dump executed in April 2019 where HTML tags and tables were removed, together with template expansion using Attardi's tool (WikiExtractor -GitHub, see 2.1.3.). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Corpora Lexical richness</head><p>Focusing on a useful measure of complexity that documents lexical richness or variety in vocabulary we present typetoken ration (TTR) of the corpora under analysis. Usually used to asses language use aspects like the variety of different words used to communicate by learners or children, it represents the total number of unique words (types/forms) divided by the total number of tokens in a given sample of language production. Hence, the closer the TTR ratio is to 1, the greater the lexical richness of the corpus. Table 4 summarises the lexical variety of the five sub-portions of CaBeRnet, respectively taken as representative of Oral, Popular, Fiction, News, and Academic genres. The domain diversity of academic texts is here evident. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Corpora Morphological richness</head><p>To select a measure that would help quantifying the different corpora morphological richness, we follow <ref type="bibr" target="#b6">(Bonami and Beniamine, 2015)</ref> and evaluated on randomly selected samples of 3 million words from each corpus under analysis the proportion of lemmas with multiple forms in a given vocabulary size, see Table <ref type="table" target="#tab_7">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Corpora Computational Evaluation tasks</head><p>This section reports the experiments designed to better understand the computational impact of the quality and linguistic balance versus size of ELMo's <ref type="bibr" target="#b35">(Peters et al., 2018)</ref> training corpora with the pre-training method ( §4.1.) and tasks described in 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ELMo Pre-traing and fine-tuning -Method</head><p>Two protocols were carried out to evaluate the impact of corpora characteristics on the tasks under analysis.  Methodologically, we seek to understand through a computational approach of fine-tuning with resources that are up to 30 times smaller than pre-training corpora has a observable impact on NLP tasks scores. It is namely for this reason, we selected ELMo which not only performs generally better on sequence tagging than other architectures, but is also better suited to pre-train on small corpora because of its inferior rage of parameter (93.6 million) compared to RoBERTa-base architecture used for CamBERT (BERTbase, 12,110 million -Transformer).</p><p>Embeddings from Language Models (ELMo) <ref type="bibr" target="#b35">(Peters et al., 2018</ref>) is a neurla Language Model, that is, a model that given a sequence of N input tokens, (t 1 , t 2 , ..., t N ), computes the probability of the sequence by modeling the probability of token t k given the history (t 1 , ..., t k-1 ):</p><formula xml:id="formula_0">p(t 1 , t 2 , . . . , t N ) = N k=1 p(t k | t 1 , t 2 , . . . , t k-1 ).</formula><p>ELMo in particular uses a biLM consisting of LSTM layers, that is, it concatenates both a forward and a backward language model generating a contextualized bi-directional representation of each token in a given sentence.</p><p>All the training experiments are performed with a fully trained model for 10 epochs. As is was done for the original English ELMo <ref type="bibr" target="#b35">(Peters et al., 2018)</ref>. Hence, all our FRrELMo-based language models build on top of the UD-Pipe Future parser and tagger <ref type="bibr" target="#b49">(Straka, 2018)</ref> as implemented in <ref type="bibr" target="#b48">Straka et al. (2019)</ref> which is open source and freely available.<ref type="foot" target="#foot_9">10</ref> </p><p>The UDPipe Future architecture is a multi-task model that predicts POS tags, lemmas and dependency trees jointly. It consists of an embedding step containing: character level word-embeddings that are trained along the rest of the network, pre-trained word-embeddings<ref type="foot" target="#foot_10">11</ref> , a randomly initialized word embeddings that are trained along the rest of the network, and contextualized word-embeddings for which we plug our customly trained ELMos. All these embeddings are then concatenated and are fed to two shared Bi-LSTMs that generate shared representations that are forwarded to two separate Bi-LSTMs; one that is followed by a softmax layer and predicts the POS tags, and another that is followed by a Deep Bi-Affine Attention Layer <ref type="bibr" target="#b13">(Dozat and Manning, 2017</ref>) that produces dependency trees.</p><p>In other words we add to UDPipe Future, five differently trained ELMo language model pre-trained on the qualitatively and quantitatively different corpora under comparison. Additionally, we also test the impact of the CaBeRnet Corpus on ELMo fine-tuning.</p><p>The LSTM-CRF is a model originally concived by Lample et al. ( <ref type="formula">2016</ref>) is just a Bi-LSTM pre-appended by both character level word embeddings and pre-trained word embeddings and pos-appended by a CRF decoder layer. For our experiments, we use the implementation of <ref type="bibr">(Straková et al., 2019)</ref> which is readily available<ref type="foot" target="#foot_11">12</ref> and it is designed to easily pre-append contextualized word embeddings to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Tasks -Method</head><p>We distinguish three main evaluation tasks that were performed by ELMo pre-trained on OSCAR (ELMo OSCAR ), frWIKI (ELMo Wikipedia ), CaBeRnet (ELMo CaBeRnet ) and CBT-fr (ELMo CBT ) and comparing them with ElMo pretrained on OSCAR and fine-tuned with CaBeRnet, i.e. ELMo OSCAR+CaBeRnet (see Results Table <ref type="table" target="#tab_11">7</ref>). The focus is given here on what is evaluated of the quality of contextualized word-embeddings obtained from different pre-training corpora under comparison. Crucially, manipulating the presence of oral transcriptions and oral proceeding will be interesting to understand the impact on accuracy of our language model and their impact on several language tasks after fine-tuning. Our development experiments compare the corpora presented in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic tasks</head><p>The evaluation tasks were selected to probe to what extent corpus "representativeness" and balance is impacting syntactic representations, in both (1) low-level syntactic relations in POS-tagging tasks, and</p><p>(2) higher level syntactic relations at constituent-and sentence-level thanks to dependency-parsing evaluation task. Namely, POS-tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency-parsing consists of higher order syntactic task like predicting the labeled syntactic tree capturing the syntactic relations between words.  <ref type="bibr" target="#b9">(Candito and Seddah, 2012;</ref><ref type="bibr" target="#b10">Candito et al., 2014)</ref> comprises more than 3000 sentences, from the French Europarl, the regional newspaper L'Est Républicain, the French Wikipedia and documents from the European Medicines Agency.</p><p>Spoken is a corpus converted automatically from the Rhapsodie treebank 14 <ref type="bibr" target="#b27">(Lacheret et al., 2014;</ref><ref type="bibr" target="#b5">Bawden et al., 2014)</ref> with manual corrections. It consists of 57 sound samples of spoken French with orthographic transcription and phonetic transcription aligned with sound (word boundaries, syllables, and phonemes), syntactic and prosodic annotations.</p><p>Finally, ParTUT is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety of text genres, including talks, legal texts, and Wikipedia articles, among others; ParTUT data is derived from the already-existing parallel treebank Par(allel)TUT <ref type="bibr" target="#b46">(Sanguinetti and Bosco, 2015)</ref> . Table <ref type="table" target="#tab_10">6</ref> contains a summary comparing the sizes of the treebanks 15 .</p><p>We evaluate the performance of our models using the standard UPOS accuracy for POS-tagging, and Unlabeled Attachment Score (UAS) and Labeled Attachment Score  (LAS) for dependency parsing. We assume gold tokenisation and gold word segmentation as provided in the UD treebanks.</p><p>State-of-the-art We compare our models to UDify (Kondratyuk, 2019). UDify is a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD languages including French for both POS-tagging and dependency parsing.</p><p>It is relevant to compare our results CamemBERT on those tasks because compared to UDify is the work that pushed the furthest the performance in fine-tuning endto-end a BERT-based model on downstream POS-tagging and dependency parsing. Finally, we compare our model to UDPipe Future <ref type="bibr" target="#b49">(Straka, 2018)</ref>, a model ranked 3rd in dependency parsing and 6th in POS-tagging during the CoNLL 2018 shared task <ref type="bibr" target="#b47">(Seker et al., 2018)</ref>. UDPipe Future provides us a strong baseline that does not make use of any pre-trained contextual embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Named Entity Recognition</head><p>Treebanks test data-set The benchmark data set from the French Treebank<ref type="foot" target="#foot_12">16</ref> (FTB) <ref type="bibr" target="#b0">(Abeillé et al., 2003)</ref> was selected in its 2008 versio, as introduced by Candito and Crabbé ( <ref type="formula">2009</ref>) and complemented with NER annotations by <ref type="bibr" target="#b42">Sagot et al. (2012)</ref>.</p><p>The NER-annotated FTB contains approximately than 12k sentences, and more than 350k tokens were extracted from articles of Le Monde newspaper <ref type="bibr">(1989 -1995)</ref>. As a whole, it encompasses 11,636 entity mentions distributed among 7 different types : 2025 mentions of "Person", 3761 of "Location", 2382 of "Organisation", 3357 of "Company", 67 of "Product", 15 of "POI" (Point of Interest) and 29 of "Fictional Character". The tree-bank, shows a large proportion of the entity mentions that are multi-word entities. We therefore report the three metrics that are commonly used to evaluate models: precision, recall, and F1 score. Specifically, (1) precision measures account for the percentage of entities found by the system that are correctly tagged, (2) recall measures sand for the percentage of named entities present in the corpus that are found, and (3) F1 score measure combines both precision and recall measures giving a global measure of a model's performance.</p><p>NER State-of-the-art Most of the advances in NER haven been achieved in English, particularly focusing on the CoNLL 2003 <ref type="bibr" target="#b43">(Sang and Meulder, 2003)</ref> and the Ontonotes v5 <ref type="bibr" target="#b37">(Pradhan et al., 2012;</ref><ref type="bibr" target="#b39">Pradhan et al., 2013)</ref> English corpora.</p><p>Importantly, NER task was traditionally tackled using Conditional Random Fields (CRF) <ref type="bibr" target="#b28">(Lafferty et al., 2001)</ref>, CRFs were later used as decoding layers for Bi-LSTM architectures <ref type="bibr" target="#b23">(Huang et al., 2015;</ref><ref type="bibr" target="#b29">Lample et al., 2016)</ref> showing considerable improvements over CRFs alone. Later, these Bi-LSTM-CRF architectures were enhanced with contextualised word-embeddings which yet again brought major improvements to the task <ref type="bibr" target="#b35">(Peters et al., 2018;</ref><ref type="bibr" target="#b1">Akbik et al., 2018)</ref>. Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures <ref type="bibr" target="#b12">(Devlin et al., 2019;</ref><ref type="bibr" target="#b3">Baevski et al., 2019)</ref>.</p><p>In non-English NER the CoNLL 2002 shared task included NER corpora for Spanish and Dutch corpora <ref type="bibr" target="#b45">(Sang, 2002)</ref> while the CoNLL 2003 included a German corpus <ref type="bibr" target="#b43">(Sang and Meulder, 2003)</ref>. Here the recent efforts of <ref type="bibr">(Straková et al., 2019)</ref> settled the state of the art for Spanish and Dutch, while <ref type="bibr" target="#b1">(Akbik et al., 2018)</ref> did it for German. In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the stable baselines settled by <ref type="bibr" target="#b15">(Dupont, 2018)</ref>, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pre-trained word-embeddings. And additional term of comparison was identified in a recently released state-of-the-art language model for French, CamemBERT, based on the RoBERTa architecture pretrained on the French sub-corpus of the newly available multilingual corpus OSCAR ( <ref type="bibr" target="#b31">(Martin et al., 2019)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results &amp; Discussion</head><p>5.1. Dependency Parsing and POS-tagging 5.1.1. ELMo CaBeRnet : Spoken a test for balance ELMo CaBeRnet offers representation that are not only competitive but sometimes better than Wikipedia especially considering that the majority of evaluation tree-banks are built on Wikipedia data. For Spoken ELMo CaBeRnet is reaching state-of-the-are results in POS-tagging on this oral specialized tree-bank (see dark gray highlight on Table <ref type="table" target="#tab_11">7</ref>. It performs better than CamemBERT which was the previous the state of the art on Spoken.</p><p>ELMo CaBeRnet shows a clear effect of balance when tested upon a purely spoken test-set like the Spoken tree-bank. Importantly, this effect is difficultly explainable by the size of oral style in CaBeRnet, because oral sub-part is only one fifth of the total. Furthermore, in this one fifth, only an even smaller amount words comes from pure oral transcripts which constitute the Spoken tree-bank. Namely, 67 444 words from the Rhapsodie corpus, and 575 894 words form ORFEO. We understand this result as a direct consequence of the fact that CaBeRnet contains a balanced amount of oral language use, which shows to pay off in POS-tagging. These results are extremely surprising especially given the fact that our evaluation method was fundamentally aiming at comparing the quality of word-embedding representations and not beating state-of-the-art results.  <ref type="table" target="#tab_11">7</ref>, we discover that not only balance, but also the broad and diverse genre converge of CaBeRnet may play a role in its POS-tagging success. Broad coverage possibly contributes to enhancing representations about oral language, in that a balanced sample may enhance the convergence of generalization about oral style from distinct genre that still imply oral like dialogues in fiction narratives. ELMo CBT also features oral dialogues in youth literature but does not show the same results because of the lack of variety of genres, thus demonstrating again the advantage of a comprehensive coverage of language use.</p><p>5.1.3. The effect of balance on ELMo OSCAR of CaBeRnet Fine-tuning Comparing ELMo OSCAR and ELMo OSCAR+CaBeRnet we can observe that for GSD and Sequoia fine-tuning OSCAR pretrained emdedding with CaBeRnet yields better representations, especially on UAS and LAS results. However, finetuning does not always yields better findings as one can observe in Spoken and ParTUT tree-banks, see Table <ref type="table" target="#tab_11">7</ref>. For POS-tagging in GSD and ParTUT the results of ELMo OSCAR and ELMo OSCAR+CaBeRnet are in second place position compared to ELMo Wikipedia, but are still ex-tremely close. As for parsing results, we can observe in Table <ref type="table" target="#tab_11">7</ref> a interesting pattern of results across treebanks highlighted in light gray. We see that for GSD and Sequoia the CaBeRnet finetuned version ELMo OSCAR+CaBeRnet compared to the pure Oscar pre-trained ELMo OSCAR is achieving higher scores. while the revers pattern is observable for the other two treebacks, namely Spoken and ParTUT. This configuration can be explained if we understand this pattern as due to the reinforcement and unlearning of ELMo OSCAR of some of its representations during the process of fine-tuning. Specifically, we can observe that parsing scores are better on treebaks that share the kind of language use represented in CaBeRnet, while they are worst on corpora that are closer in language sample to OSCAR corpus like Spoken and Par-TuT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">ELMo CBT : small but relevant contribution</head><p>ELMo CBT shows a very interesting pattern of results. Even if its results are under the baseline in GSD and Sequoia, it yields better results than the baseline for Spoken and Par-TUT. Given its reduced size, we were expecting it to overfit, which would explain an under baseline performance. However, this was not the case on Spoken and ParTUT treebanks. These results demonstrate ELMo CBT contribution in generating representations that are useful to UDPipe model to achieve better results in POS-tagging and dependency parsing tasks on the ParTUT treebank. The presence of oral dialogues is certainly playing a role in this pattern of results. This astonishing result calls for further investigation on the impact of pre-training wit reduced-size, noiseless and domain-specific corpora of the kind of CBT-fr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NER</head><p>For named entity recognition, our experiments show that LSTM-CRF+FastText+ELMo OSCAR+CaBeRnet achieves a better precision than the traditional CRF-based SEM architectures described above in Section 4.2.2. (CRF and Bi-LSTM+CRF) and CamemBERT, which is currently stateof-the-art. Importantly, LSTM-CRF + FastText + ELMo CaBeRnet reaches better results in finding entity mentions, than Wikipedia which is a highly specialized corpus in terms of vocabulary variety and size, as can be seen in the overwhelming total number of forms reported in Table <ref type="table" target="#tab_7">5</ref>. We can conclude that fine-tuning with CaBeRnet on ELMo OSCAR generates better word-embedding representations than Wikipedia in this task. Overall, NER scores shows improvements compared to CamemBERT. Fine-tuning with CaBeRnet has a second effect on recall, we understand this slight drop as possibly due to unlearning of the wide spectrum of vocabulary that is in OSCAR and not in CaBeRnet. For instance the whole french Wikipedia is included in OSCAR and not in CaBeRnet. Nonetheless, it has to be noted that these scores are still better than previous state-of-the-art. CBT-fr is under the baseline LSTM-CRF. This can possibly be explained because the corpus is very distant from FTB tree-bank (i.e. newspaper articles) in terms of topics and domain, or that the size of the corpus is too little to yield good-enough representation to perform entity mentions recognition. All in all, we showed that CaBeRnet corpus can reliably be used as a basis for training neural language models that perform in down-stream tasks, as well as suited for the creation of balanced lexical frequency-based dictionary entries, grammar studies, other language reference materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Perspectives &amp; Conclusion</head><p>The paper investigates the relevance of different types of corpora on ELMo pre-training and fine-tuning, and confirms the effectiveness of pre-trained language models with a balanced and linguistically representative corpus, like CaBeRnetFRanc, on several downstream tasks. By adding to UDPipe Future 5 differently trained ELMo language model that were pre-trained on qualitatively and quantitatively different corpora, our French Balanced Reference Corpus CaBeRnet shows on three different downstream tasks for French (POS-tagging, dependency parsing, named-entity recognition), achieves to improve the stateof-the-art for POS-tagging over previous monolingual and multilingual approaches. The proposed evaluation methods are showing that the two newly built corpora that we publish here are relevant for neural NLP and language modelling in French. Corpus balance shows to be a significant predictor of ELMo's accuracy on Spoken test data-set and for NER tasks. It goes without saying that a balanced corpus like CaBeRnet will be useful to calculate stable lexical frequency measures, like association measures and grant their comparability cross-linguistic comparability with English. The stability and representativeness probed through our experimental approach are key aspects that allow measures like Pointwise Mutual Information or DICE's Coefficient to be tested against psycho-linguistic and neuro-linguistic data as show in previous neuro-imaging studies <ref type="bibr" target="#b16">(Fabre et al., 2018;</ref><ref type="bibr" target="#b17">Fabre et al., 2019;</ref><ref type="bibr" target="#b18">Fabre et al., 2020)</ref> The results obtained for the parsing tasks on ParTUT open a new perspective for the development of the French Balanced Reference Corpus, involving the enhancement of the terminological coverage of CaBeRnet. A sixth sub-part could be included to cover technical domains like legal and medical ones, and thereby enlarge the specialized lexical coverage of CaBeRnet. Further development of this resource would additionally consider a further extension to cover user-generated content, ranging from well written blogs, tweets to more variable written productions like newspaper's comment or forums, as present in the CoMeRe corpus. 17 Results on the NER task show that size -usually presented as the more important factor to enhance the precision of representation of word-embeddings -matters less than linguistic representativeness, as achieved thorough balanced corpus building. ELMo CaBeRnet and ELMo OSCAR+CaBeRnet set new state-of-the art results that are superior than those obtained with a 30 times larger corpus, respectively on POS-tagging and NER. The computational experiments conducted here, namely, show that pre-training language models like ELMo on a very small sample like the French Children Book Test corpus (6 million words), or on a relatively small corpus like CaBeRnet yields unexpected results. This opens a perspective for languages that have a smaller training thesaurus : ELMo could a better suited language model for those languages than it is for others having larger size resources. In the same line, an additional perspective to this work is to better understand why we observe better NER scores with ELMo architecture than we do with BERT-base language model. In sum, this paper offers three main contributions: (1) two newly built corpora one French Balanced Reference Corpus and a second domain-specific corpus having both oral and written style, (2) five versions of FrELMo, and (3) a whole array of computational results that deepen our understanding on the effects of balance and register in NLP evaluation. To conclude, our current evaluations results show that linguistic quality in terms of representativeness and balance is yielding better performing contextualized wordembeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of number of words in the corpora under study.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of number of words in the corpora under study.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 3, this data-set is relatively large with its around 660 million words, sentences are relatively long compared to other corpora. It has the advantage of having a comparable size to CaBeRnet, but its homogeneity in terms of written genre is set to Wikipedia entries descriptive style.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">CORPUS SUB SET NB TOCKENS NB FORMS</cell><cell>TTR</cell></row><row><cell>Oral</cell><cell>122 864 888</cell><cell cols="2">291 744 0.0024</cell></row><row><cell>Popular</cell><cell>131 444 017</cell><cell cols="2">458 521 0.0035</cell></row><row><cell>News</cell><cell>132 708 943</cell><cell cols="2">462 971 0.0035</cell></row><row><cell>Fiction</cell><cell>198 343 802</cell><cell cols="2">983 195 0.0050</cell></row><row><cell>Academic</cell><cell>126 431 211</cell><cell cols="2">1 433 663 0.0113</cell></row><row><cell>Total</cell><cell>711 792 861</cell><cell cols="2">2 558 513 0.0036</cell></row></table><note><p>Comparison of proportion of Forms in 3 millions words samples from the different register represent by CaBeRnet partition into Oral, Popular, News, Fiction and Academic.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Lexical Statistics comparing morphological richness of the corpora under study.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Sizes in Number of tokens, words and phrases of the 4 treebanks used in the evaluations of POS-tagging and dependency parsing.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Final POS and dependency parsing scores of CamemBERT and mBERT (fine-tuned in the exact same conditions as CamemBERT), UDify as reported in the original paper on 4 French treebanks (French GSD, Spoken, Sequoia and ParTUT), reported on test sets (4 averaged runs) assuming gold tokenisation. Best scores in bold, second to best underlined, state-ofthe-art results in italics.</figDesc><table><row><cell></cell><cell></cell><cell>GSD</cell><cell></cell><cell></cell><cell>SEQUOIA</cell><cell></cell><cell></cell><cell>SPOKEN</cell><cell></cell><cell>PARTUT</cell></row><row><cell>MODEL</cell><cell cols="2">UPOS UAS</cell><cell>LAS</cell><cell cols="2">UPOS UAS</cell><cell>LAS</cell><cell cols="2">UPOS UAS</cell><cell>LAS</cell><cell>UPOS UAS</cell><cell>LAS</cell></row><row><cell>Baseline UDPipe Future</cell><cell>97.63</cell><cell cols="2">90.65 88.06</cell><cell>98.79</cell><cell cols="2">92.37 90.73</cell><cell>95.91</cell><cell cols="2">82.90 77.53</cell><cell>96.93</cell><cell>92.17 89.63</cell></row><row><cell>+ELMoCBT</cell><cell>97.49</cell><cell cols="2">90.21 87.37</cell><cell>98.40</cell><cell cols="2">92.18 90.56</cell><cell>96.60</cell><cell cols="2">85.05 79.82</cell><cell>97.27</cell><cell>92.55 90.44</cell></row><row><cell>+ELMoWikipedia</cell><cell>97.92</cell><cell cols="2">92.13 89.77</cell><cell>99.22</cell><cell cols="2">94.28 92.97</cell><cell>97.28</cell><cell cols="2">85.61 80.79</cell><cell>97.62</cell><cell>94.01 91.78</cell></row><row><cell>+ELMoCaBeRnet</cell><cell>97.76</cell><cell cols="2">91.91 89.49</cell><cell>99.27</cell><cell cols="2">94.65 93.40</cell><cell>97.32</cell><cell cols="2">85.63 80.61</cell><cell>97.58</cell><cell>94.24 91.90</cell></row><row><cell>+ELMoOSCAR</cell><cell>97.85</cell><cell cols="2">92.41 90.05</cell><cell>99.30</cell><cell cols="2">94.43 93.25</cell><cell>97.10</cell><cell cols="2">85.83 80.94</cell><cell>97.47</cell><cell>94.74 92.55</cell></row><row><cell>+ELMoOSCAR+CaBeRnet</cell><cell>97.88</cell><cell>92.67</cell><cell>90.34</cell><cell>99.26</cell><cell cols="2">94.75 93.54</cell><cell>97.22</cell><cell cols="2">85.77 80.80</cell><cell>97.50</cell><cell>94.66 92.43</cell></row><row><cell>State-of-the-art</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UDify</cell><cell>97.83</cell><cell cols="2">93.60 91.45</cell><cell>97.89</cell><cell cols="2">92.53 90.05</cell><cell>96.23</cell><cell cols="2">85.24 80.01</cell><cell>96.12</cell><cell>90.55 88.06</cell></row><row><cell>UDPipe Future + mBERT</cell><cell>97.98</cell><cell cols="2">92.55 90.31</cell><cell>99.32</cell><cell cols="2">94.88 93.81</cell><cell>97.23</cell><cell cols="2">86.27 81.40</cell><cell>97.64</cell><cell>94.51 92.47</cell></row><row><cell>CamemBERT</cell><cell>98.19</cell><cell cols="2">94.82 92.47</cell><cell>99.21</cell><cell cols="2">95.56 94.39</cell><cell>96.68</cell><cell cols="2">86.05 80.07</cell><cell>97.63</cell><cell>95.21 92.90</cell></row><row><cell cols="3">NER -RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SEM (CRF) (Dupont, 2018)</cell><cell>87.89</cell><cell cols="2">82.34 85.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LSTM-CRF (Dupont, 2018)</cell><cell>87.23</cell><cell cols="2">83.96 85.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM-CRF</cell><cell></cell><cell>85.87</cell><cell cols="2">81.35 83.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+FastText</cell><cell></cell><cell>88.53</cell><cell cols="2">84.63 86.53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+FastText+ELMo CBT</cell><cell></cell><cell>79.77</cell><cell cols="2">77.63 78.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+FastText+ELMo Wikipedia</cell><cell></cell><cell>88.87</cell><cell cols="2">87.56 88.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+FastText+ELMo CaBeRnet</cell><cell></cell><cell>88.82</cell><cell cols="2">87.82 88.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+FastText+ELMo OSCAR</cell><cell></cell><cell>88.89</cell><cell cols="2">88.43 88.66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+FastText+ELMo OSCAR+CaBeRnet</cell><cell>88.93</cell><cell cols="2">88.08 88.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CamemBERT</cell><cell></cell><cell>88.35</cell><cell cols="2">87.46 87.93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Results for NER on the FTB. Best scores in bold, second to best underlined.</figDesc><table><row><cell>5.1.2. ELMo CaBeRnet : a test for coverage</cell></row><row><cell>From Table</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The link to the corpus and FrElMos will be available upon acceptance of the paper. Following the link the reader will have access to a dedicated website cabernet-corpus.fr where raw text version and metadata for each sub-part of the corpus are also be available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.corpusdata.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>ORFEO www.cocoon.huma-num.fr/exist/crdo/; Rhapsodie www.projet-rhapsodie.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>www.cnrtl.fr/corpus/estrepublicain/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>At the time being, this part of CaBeRnet corpus is still subject to Licence restrictions. This restricted amount of AFP news reports can reasonably fall in the public domain.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>  6  This corpus of proceedings builds on a subset of scientific articles presented at two conferences between 2007 to 2013, namely TALN (Traitement Automatique des Langues Naturelles) and RECITAL (Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues). It consists of 586 articles for a total of about 2 million words.redac. univ-tlse2.fr/corpus/taln_en.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Based on the following tag-set https://www.cis. uni-muenchen.de/ ˜schmid/tools/TreeTagger/ data/french-tagset.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://commoncrawl.org/about/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>We did not use CCNet because of its difficult availability and download.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://github.com/CoNLL-UD-2018/UDPipe-Future</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>We use the French fastText embeddings distributed by<ref type="bibr" target="#b21">(Grave et al., 2018b)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>https://github.com/ufal/acl2019_nested_</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12"><p>This data-set has only been stored and used on Inria's servers after signing the research-only agreement.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We acknowledge <rs type="person">Benoit Crabbé</rs> for his helpful suggestions at the beginning of reflection on balanced corpora. We are indebted to <rs type="person">Yoann Dupont</rs> for his help in collecting data from Wikimedia dumps and for his critical comments. <rs type="person">Olivier Bonami</rs> and <rs type="person">Kim Gerdes</rs> conversations were instrumental.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Building a Treebank for French</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abeillé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toussenel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="165" to="187" />
		</imprint>
	</monogr>
	<note>Dordrecht</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<ptr target="https://repository.ortolang.fr/api/content/comere/v2/comere.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, COL-ING 2018</title>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</editor>
		<meeting>the 27th International Conference on Computational Linguistics, COL-ING 2018<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08-20">2018. August 20-26, 2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Proceedings of the Joint Conference JEP-TALN-RECITAL 2012</title>
		<editor>Georges Antoniadis, et al.</editor>
		<imprint>
			<date type="published" when="2012-06-04">2012. June 4-8, 2012</date>
			<publisher>ATALA/AFCP</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>TALN, Grenoble, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>CoRR, abs/1903.07785</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The wacky wide web: A collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009-09">2009. 09</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Correcting and validating syntactic dependency in the spoken French treebank rhapsodie</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Botalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2014-05">2014. May</date>
			<biblScope unit="page" from="2320" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Implicative structure and joint predictiveness</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bonami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beniamine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">520 million words, 1990-present</title>
		<author>
			<persName><forename type="first">L</forename><surname>Burnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British National Corpus, version 3 -BNC XML Edition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving generative statistical parsing with semi-supervised word clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT&apos;09</title>
		<meeting>of IWPT&apos;09<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Le corpus sequoia : annotation syntaxique et exploitation pour l&apos;adaptation d&apos;analyseur par pont lexical (the sequoia corpus : Syntactic annotation and use for a parser lexical domain adaptation method)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<editor>Antoniadis et al.</editor>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="321" to="334" />
		</imprint>
	</monogr>
	<note>in french</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep syntax annotation of the sequoia french treebank</title>
		<author>
			<persName><forename type="first">M</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Perrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ribeyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><forename type="middle">V</forename><surname>De La Clergerie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014</title>
		<editor>
			<persName><surname>Nicoletta Calzolari</surname></persName>
		</editor>
		<meeting>the Ninth International Conference on Language Resources and Evaluation, LREC 2014<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2014-05-26">2014. May 26-31, 2014</date>
			<biblScope unit="page" from="2298" to="2305" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">520 million words, 1990-present</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Corpus of Contemporary American English (COCA)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploration de traits pour la reconnaissance d&apos;entités nommées du franc ¸ais par apprentissage automatique</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conférence sur le Traitement Automatique des Langues Naturelles (TALN)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploration de traits pour la reconnaissance d&apos;entit&apos;es nomm&apos;ees du franc ¸ais par apprentissage automatique</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24e Conf&apos;erence sur le Traitement Automatique des Langues Naturelles (TALN)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Processing mwes: Neurocognitive bases of verbal mwes and lexical cohesiveness within mwes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattasali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Workshop on Multiword Expressions (COLING 2018)</title>
		<meeting>the 14th Workshop on Multiword Expressions (COLING 2018)<address><addrLine>Santa Fe, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Localising memory retrieval and syntactic composition: an fmri study of naturalistic language comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattasali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Luh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Saied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pallier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Spreng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language, Cognition and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="510" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling conventionalization and predictability in multiword expressions at the brain level</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattasali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pallier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Society for Computation in Linguistics (SCiL 2020)</title>
		<meeting>the Society for Computation in Linguistics (SCiL 2020)<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017</title>
		<editor>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</editor>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04-03">2017. April 3-7, 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018</title>
		<editor>
			<persName><surname>Nicoletta Calzolari</surname></persName>
		</editor>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC 2018<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2018-05-07">2018. May 7-12, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Language Resources and Evaluation Conference</title>
		<meeting>the 11th Language Resources and Evaluation Conference<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resource Association</publisher>
			<date type="published" when="2018-05">2018b. May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fasttext.zip: Compressing text classification models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR, abs/1612.03651</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings: the tenth Machine Translation Summit</title>
		<meeting><address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>AAMT, AAMT</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">75 languages, 1 model: Parsing universal dependencies universally</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kondratyuk</surname></persName>
		</author>
		<idno>CoRR, abs/1904.02099</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rhapsodie: a prosodic-syntactic treebank for spoken French</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lacheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beliao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Obin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pietrandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tchobanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2014-05">2014. May</date>
			<biblScope unit="page" from="295" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001)</title>
		<editor>
			<persName><forename type="first">Carla</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</editor>
		<meeting>the Eighteenth International Conference on Machine Learning (ICML 2001)<address><addrLine>Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Williams College</publisher>
			<date type="published" when="2001-06-28">2001. June 28 -July 1, 2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Villemonte De La Clergerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03894</idno>
		<title level="m">CamemBERT: a Tasty French Language Model</title>
		<imprint>
			<date type="published" when="2019-11">2019. Nov</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Quirmbach-Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bedini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertomeu Castelló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08">2013. August</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ž</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arutie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Augustinus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Barbu Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bellato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Biagetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blokland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bobicev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Börstell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cebiroglu Eryigit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G A</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>De Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Diaz De Ilarraza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dirix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dobrovoljc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Droganova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elkahky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ephrem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Etienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez Alcalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gajdošová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Galbraith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gärdenfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gökırmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gómez Guinovart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gonzáles Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grūzītis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guillot-Barbance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hà Mỹ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hladká</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hlaváčová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hociung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)</title>
		<editor>
			<persName><forename type="first">Piotr</forename><surname>Bański</surname></persName>
		</editor>
		<meeting><address><addrLine>Cardiff, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Leibniz-Institut für Deutsche Sprache</publisher>
			<date type="published" when="2019-07">2019. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<editor>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1104.2086</idno>
		<title level="m">A universal part-of-speech tagset</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning -Proceedings of the Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes</title>
		<author>
			<persName><surname>Sameer Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL 2012</title>
		<meeting><address><addrLine>Jeju Island, Korea; Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-13">July 13, 2012</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL 2013</title>
		<editor>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning, CoNLL 2013<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-08">2013. August 8-9, 2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">ACL</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified textto-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Annotation référentielle du corpus arboré de Paris 7 en entités nommées (referential named entity annotation of the paris 7 french treebank)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stern</surname></persName>
		</author>
		<editor>Antoniadis et al. (Antoniadis et al.</editor>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
	<note>in french</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F T K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003</title>
		<editor>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</editor>
		<meeting>the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05-31">2003. May 31 -June 1, 2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">ACL</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F T K</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Natural Language Learning, CoNLL 2002, Held in cooperation with COLING 2002</title>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</editor>
		<meeting>the 6th Conference on Natural Language Learning, CoNLL 2002, Held in cooperation with COLING 2002<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Harmonization and Development of Resources and Tools for Italian Natural Language Processing within the PARLI Project</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Computational Intelligence</title>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">589</biblScope>
			<biblScope unit="page" from="51" to="69" />
		</imprint>
	</monogr>
	<note>PartTUT: The Turin University Parallel Treebank</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Universal morpho-syntactic parsing and the contribution of lexica: Analyzing the onlp lab submission to the conll 2018 shared task</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Evaluating contextualized embeddings on 54 languages in POS tagging, lemmatization and dependency parsing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
		<idno>CoRR, abs/1908.07448</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">UDPipe 2.0 prototype at CoNLL 2018 UD shared task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">2018. October</date>
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
