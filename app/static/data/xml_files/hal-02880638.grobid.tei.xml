<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A detailed study of the distributed rough set based locality sensitive hashing feature selection technique</title>
				<funder ref="#_kKsR6vn">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zaineb</forename><forename type="middle">Chelly</forename><surname>Dagdia</surname></persName>
							<email>zaineb.chelly-dagdia@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Zarges</surname></persName>
							<email>c.zarges@aber.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">LARODEC</orgName>
								<orgName type="institution">Aberystywth University</orgName>
								<address>
									<settlement>Aberystwyth</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Institut Supérieur de Gestion de Tunis</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aberystywth University</orgName>
								<address>
									<settlement>Aberystwyth</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">INRIA Nancy -Grand</orgName>
								<address>
									<addrLine>Est 615 Rue du Jardin Botanique</addrLine>
									<postCode>54600</postCode>
									<settlement>Villers-lès-Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A detailed study of the distributed rough set based locality sensitive hashing feature selection technique</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F74CEAE9130EDAE917D1F024D753CD71</idno>
					<idno type="DOI">10.3233/FI-2016-0000</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-23T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Granular Computing</term>
					<term>Rough Set Theory</term>
					<term>Big Data</term>
					<term>Feature Selection</term>
					<term>Locality Sensitive Hashing</term>
					<term>Distributed Processing nodes: 51.59557 2 nodes: 95.21888 4 nodes: 34.44596 4 nodes: 21.82497 4 nodes: 56.27094 8 nodes: 36.65276 8 nodes: 15.59631 8 nodes: 52 nodes: 98.40898 8 nodes: 66.70733 8 nodes: 20.38688 8 nodes: 87 nodes: 37.38687 4 nodes: 70.01903 8 nodes: 31.49336 8 nodes: 20.16216 8 nodes: 51 nodes: 21.72356 4 nodes: 35.68894 4 nodes: 57.4125 8 nodes: 23.83919 8 nodes: 23.53858 8 nodes: 47.37777 16 nodes: 23.56475 16 nodes: 21.80079 16 nodes: 45.36554</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the context of big data, granular computing has recently been implemented by some mathematical tools, especially Rough Set Theory (RST). As a key topic of rough set theory, feature selection has been investigated to adapt the related granular concepts of RST to deal with large amounts of data, leading to the development of the distributed RST version. However, despite of its scalability, the distributed RST version faces a key challenge tied to the partitioning of the feature search space in the distributed environment while guaranteeing data dependency. Therefore, in this manuscript, we propose a new distributed RST version based on Locality Sensitive Hashing (LSH), named LSH-dRST, for big data feature selection. LSH-dRST uses LSH to match similar features into the same bucket and maps the generated buckets into partitions to enable the splitting of the universe in a more efficient way. More precisely, in this paper, we perform a detailed analysis of the performance of LSH-dRST by comparing it to the standard distributed RST version, which is based on a random partitioning of the universe. We demonstrate that our LSH-dRST is scalable when dealing with large amounts of data. We also demonstrate</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Granular computing <ref type="bibr" target="#b10">[1]</ref> is a concept covering the family of theories, methodologies, tools, and techniques that use the notion of "granules". Granules can be in the form of groups, sets, classes, or clusters of a universe, which are used in the process of problem solving <ref type="bibr" target="#b11">[2,</ref><ref type="bibr" target="#b12">3,</ref><ref type="bibr" target="#b13">4]</ref>. These granules are drawn together by indistinguishability, equivalence, similarity, proximity or functionality <ref type="bibr" target="#b14">[5]</ref>. In the literature, the basic notion of granular computing, i.e., granules, has appeared in various research approaches among these we mention fuzzy set theory, rough set theory, belief function theory (also called Dempster-Shafer theory), quantization, and many other mathematical approaches <ref type="bibr" target="#b14">[5]</ref>. Recently, granular computing has received more attention by researchers from a very different perspective: the challenging big data context. Today, there is a fast growing interest in the study of granular computing in the context of big data, and this manuscript is embedded in this context.</p><p>Within the context of big data, granular computing has begun to play important roles in many application domains such as in big data processing <ref type="bibr" target="#b15">[6]</ref> where the theory of fuzzy sets was applied to offer a novel promising processing environment, in sentiment analysis where fuzzy set theory was applied on big social data <ref type="bibr" target="#b16">[7]</ref>, in knowledge acquisition where rough set theory was applied <ref type="bibr" target="#b17">[8]</ref>, in epidemiology where rough set theory was applied as a big data mining technique <ref type="bibr" target="#b18">[9]</ref>, in mammography mass classification where rough set theory was applied to analyse deep and hand-crafted features <ref type="bibr" target="#b19">[10]</ref>, etc.</p><p>For these example application domains and many others, there is always the same big challenge in granular computing, which is the task of performing big data pre-processing, specifically feature selection. In this context, the theory of rough sets has recently been adapted to the big data context and successfully been applied as a scalable and effective feature selection technique within a distributed environment <ref type="bibr" target="#b20">[11,</ref><ref type="bibr" target="#b21">12]</ref>. This success is due to many characteristics of the theory, among these the capability of the theory to analyze the facts hidden in data, its independence from the user or expert knowledge as it does not require any supplementary information about the given data, and its ability to find a minimal knowledge representation <ref type="bibr" target="#b22">[13]</ref>. These characteristics are gained from the use of the RST granularity structure.</p><p>By adapting the basic granular concepts of RST to deal with large amounts of data, a first scalable version of RST within a distributed environment, called Sp-RST, was proposed in <ref type="bibr" target="#b20">[11]</ref>. Sp-RST, also detailed in <ref type="bibr" target="#b23">[14]</ref>, was proposed to avoid the prohibitive complexity of the standard RST, i.e., the sequential/classical RST version, which is caused by the search for an optimal feature subset through the competing of an exponential number of candidate subsets; an exhaustive search, which becomes impractical for big data as it becomes unmanageable to generate the set of all possible feature combinations. In this context, it is important to mention that in the literature there are several RST heuristics and other RST based methods and algorithms <ref type="bibr" target="#b24">[15,</ref><ref type="bibr" target="#b25">16]</ref> that can find a minimal reductwhich is not globally minimal -in a much faster way. Despite the fact that these approaches avoid such an exhaustive search performed by the classical RST version, the problem of finding a globally minimal reduct remains a big challenge mainly in the context of big data, and specifically if the application domain requires it. For instance, in many real-world applications, specifically in health or medical research, it is essential to have a globally minimal reduct to better guide the decision maker and facilitate their task as revealed in <ref type="bibr" target="#b18">[9,</ref><ref type="bibr" target="#b19">10]</ref>. In this context, it is important to mention that in our research study and in line with our manuscript's goals, we mainly focus on the generation of a globally minimal reduct without calling for heuristics, which makes our task more challenging in the context of big data, and hence we aim to revise Sp-RST. It is important to highlight that the generation of the globally minimal reduct will be performed with respect to the partitioning process, i.e., with respect to the generated smaller data sets that will result from the data partitioning process and where data dependency is guaranteed.</p><p>Technically, to perform feature selection in the context of big data, Sp-RST partitions the feature search space in a random manner so that every single partition can hold a random set of attributes. Each of the generated partitions is dealt with in a separate way, i.e., independently in the distributed environment so that at the end of the feature selection process all the selected attributes from every single partition are assembled together to generate the ultimate reduced set of features. However, it can be noticed that in such implementation design, it is very likely that similar attributes will appear in different partitions and hence a cut in data dependency will occur. It is crucial to point out that data dependency is a significant matter in a distributed environment and in parallel computing; and hence considered as a challenge. Based on the defined Sp-RST architecture <ref type="bibr" target="#b20">[11]</ref>, data dependency will not be guaranteed as the algorithm uses a randomized procedure when partitioning the feature search space. Hence, in this manuscript, we propose to revise the Sp-RST architecture and develop instead a novel efficient distributed algorithm, which is based on the distributed granular concepts of rough set theory and adopts a hashing based technique. Specifically, our proposed solution is based on the Locality Sensitive Hashing (LSH) algorithm <ref type="bibr" target="#b26">[17]</ref> for large-scale data pre-processing.</p><p>The main motivation behind the choice of LSH among other hashing techniques proposed in the literature <ref type="bibr" target="#b26">[17,</ref><ref type="bibr" target="#b27">18,</ref><ref type="bibr" target="#b28">19]</ref> is that LSH is often taken as a baseline. But most importantly, the algorithm is widely used in industry, e.g., for image recognition, clustering, and some other tasks, but specifically in database systems for high dimensional similarity search. The choice of LSH is also based on several advantages and characteristics of the algorithm in comparison to other hashing techniques when dealing with high dimensional data sets as demonstrated in <ref type="bibr" target="#b29">[20]</ref>.</p><p>Our proposed solution, dubbed LSH-dRST, adopts LSH, which maps similar data instances based on their feature values into the same bucket in low dimensional cases. Based on this process, LSH-dRST uses the generated buckets to partition the feature search space in a more reliable way, hence better preserving data dependency and a lower computational cost as will be demonstrated in Section 7. Please, note that a preliminary version of LSH-dRST was presented in <ref type="bibr" target="#b30">[21]</ref>. In this paper, we will conduct a more detailed study of the work presented in <ref type="bibr" target="#b30">[21]</ref> and a deeper analysis of its performance as a distributed feature selection technique. Based on the shortcomings of <ref type="bibr" target="#b20">[11]</ref> and the limitations of <ref type="bibr" target="#b30">[21]</ref> as it presents a preliminary version not thoroughly studied, a summary of the main contributions of this manuscript are presented as follows:</p><p>• Revise the work presented in <ref type="bibr" target="#b20">[11]</ref> by developing a novel efficient distributed RST algorithm that adopts a hashing technique to better preserve data dependency in the distributed environment, specifically when it comes to partitioning the universe.</p><p>• Execute a thorough analysis of the work presented in <ref type="bibr" target="#b30">[21]</ref> by providing (1) a detailed explanation of LSH-dRST in terms of its architecture, its algorithmic design, and its functioning as a distributed algorithm, and (2) by making a detailed analysis of the performance of LSH-dRST by studying the number of features selected and the runtime performance in terms of the different parts of the algorithms as well as its speedup, sizeup and scaleup. Additionally, we present a more detailed model evaluation by additionally considering a simply Naive Bayes classifier (in addition to the random forest classifier originally used).</p><p>• Demonstrate that LSH-dRST is not only scalable but also more reliable for feature selection, which will show that it is more relevant to big data pre-processing.</p><p>• Demonstrate that LSH-dRST performs the partitioning of the high dimensional feature search space in a more reliable way. This is to show that it better preserves data dependency in the distributed environment, and ensures a lower computational cost.</p><p>The rest of this paper is structured as follows. Section 2 presents a review of attribute selection techniques in the context of big data. Section 3 presents preliminaries for Locality Sensitive Hashing and Rough Set Theory for feature selection. Section 4 presents a description of parallel computing frameworks as well as a description of the MapReduce programming model. Section 5 formalizes the motivation of this work and introduces our novel distributed LSH-dRST algorithm for large-scale data pre-processing. The experimental setup is introduced in Section 6. The results of the performance analysis are given in Section 7 and the work is concluded in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review of Feature Selection Techniques in the Context of Big Data</head><p>In the literature, several feature selection techniques for non-high dimensional environments have been proposed, which were initially developed in a sequential way <ref type="bibr" target="#b31">[22,</ref><ref type="bibr" target="#b32">23]</ref>. However, their scalability was questioned in the context of big data. In this context, in <ref type="bibr" target="#b33">[24]</ref>, a detailed study was conducted where authors performed a deep analysis of the scalability of the state-of-the-art feature selection techniques in all their three categories: filter techniques, i.e., approaches, which are independent from any specific induction algorithm, the embedded techniques, i.e., approaches that fuse feature selection and the learning approach into a single process, and the wrapper techniques, i.e., approaches involving a specific learning algorithm when evaluating the attribute subset.</p><p>In <ref type="bibr" target="#b33">[24]</ref>, it was demonstrated that the state-of-the-art feature selection techniques would obviously have scalability issues when dealing with big data. Authors have proved that the existent techniques will be inadequate to handle a high number of attributes in terms of training time and/or effectiveness in selecting the relevant set of features. Therefore, the adaptation of feature selection techniques for big data problems seems essential and it may require the redesign of these algorithms and their incorporation in parallel and distributed environments/frameworks. Among the possible alternatives is the MapReduce paradigm <ref type="bibr" target="#b34">[25]</ref>, which was introduced by Google and offers a robust and efficient framework to deal with big data analysis. Several recent works have been concentrated on parallelizing and distributing machine learning techniques using the MapReduce paradigm <ref type="bibr" target="#b35">[26,</ref><ref type="bibr" target="#b36">27,</ref><ref type="bibr" target="#b37">28]</ref>. Recently, a set of new and more flexible paradigms have been proposed aiming at extending the standard MapReduce approach, mainly Apache Spark<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b38">[29]</ref>, which has been applied with success over a number of data mining and machine learning real-world problems <ref type="bibr" target="#b38">[29]</ref>. Further details and descriptions of such distributed processing frameworks will be given in <ref type="bibr">Section 4.</ref> With the aim of choosing the most relevant and pertinent subset of features, a variety of feature reduction techniques were proposed within the Apache Spark framework to deal with big data in a distributed way. Among these are several feature extraction methods, i.e., methods that create new attributes from the initial feature set (such as nn-gram, Principal Component Analysis, Discrete Cosine Transform, Tokenizer, PolynomialExpansion, ElementwiseProduct, etc) contrary to feature selection approaches that select a subset from the initial features. There have been very few feature selection techniques, which were proposed, and these are the VectorSlicer, the RFormula and the ChiSqSelector. To further expand this restricted research, i. e., the development of parallel feature selection methods, lately, some other feature selection techniques were proposed in the literature, which are based on evolutionary algorithms <ref type="bibr" target="#b39">[30]</ref>. Specifically, the evolutionary algorithms were implemented based on the MapReduce paradigm to obtain subsets of features from big data sets<ref type="foot" target="#foot_1">2</ref> . These include a generic implementation of greedy information theoretic feature selection methods <ref type="foot" target="#foot_2">3</ref> , which are based on the common theoretic framework presented in <ref type="bibr" target="#b40">[31]</ref>, and an improved implementation of the classical Minimum Redundancy and Maximum Relevance feature selection method <ref type="bibr" target="#b40">[31]</ref>. This implementation includes several optimizations such as cache marginal probabilities, accumulation of redundancy (greedy approach) and a data-access by columns <ref type="foot" target="#foot_3">4</ref> . However, when studying these distributed techniques, it is noticed that most of them suffer from some limitations. For example, they usually require the user or expert to deal with the algorithms' parameterization, noise levels specification, or simply order the set of attributes and let the user choose their own subset. There are some other feature selection techniques that require the user to indicate how many attributes should be selected, or require the user to give a threshold that determines when the algorithm should stop. This are all considered as significant drawbacks as they require users/experts to make a decision based on their own (possibly subjective) perception.</p><p>To overcome the shortcomings of the parallel state-of-the-art techniques, it seems to be crucial to look for a filter approach that does not require any external or supplementary information to function properly. Rough Set Theory (RST) can be used as such a technique. As previously mentioned, most of the classical rough set algorithms are sequential ones, computationally expensive and can only deal with non-large data sets. With a special focus on the generation of a globally minimal feature set, the prohibitive complexity of these algorithms comes from the search for an optimal attribute subset through the computation of an exponential number of candidate subsets. Although it is an exhaustive method, this is quite impractical for most data sets specifically for big data as it becomes clearly unmanageable to build the set of all possible combinations of features. In order to overcome these weaknesses, a first version presenting a parallel rough set model was presented in <ref type="bibr" target="#b20">[11]</ref>. The main idea in <ref type="bibr" target="#b20">[11]</ref> is to randomly split the given big data set into several partitions, each with a smaller number of randomly selected features, which are all then processed in a parallel way. This is to minimize the computational effort of the RST computations when dealing with a very large number of features particularly. As it can be noticed, and as previously explained in Section 1, this parallel RST version suffers from one main limitation: it cannot guarantee data dependency within the distributed environment. Our work, which is a revision of <ref type="bibr" target="#b20">[11]</ref>, is based on a distributed partitioning procedure, within a Spark/MapReduce paradigm, that makes our proposed solution scalable and effective in dealing with big data. The new version that we are proposing in this manuscript, LSH-dRST, adopts a hashing technique that will better preserve data dependency as it will match similar features into the same bucket and map the generated buckets into partitions to enable the splitting of the universe in a more efficient way. As previously mentioned in Section 1, a preliminary version of LSH-dRST was introduced in <ref type="bibr" target="#b30">[21]</ref> where a non-detailed description of the algorithm as well as a limited analysis of the algorithm's performance were presented. In this paper, a deep and thorough elucidation and performance investigation of LSH-dRST will be given as highlighted in the contributions presented in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary knowledge</head><p>In this section, we provide preliminary knowledge about the Locality Sensitive Hashing algorithm and the granulation structure of Rough Set Theory for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Locality Sensitive Hashing</head><p>There are several hashing techniques that have been proposed in the literature <ref type="bibr" target="#b26">[17,</ref><ref type="bibr" target="#b27">18,</ref><ref type="bibr" target="#b28">19]</ref>. Among these the Locality Sensitive Hashing (LSH) algorithm <ref type="bibr" target="#b26">[17]</ref> is considered as the most representative and popular one. LSH is presented as a probabilistic similarity-preserving dimensionality reduction method. Based on the adopted distances and similarities, including l p distance <ref type="bibr" target="#b41">[32]</ref>, angular distance <ref type="bibr" target="#b42">[33]</ref>, Hamming distance <ref type="bibr" target="#b43">[34]</ref>, Jaccard coefficient <ref type="bibr" target="#b44">[35]</ref>, etc., different types of LSH can be designed, which also depends on the type of the used data <ref type="bibr" target="#b26">[17]</ref>. Many variants are developed based on these basic LSH families such as Spectral hashing <ref type="bibr" target="#b45">[36]</ref>, Kernelized spectral hashing <ref type="bibr" target="#b46">[37]</ref>, and independent component analysis hashing <ref type="bibr" target="#b47">[38]</ref>. These methods aim at learning the hash functions for better fitting the data distribution <ref type="bibr" target="#b48">[39]</ref>.</p><p>In this section, we will mainly introduce LSH, among other hashing techniques, as this algorithm is often taken as a baseline. But most importantly, the algorithm is widely used in database systems for high dimensional similarity search. The choice of LSH is also based on several advantages and characteristics the algorithm has in comparison to other hashing techniques when dealing with high dimensional data sets as demonstrated in <ref type="bibr" target="#b29">[20]</ref>.</p><p>LSH was introduced as a probabilistic technique suitable for solving the approximate K-nearest neighbors (K-NN) problem in a high dimensional space. It is based on the definition of an LSH family (H), a family of hash functions mapping similar input items to the same hash code with higher probability than dissimilar items. Formally, an LSH family is defined as follows: Let H be a family of hash functions such that h ∈ H : R d → U. Consider a function h that is chosen uniformly at random from H and a similarity function sim : R d × R d → <ref type="bibr">[0,</ref><ref type="bibr" target="#b10">1]</ref>. The family H is called locality sensitive if for any vectors u, v ∈ R d , it satisfies the property:</p><formula xml:id="formula_0">P (h(u) = h(v)) = sim(u, v).</formula><p>That means, the more similar a pair of vectors is, the higher the collision probability is. The LSH scheme indexes all items in hash tables and searches for near items via hash table lookup. Formally, for an integer k, we define a function family</p><formula xml:id="formula_1">G = {g : R d → U k } such that g(v) = (h 1 (v), . . . , h k (v))</formula><p>where h i ∈ H, i.e., g is the concatenation of k LSH functions. For an integer , we choose functions G = {g 1 , . . . , g } from G independently and uniformly at random. Each g i , 1 ≤ i ≤ effectively constructs a hash table denoted by D g i . The hash table is a data structure that is composed of buckets, each of which is indexed by a hash code. A bucket in D g i stores all v ∈ V that have the same g i values. Only the nonempty buckets are retained using standard hashing. G defines a collection of tables I G = {D g 1 , . . . , D g } and we call it an LSH index.</p><p>As previously mentioned, there are different kinds of LSH families for different (dis)similarity measures including Hamming distance, Jaccard similarity, and cosine similarity. In this paper, we rely on the LSH scheme that supports the p-stable similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Rough Sets for Feature Selection</head><p>Rough Set Theory (RST) <ref type="bibr" target="#b49">[40]</ref> is a formal approximation of the conventional set theory which supports approximations in decision making. It provides a filter-based technique by which knowledge may be extracted from a domain in a concise way; retaining the information content whilst reducing the amount of knowledge involved <ref type="bibr" target="#b25">[16]</ref>. The fundamentals of RST for feature selection are as follows:</p><p>In RST, an information table is defined as a tuple T = (U, A) where U and A are two finite, nonempty sets, U the universe of primitive objects and A the set of attributes. Each attribute or feature a ∈ A is associated with a set V a of its value, called the domain of a. We may partition the attribute set A into two subsets C and D, called condition and decision attributes, respectively. Any pair (x, a), where x ∈ U and a ∈ A defines the table entry consisting of the value a(x).</p><p>Let For any concept X ⊆ U , two operations can be defined: B(X) = {x ∈ U : B(x) ⊆ X}, and B(X) = {x ∈ U : B(x) ∩ X = ∅}, assigning to every subset X of the universe U two sets B(X) and B(X) called the B-lower and the B-upper approximation of X, respectively. The lower approximation of a set X with respect to B is the set of all objects, which can be for certain classified as X using B. The upper approximation of a set X with respect to B is the set of all objects, which can be possibly classified as X using B. The concept defining the set of objects that can be classified neither as X nor as not-X using B is called the boundary region, and is defined as: BN D B (X) = B(X) -B(X). If the boundary region of X is empty, that is B(X) = B(X), concept X is said to be B-definable; otherwise X is a rough set with respect to B.</p><p>To discover dependencies between attributes in a given decision system T = (U, C, D), the dependency degree is defined. Intuitively, a set of attributes D depends totally on a set of attributes C, denoted C ⇒ D, if the values of attributes from C uniquely determine the values of attributes from D. This can be formally defined as: we say that D depends on C to a degree k (0 Based on these basics, RST defines the D-reduct concept for feature selection. Let C, D ⊆ A, be sets of condition and decision attributes respectively. We will say that C ⊆ C is a D-reduct (reduct with respect to D) of C, if C is a minimal subset of C such that: γ(C, D) = γ(C , D), and there is no C ⊂ C such that γ(C , D) = γ(C , D). In other words, the Reduct is the minimal set of selected attributes preserving the same dependency degree as the whole set of attributes. Meanwhile, RST may generate a set of D-reducts, called RED F D (C, D), from the given information table. In this case, any reduct from RED F D (C, D) can be chosen to replace the initial information table.</p><formula xml:id="formula_2">≤ k ≤ 1), denoted C ⇒ k D if: k = γ(C, D) = |P OS C (D)|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Parallel Computing Frameworks and the MapReduce Programming Model</head><p>With the dramatic increase of the amount of data, a specific computing paradigm has been stressed out, which is called parallel computing. Parallel computing is a type of computation in which multiple compute resources are simultaneously used to perform several calculations to solve a computational problem. Within a parallel computing schema, a problem is broken into discrete and smaller parts that can be solved concurrently. Each of these parts is further broken down to a series of instructions, which are executed simultaneously on different CPUs so that they can be solved at the same time.</p><p>In the context of big data, it has become necessary to implement a new set of technologies and tools that permit parallel computing in an effective way. Different techniques <ref type="bibr" target="#b50">[41]</ref> have been developed to handle high dimensional data sets where most of these proposed tools are based on distributed processing, e.g., the Message Passing Interface (MPI) programming paradigm <ref type="bibr" target="#b51">[42]</ref>.</p><p>The encountered challenges are essentially linked to the access to the given big data, to the transparency of the development process of the software with respect to its prerequisites, as well as to the available programming paradigms <ref type="bibr" target="#b52">[43]</ref>. For example, standard techniques require that all the given data should be loaded into the main machine's memory. This obviously presents a technical issue in big data since the data, which is given as input, is usually stored in different locations causing an intensive communication in the network as well as some supplementary input and output costs. It is true that it is possible to afford this but it is also important to mention that it will be crucial to afford an intensively large main memory to be able to retain all the pre-loaded given data for computing and processing purposes.</p><p>To overcome these serious limitations, a new set of highly efficient and fault-tolerant parallel frameworks has been developed and set in the market. These distributed frameworks can be categorized with respect to the nature or type of the data they are able to process. Actually, there are some frameworks that can only process batch data. Within this schema, the parallel processing system functions over a high dimensional and static data set. At a later level of the distributed processing, the system returns the output result(s) when all the processes of computations are successfully achieved. Among the well-known open-source distributed processing frameworks dedicated for batch processing, we mention Hadoop <ref type="foot" target="#foot_4">5</ref> . Hadoop is based on simple programming paradigms that allow a highly scalable and reliable parallel processing of high dimensional data sets. The framework offers a costeffective solution to store and process different types of data such as structured, semi-structured and unstructured data without any specific format specifications. Technically, Hadoop works on top of the Hadoop Distributed File System (HDFS), which duplicates the input data files in various storage machines (nodes). In this manner, the framework facilitates a fast transfer rate of the data among nodes set in the cluster and allows the system to operate without any interruption if one or a number of nodes fail. MapReduce is the core of the Hadoop framework. This paradigm offers an intensive scalability over a large number of nodes within a Hadoop cluster.</p><p>On the other hand, there are some other distributed frameworks that can only deal with streaming data. Within these frameworks' design, the distributed calculations are performed over data (to each individual data item) at the time it enters the parallel framework. Apache Storm <ref type="foot" target="#foot_5">6</ref> and Apache Samza<ref type="foot" target="#foot_6">7</ref> are among the most popular stream processing frameworks. A third category of distributed frameworks can be highlighted, which is considered as hybrid systems. This is because these frameworks are capable of processing not only batch data but also stream data. In these frameworks' designs, similar or some linked elements can be used for both types of data. This makes the diverse processing requirements of the hybrid systems much easier and simpler. Among the well-known streaming processing parallel frameworks, we mention Apache Spark<ref type="foot" target="#foot_7">8</ref> and Apache Flink <ref type="foot" target="#foot_8">9</ref> .</p><p>In this research, we focus on Apache Spark. This distributed open source framework was initially developed in the UC Berkeley AMP Lab for big data processing. Apache Spark is characterized by its capability of improving the system's effectiveness-which is achieved via the use of intensive memory-, its efficiency, and its high transparency for users. These characteristics allow to perform parallel processing of diverse application domains in a simple and easy way. More precisely and in comparison to Spark, in Hadoop MapReduce multiple jobs would be adjusted together to build a data pipeline. In this process, and in every level of this pipeline, MapReduce will have to read the data from the disk, and then write it back to the disk again. This process was obviously ineffective as it had to read all the data and write it from and back to the disk at each level of the process. To deal with this issue, Apache Spark comes into play. Based on the same MapReduce paradigm, the Spark framework could offer an immediate 10 times increase in the system's performance. This is explained by the nonnecessity to store the given data back to the disk at every stage of the process as all activities remain in the memory <ref type="bibr" target="#b38">[29]</ref>. Spark allows a much faster data process in contrast to transferring it through needless Hadoop MapReduce mechanisms. Adding to this specificity, the key concept that Spark offers is a Resilient Distributed Data set (RDD), which is a set of elements that are distributed across the nodes of the used cluster that can be operated on in a parallel way. Indeed, Spark has a number of high-level libraries for working with structured data (Spark SQL 10 ), for stream processing (Spark Streaming 11 ), for machine learning (MLlib) 12 [44], and for graphs and graph-parallel computation (GraphX 13 ). Other than that, there are also many R 14 and Python 15 libraries, among others, which allow the programmers to code without writing mappers and reducers themselves.</p><p>The choice of Spark to design our proposed algorithm based on rough sets for big data feature selection is based on several reasons, which are as follows: (i) To offer a general solution based on a hybrid parallel framework. (ii) Apache Spark provides high speed benefits with a trade-off in the usage of high memory. (iii) Spark is one of the well-known and certified distributed frameworks and also a mature hybrid system. This is specifically true when comparing it to some other frameworks in the market, which are considered as more niche in terms of their usage but more importantly they are still in their initial periods of adoption 16 .</p><p>As previously mentioned, Spark is based on MapReduce <ref type="bibr" target="#b34">[25]</ref> which is one of the most popular processing techniques and program models for distributed computing to deal with big data. Spark revolves around the concept of a resilient distributed dataset (RDD) 17 , which is the Spark programming model. It is a fault-tolerant collection of elements that can be operated on in parallel. RDDs, used as our programming model, support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program. The map and the reduce concepts constitute the MapReduce paradigm, which was proposed by Google in 2004 and designed to easily scale data processing over multiple computing nodes. As mentioned, the MapReduce paradigm is composed of two main tasks/phases, namely the map phase and the reduce phase, which will be the main concepts used in our developed approach (see Sections 5 and 5.2.2). At in abstract level, the map process takes as input a set of data and transforms it into a different set where each element is represented in the form of a tuple key/value pair, producing some intermediate results. Then, the reduce process collects the output from the map task as an input and combines these given key/value tuples into a smaller set of pairs to generate the final output. A representation of the MapReduce framework is given in Figure <ref type="figure" target="#fig_2">1</ref>. Technically, the MapReduce paradigm is based on a specific data structure, which is the (key, value) pair. More precisely, during the map phase, on each split of the data the map function gets a unique (key, value) tuple as an input and generates a set of intermediate (key', value') pairs as output. This is represented as follows: map(key, value) → {(key , value ), . . .}.</p><p>(1)</p><p>After that, the MapReduce paradigm assembles all the intermediate (key', value') pairs by key via the shuffling phase. Finally, the reduce function takes the aggregated (key', value') pairs and generates a new (key', value") pair as output. This is defined as: reduce(key , {value , . . .}) → (key , value ).</p><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>As discussed, a variety of open source parallel computing frameworks are proposed in the market and in this section, we have highlighted the well-known ones. However, it is important to mention that choosing a particular distributed framework is always dependent to the type or kind of the given data that the system will process. The choice also depends on how time bound the specifications of the users are, and on the types of output results that users are looking for. In this paper, we mainly focus on the use of Apache Spark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">LSH-dRST: The proposed Solution</head><p>In this section, we will make a detailed description of our proposed solution named LSH-dRST. LSH-dRST has a distributed architecture with respect to the Apache Spark framework for a parallel and inmemory processing job. At the beginning of this section, we will present the main motivation behind the development and implementation of the LSH-dRST solution. To do so, we will first explain the limitations of the standard distributed RST version. Next, we will explain LSH-dRST as an effective technique that is capable of performing big data feature selection in a more intelligent and convenient way without any considerable information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Motivation and Problem Statement</head><p>As explained in Section 3.2, the theory of rough sets goes through the calculation of the dependency of attributes, γ(C, D), so that it can perform feature selection. With this aim, and as a first process, the indiscernibility relation, defined as IN D(B), for all attributes has to be calculated. As previously defined, the IN D(B) searches for similar attribute values and gathers the corresponding features to form the set of the equivalence relations. With respect to these fundamental RST notions, it is essential to guarantee data dependency in order to define the most reliable and consistent equivalence relations, so that the most representative reduct set can be determined. However, assuring data dependency is considered as a big challenge when it comes to distributed environments and parallel computing.</p><p>The standard distributed RST version <ref type="bibr" target="#b20">[11]</ref>, named Sp-RST, applies a random process when partitioning the feature search space; a process that does not guarantee data dependency. More precisely, Sp-RST partitions the information table T (the big data set) into a set of m data blocks based on splits from the conditional attribute set C, i.e., m smaller data sets with a fewer number of features instead of using a single data block (T ) with an unmanageable number of features |C| (noted as T (C)). The key idea is to generate m smaller data sets named T i , where i ∈ {1, . . . , m}, from the big data set T , where each T i is defined via a manageable number of features r with r ≪ |C| ∈ {c 1 , . . . , c V } and r ∈ {1, . . . , V }. The resulting data blocks are defined as T i (r). This is formalized as T = m i=1 T i (r). r is a user defined parameter that refers to the number of attributes, which will be considered to build each data block T i , and V refers to the total number of attributes. Specifically, every T i is built using r random and distinct attributes, which are selected from C, where there are no common attributes between all the built T i . With respect to the parallel implementation design, and as each constructed partition is processed by a different machine, i.e., node, the distributed Sp-RST algorithm is applied to every T i (r) while gathering all the intermediate results from the m distinct created partitions (instead of being applied to the complete T that encloses the whole set C of conditional features (T (C))).</p><p>Based on this Sp-RST architecture and implementation design, it is very probable that similar attributes will be part of different partitions T i . Consequently, an erroneous estimation of the constructed IN D(B) is more likely to occur. More precisely, the applied random process may mislead the RST feature selection process by generating a non-relevant reduct. Based on these limitations, and aiming at guaranteeing data dependency, in this paper we propose the LSH-dRST solution that makes use of the locality sensitive hashing algorithm. Using LSH can guarantee the process of gathering similar or close data instances based on their attribute values into the same bucket. By using the generated buckets, a more intelligent and reliable partitioning of the universe can be applied. In such a way, LSH-dRST can conserve data dependency within the same buckets and hence it is capable of solving the standard distributed RST limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">LSH-dRST</head><p>In order to deal with big data sets and to make use of the adopted LSH technique within a distributed environment, the appropriate set of buckets, which is based on LSH, is generated first. After that, these generated buckets will be mapped into several partitions. Then, the entire rough set feature selection process, with all its granular concepts, will be partitioned into different elementary tasks where each of these will be executed independently on each generated bucket. As a last step, the intermediate results will be conquered to finally acquire the final output, i.e., the reduct set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">General Model Formalization</head><p>To perform feature selection, our learning problem has to select a set of high discriminating attributes from the original large set of features that describes the high dimensional input database. The input dataset corresponds to the data stored in the given Distributed File System (DFS). To operate on the given DFS, a Resilient Distributed Data set (RDD) is created (defined in Section 4). This design can be formalized as follows: the created RDD reflects the given information table, which we name T RDD . T RDD is defined via a universe U = {x 1 , . . . , x N }, which refers to the set of data items, a large conditional attribute set C = {c 1 , . . . , c V } that contains every single attribute of the T RDD information table, and via a decision attribute D of the given learning problem. D corresponds to the class, i.e., label, of each T RDD sample and is defined as D = {d 1 , . . . , d W }. The conditional feature set C reflects the pool from where the most convenient attributes will be selected.</p><p>To make LSH-dRST scalable with the large number of attributes -instead of applying the algorithm to a single data block (T RDD ) defined via its unmanageable feature set C (that we also note as T RDD (C)) -and with respect to data dependency, the given T RDD information table is partitioned into B data blocks based on the B generated LSH buckets, named as T RDD (b) where b ∈ {1, . . . , B}. This is achieved by performing a transpose of the feature set C, on which LSH is applied to generate the buckets based on the features. Specifically, LSH is applied based on a family of hash functions H and based on the p-stable similarity and on a Gaussian distribution as explained in Section 3.1. The different buckets correspond to splits from the conditional feature set C and each bucket covers a definite feature space h incorporating all similar and close data instances based on their attribute values. The resulting data block is denoted as T RDD (b) (h). This can be formalized as:</p><formula xml:id="formula_4">T RDD = B b=1 T RDD (b) (h)</formula><p>, where h ∈ {1, . . . , V }. The parameter h refers to the value, which is generated by LSH, and corresponds to the number of attributes per bucket that will be considered to build each T RDD (b) data block. h is equal to the size of the feature space C divided by B. This part of the LSH-dRST functioning is presented in Figure <ref type="figure" target="#fig_3">2</ref>.</p><p>Once the data blocks (also referred to as buckets) T RDD (b) are defined, the K nearest neighbors approach is applied on every T RDD (b) . This results in automatically creating a set of S sub-information tables that we name as Cl, i.e., T RDD (b) is partitioned into S sub-information tables Cl. K corresponds to the number of attributes per sub-information table Cl s , with s ∈ {1, . . . , S}, and on which LSH-dRST will be applied. The resulting information table is denoted as Cl s (K). This can be formalized Aiming at ensuring scalability, instead of applying LSH-dRST to T RDD , which covers the whole conditional attribute set C (T RDD (C)), the distributed algorithm will be applied to every single Cl s (K). At the end, all the intermediate results will be congregated from the different sub-information tables Cl of every single T RDD (b) partition. Based on such a process, we can ensure that LSH-dRST can be applied to a computable and manageable number of attributes while preserving data dependency. Consequently, the limitations of the standard distributed RST <ref type="bibr" target="#b20">[11]</ref>, i.e., Sp-RST, can be solved. The pseudo-code of LSH-dRST is presented in Algorithm 1.</p><p>Technically, as a first step, LSH-dRST will generate the B partitions using LSH, defined as T RDD (b) , while preserving data dependency as previously explained and as presented in Figure <ref type="figure" target="#fig_3">2</ref> (see Algorithm 1, line 1). After that, for each specific partition T RDD (b) , a set S of sub-information tables (called Cl) will be built in such a way that the K nearest neighbors from any data point within the T RDD (b) feature search space form a sub-information table called Cl s , where s ∈ {1, . . . , S}, which is defined via its number of features K (noted as Cl s (K)) (see Algorithm 1, line 4).</p><p>The different tasks of the distributed LSH-dRST, Algorithm 1, lines 6 -11, will be performed on . This task is independent from the B partitions, which are built, as the result depends on the data items class/label and not on the set of attributes. Now, outside the iteration loops, Algorithm 1, line 14, each Cl s (K) will have an output, which can be in one of the following two forms: (i) either a single reduct RED s (K, D) or (ii) a family of reducts RED F s (K, D). As previously explained in Section 3.2, based on the RST preliminaries and granular concepts, any reduct of RED F s (K, D) can be used to represent the Cl s (K) sub-information table. Accordingly, in the case where LSH-dRST generates a single reduct for a specific Cl s (K) subinformation table, the output of this attribute selection phase is the set of the RED s (K, D) attributes. These selected attributes reflect the most informative features among the initial K features defining Cl s (K). This results in a new reduced Cl s (K), defined as Cl s (RED), which preserves nearly the same data quality as its corresponding Cl s (K), which is based on the whole feature set K. The second case is when LSH-dRST generates a family of reducts. In this particular case, the algorithm will randomly select one reduct among RED F s (K, D) to represent the corresponding Cl s (K). Let us recall, as seen in Section 3.2, that this random choice is mathematically justified by the different granular concepts of rough set theory, which define the same priority for all the possible generated reducts in RED F s (K, D). This means that any reduct, which is included in RED F s (K, D), can be used to replace the K features of Cl s (K). At this algorithmic phase, each Cl s sub-information table has its output RED s (K, D), which corresponds to the selected attributes. All the selected For each Cl s (K) -where s ∈ [1, . . . , S]</p><formula xml:id="formula_5">6: Generate AllComb (K) 7: Calculate IN D(AllComb (K) ) 8: Calculate DEP (AllComb (K) ) 9: Select DEP max (AllComb (K) ) 10: Filter DEP max (AllComb (K) ) 11:</formula><p>Filter N bF min (DEP max (AllComb (K) ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>End For 13: End For 14: Gather the outputs: Each Cl s (K) will have an output, which can be (i) either a single reduct RED s (K, D) or (ii) a family of reducts RED F s (K, D) from which a single reduct RED s (K, D) is selected randomly. All the selected reducts are irreducible within their Cl s (K). By removing the set of irrelevant and redundant attributes, LSH-dRST can reduce the dimensionality of the data, specifically the large number of features, from T RDD (C) to T RDD (Reduct). A high level description of the full functioning of LSH-dRST is given in Figure <ref type="figure" target="#fig_7">4</ref>.</p><p>In the following subsections, the different elementary distributed tasks of LSH-dRST will be described in more detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Algorithmic Details</head><p>As previously mentioned, the different elementary distributed tasks of LSH-dRST feature selection will be performed on every single Cl s (K) sub-information table, which is defined by its K attributes within the T RDD (b) partitions; except for the task defined in line 2 in Algorithm 1, which deals with IN D(D). To generate the final output, Reduct, LSH-dRST performs ten main jobs.</p><p>Initially, LSH-dRST applies the hashing technique, i.e., LSH, to build and generate the B different buckets based on a hash table as previously explained in Section 3.1. To do so, LSH-dRST creates the hash table based on a set of random vectors following a Gaussian distribution, referred to as the family H of hash functions. The hash table created is based on the number of the T RDD attributes. As a next step, LSH-dRST maps the T RDD to work on each single partition in a separate and parallel way. On each partition, the algorithm applies a projection for each vector based on the set of the mapped feature vectors in T RDD . Consequently, the buckets are automatically generated, each with a specific index, referred to as a hash code (as explained in Section 3.1). Lastly, the algorithm performs a sort operation/action to order the different buckets with respect to the given number of buckets B. Algorithm 2 presents the pseudo-code of this distributed task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The next process is that LSH-dRST has to compute the indiscernibility relation for the decision class</head><formula xml:id="formula_6">D = {d 1 , . . . , d W }, defined as IN D(D): IN D(d i ).</formula><p>Technically, the algorithm calculates the indiscernibility relation for every single decision class d i by collecting the same data items from T RDD , which are defined in the universe U = {x 1 , . . . , x N } and belong to the same class d i . To achieve this task, LSH-dRST processes a first map transformation operation taking the data in its format of (id i of x i , List of the features of x i , Class d i of x i ) and transforming it to a key, value pair: Class d i of x i , List of id i of x i . Based on this transformation, the decision class d i defines the key of the generated output and the data items identifiers id i of x i of T RDD define the values. After In the third algorithmic step of LSH-dRST, the algorithm has to generate the set S of the subinformation tables Cl s (K) based on the number of attributes K. At this stage, let us recall that LSH has already gathered all the instances based on their similar attributes within a same specific bucket. On these similar attributes collected, an additional partitioning is required to build and generate the sub-information tables that can be handled by LSH-dRST. As previously explained in Section 1, and as we are focusing on the generation of a globally minimal reduct, to perform feature selection, the theory of rough sets has to generate all the combination of features at once, process them in turn to finally generate the ultimate reduct. Since it is infeasible to generate all the combinations of attributes based on the large amounts of features, the distributed LSH-dRST algorithm will function on the Cl s (K) sub-information tables built on K features. K is a manageable size that can be handled by LSH-dRST.</p><p>Based on this formalization, and to achieve this distributed job, for every bucket T RDD (b) , LSH-dRST performs a mapP artitionsW ithIndex <ref type="foot" target="#foot_10">19</ref> transformation operation using the buckets indexes, i.e., the already generated hash codes in Algorithm 2. This is to be able to apply the k-nearest neighbors (KNN) separately on each T RDD (b) partition while providing an index of the partition. After that, the output of the applied transformation function is mapped in a way that: for each partition, the K features, which are the nearest to a randomly chosen feature within the same T RDD (b) hash code, are selected to build a sub-information table (Cl s (K)). Algorithm 4 presents the pseudo-code related to this distributed task. Cl s (K): sub-information tables 1: Perform a mapP artitionsW ithIndex on every T RDD (b) using its index 2: Map the result of step (1) 3: Perform a KNN by looking for the K nearest features within a randomly selected attribute within each T RDD (b) 4: Generate the set S of the sub-information tables Cl s (K) At this stage, and for all the T RDD (b) , the set S of the sub-information tables Cl s (K) is generated. As a next step, the algorithm has to select the most pertinent features from each Cl s (K). To perform this task, LSH-dRST creates a set of all possible combinations of the sets of K features, defined as the AllComb (K) RDD, by applying the f latmap() <ref type="foot" target="#foot_11">20</ref> transformation operation and the combinations() operation. This is shown in Algorithm 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 5: Generate AllComb (K)</head><p>Input: K: number of features Output: AllComb (K) : set of all possible combinations of K features 1: Generate the AllComb (K) RDD by applying the f latmap() function and the combinations() operation on each element 2: Return AllComb (K)</p><p>After that, within the fifth LSH-dRST distributed job, the algorithm computes the indiscernibility relation IN D(AllComb (K) ) for every previously generated combination, i.e., the indiscernibility relation of every element in the output of Algorithm 5, named AllComb (K) i . With respect to the terminologies used in Section 3.2, and to calculate the indiscernibility of a combination of features, the following equations are applied: (i)</p><formula xml:id="formula_7">IN D C = U/C = {[o j ] C |o j ∈ U }, (ii) [o j ] C = {o i |C(o i ) = C(o j )},</formula><p>where o i and o j refer to data objects of the universe. In this task and as described in Algorithm 6, the algorithm aims at collecting all the identifiers id i of the data items x i that have identical values of the combination of attributes, which are extracted from AllComb (K) . To do so, a first map operation is applied, taking the data in its format of (id i of x i , List of the features of x i , Class d i of x i ) and transforming it to a key, value pair: (AllComb (K) , List of the features of x i ), List of id i of x i . Based on this transformation, the combination of features and their vector of features define the key and the identifiers id i of the data items x i define the value. After that, the f oldByKey() operation is applied to merge all values of each key in the transformed RDD output, i.e., all the identifiers id i of the data items x i that have the same combination of features with their corresponding vector of features (AllComb (K) , List of the features of x i ). This is to represent the sought IN D(AllComb (K) ). 1: Map the Cl s (K) based on its format (id i of x i , List of the features of x i , Class d i of x i ) and generate the new format as a key-value pair (AllComb (K) , List of the features of x i ), List of id i of x i 2: Merge the values of each generated key using the f oldByKey()</p><formula xml:id="formula_8">operation 3: Return AllComb (K) : IN D(AllComb (K) )</formula><p>At this algorithmic phase, LSH-dRST arranges the set of attributes that will be selected in the next algorithmic distributed jobs/steps. As presented in Algorithm 7, the dependency degree, defined as γ(AllComb (K) , D), of every single feature combination is calculated. This is achieved based on the definition of γ given in Section 3.2 and with respect to the definition of the indiscernibility of a combination of features IN D(AllComb (K) ) given above. For this task, the distributed job requires three input parameters, which are the calculated indiscernibility relations IN D(D), the IN D(AllComb (K) ) and the set of all attribute combinations AllComb (K) . For every element AllComb (K) i in AllComb (K) , and by using the intersection() transformation, the job tests first if the intersection of every</p><formula xml:id="formula_9">IN D(d i ) of IN D(d) with each element IN D(AllComb (K) i in IN D(AllComb (K) )</formula><p>holds all the elements in the latter parameter. This process refers to the calculation of the lower approximation as detailed in Section 3.2. We name the length of the resulting intersection as LengthIntersect. If the condition is satisfied, then a score, which is equal to the length of the elements resulting from the generated intersection, i.e., LengthIntersect, is assigned, else a 0 value is given.</p><p>After that a reduce function is applied over the different IN D(D) elements together with a sum() function applied on the calculated scores, which are based on the elements having the same IN D(d i ). This operation is followed by a second reduce function, which is applied over the different IN D(AllComb (K) ) elements together with a sum() function applied on the previously calculated results, which are indeed based on the elements having the same AllComb (K) i .</p><p>The latter output refers to the dependency degrees: γ(AllComb (K) , D). This distributed job generates two outputs namely the set of dependency degrees γ(AllComb (D) , D) of the attribute combinations AllComb (K) as well as their associated sizes Size (AllComb (K) ) .</p><p>After that, the algorithm searches for the maximum dependency value DEP max (AllComb (K) ) among all the γ(AllComb (K) , D) generated by applying the max() function operated on the given RDD input, referred to as RDD[AllComb (K) , Size (AllComb (K) ) , γ(AllComb (K) , D)]. Specifically, the max() function will be applied on the third argument of the given RDD, i. e., As presented in Algorithm 8, the output of this job, defined as M axDependency, corresponds to (i) the dependency of the whole feature set representing the Cl s (K) and (ii) the dependency of all the possible feature combinations satisfying the constraint γ(AllComb (K) , D) = γ(K, D) (outlined in Section 3.2). The output M axDependency represents the baseline value for feature selection.</p><p>Once M axDependency is defined, the algorithm keeps the set of all combinations having the same dependency degrees as M axDependency, i.e., γ(AllComb (K) , D) = M axDependency. This job is achieved by applying a f ilter() function. At this stage, the algorithm eliminates in each computation level the useless attributes that may negatively affect the performance of any learning algorithm. This distributed job is presented in Algorithm 9.</p><p>Algorithm 9: Filter DEP max (AllComb (K) )</p><formula xml:id="formula_10">Inputs: RDD[AllComb (K) , Size (AllComb (K) )</formula><p>, γ(AllComb (K) , D)]: RDD presenting all AllComb (K) and their characteristics M axDependency: baseline value for feature selection Output: Filtered-RDD[AllComb (K) , Size (AllComb (K) ) , γ(K, AllComb (K) )]: filtered RDD and its characteristics 1: Apply the f ilter() function on the input RDD in a way to select all combinations having a dependency that is equal to M axDependency: γ(AllComb (K) ) = M axDependency 2: Return the filtered RDD:</p><formula xml:id="formula_11">Filtered-RDD[AllComb (K) , Size (AllComb (K) ) , γ(AllComb (K) , D)]</formula><p>At a final stage, and using the results generated from the previous step, which is the input of Algorithm 10, the algorithm applies first the min() operator to look for the minimum number of features among all the Size (AllComb (K) ) . Specifically, the min() operator will be applied to the second argument of the given RDD. Once determined, a result that we name minN bF , the algorithm applies a f ilter() method to only keep the set of combinations having the same minimum number of features as minN bF . This is achieved by satisfying the full reduct constraints highlighted in Section 3.2: γ(AllComb (K) , D) = γ(K, D) while there is no AllComb (K ) ⊂ AllComb (K) such that γ(AllComb (K ) , D) = γ(AllComb (K) , D). Every combination that satisfies this constraint is evaluated as a possible minimum reduct set. The attributes of the reduct set describe all concepts in the sub-information table Cl s (K).</p><p>Algorithm 10: Filter N bF min (DEP max (AllComb (K) ))</p><formula xml:id="formula_12">Input: RDD[AllComb (K) , Size (AllComb (K) ) , γ(K, AllComb (K) )]</formula><p>: RDD presenting all AllComb (K) and their characteristics Output: Filtered-RDD[AllComb (K) , Size (AllComb (K) ) , γ(K, AllComb (K) )]: viable minimum reduct set and its characteristics 1: Apply the min() function on the input filtered RDD second argument: Size (AllComb (K) ) to get minN bF 2: Apply a f ilter() function on the input RDD while satisfying the condition Size (AllComb (K) ) = minN bF 3: Return Reduct = List of selected K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">LSH-dRST: a working example</head><p>We apply LSH-dRST to an example of an information table, T RDD (C), which is presented in Table <ref type="table" target="#tab_0">1</ref>. By assuming that the considered T RDD (C) is a big data set, the information table is defined via a universe U = {x 1 , x 2 , . . . , x 8 }, which refers to the set of data instances (items), a large conditional feature set C = {a, b, c, d, e, f, g, l} that includes all the features of the information table T RDD (C) and a decision feature X of the given learning problem. X refers to the label (or class) of each T RDD (C) data item and is defined as follows: X = {1, 2}. C presents the conditional attribute pool from where the most significant attributes will be selected. </p><formula xml:id="formula_13">x ∈ U a b c d e f g l X x1 1 2 4 0 1 2 5 3 1 x2 0 3 3 2 1 3 2 2 2 x3 2 3 1 3 3 1 6 1 2 x4 1 1 2 1 2 3 5 3 2 x5 0 2 0 1 2 4 2 3 1 x6 1 1 2 4 3 1 3 1 2 x7 2 2 1 3 2 4 2 2 2 x8 1 2 0 2 2 4 7 0 1</formula><p>Independently from the set of conditional features C, LSH-dRST computes the indiscernibility relation for the decision class X. We define the indiscernibility relation as IN D(X): IN D(X i ). LSH-dRST will calculate IN D(X) for each decision class X i by associating the same data items (instances) T RDD (C) that are expressed in the universe U and that belong to the same decision class X i . Based on the Apache Spark framework and by applying Algorithm 3, line 1, we get the following outputs from the different Apache Spark data splits, which are presented in Table <ref type="table">2</ref> and in Table <ref type="table">3</ref>:</p><formula xml:id="formula_14">x ∈ U a b c d e f g l X x 1 1 2 4 0 1 2 5 3 1 x 2 0 3 3 2 1 3 2 2 2 x 3 2 3 1 3 3 1 6 1 2 x 4 1 1 2 1 2 3 5 3 2</formula><p>Table <ref type="table">2</ref>. Toy dataset -Split 1.</p><p>• From Split 1:</p><formula xml:id="formula_15">-1, x 1 -2, x 2 x ∈ U a b c d e f g l X x 5 0 2 0 1 2 4 2 3 1 x 6 1 1 2 4 3 1 3 1 2 x 7 2 2 1 3 2 4 2 2 2 x 8 1 2 0 2 2 4 7 0 1</formula><p>Table <ref type="table">3</ref>. Toy dataset -Split 2.</p><p>-2, x 3 -2, x 4</p><p>• From Split 2:</p><p>-</p><formula xml:id="formula_16">1, x 5 -2, x 6 -2, x 7 -1, x 8</formula><p>After that, and by applying Algorithm 3, line 2, we get the following output, which refers to the indiscernibility relation of the class IN D(X):</p><formula xml:id="formula_17">• 1, {x 1 , x 5 , x 8 } • 2, {x 2 , x 3 , x 4 , x 6 , x 7 }</formula><p>In this example, we assume that the number of buckets B = 2 and K = 2. By applying LSH, Algorithm 2, to the set of features C = {a, b, c, d, e, f, g, l}, the output is a set of data items based on the following similar and close features:</p><formula xml:id="formula_18">• Bucket 1 (T RDD b=1 ): {d, e, f, g} • Bucket 2 (T RDD b=2 ): {a, b, c, l}</formula><p>By applying KN N to each bucket, , Algorithm 4, the following sub-information tables are built:</p><formula xml:id="formula_19">• Bucket 1 (T RDD b=1 ): {d, e, f, g} -Cl s=1 (k = 2): {e, f} -Cl s=2 (k = 2): {d, g} • Bucket 2 (T RDD b=2 ): {a, b, c, l} -Cl s=3 (k = 2): {a, b} -Cl s=4 (k = 2): {c, l}</formula><p>Based on these assumptions, LSH-dRST will be applied to every Cl s (k). In what follows, we will only focus on Cl s=1 (k = 2) as an example. The same process will be applied to the rest of the Cl s (k), i.e., Cl s=2 (k), Cl s=3 (k) and Cl s=4 (k). The following partitions and splits based on Apache Spark are obtained for Cl s=1 (k = 2) (Table <ref type="table">4</ref> and<ref type="table">Table 5</ref>):</p><formula xml:id="formula_20">x ∈ U e f X x 1 1 2 1 x 2 1 3 2 x 3 3 1 2 x 4 2 3 2 Table 4. Split 1. x ∈ U e f X x 5 2 4 1 x 6 3 1 2 x 7 2 4 2 x 8 2 4 1</formula><p>Table <ref type="table">5</ref>. Split 2.</p><p>Based on Split 1, and by applying Algorithm 5, which aims to generate all the possible combinations AllComb (K) of the set of K attributes, the output from both Apache Spark splits is the following:</p><formula xml:id="formula_21">• e • f • e, f</formula><p>In its third distributed job, LSH-dRST calculates the indiscernibility relation IN D(AllComb (K) ) for every created combination, i.e., the indiscernibility relation of every element in the output of the previous step (Algorithm 5). By applying Algorithm 6 and based on both Apache Spark splits, the output is the following:</p><formula xml:id="formula_22">• From Split 1: -e, {x 1 , x 2 }, {x 3 }, {x 4 } -f, {x 1 }, {x 2 , x 4 }, {x 3 } -e, f, {x 1 }, {x 2 }, {x 3 }, {x 4 } • From Split 2:</formula><p>e, {x 5 , x 7 , x 8 }, {x 6 } f, {x 5 , x 7 , x 8 }, {x 6 } e, f, {x 5 , x 6 , x 7 }, {x 6 } In a next stage, and by using the previous output as well as IN D(X), LSH-dRST computes the dependency degrees γ(AllComb (K) , D) of each attribute combination as described in Algorithm 7. This distributed job generates two outputs namely the set of dependency degrees γ(AllComb (K) , D) of the attribute combinations AllComb (K) as well as their associated sizes Size (AllComb (K) ) . The output from both splits is the following:</p><formula xml:id="formula_23">• e, 2, 1 • f, 5, 1 • e, f, 5, 2</formula><p>Once all the dependencies are calculated, in Algorithm 8, Sp-RST looks for the maximum value of the dependency among all the computed γ(AllComb (K) , D). The maximum dependency reflects the baseline value for the feature selection task. The output is the following:</p><formula xml:id="formula_24">• 5</formula><p>In a next step, LSH-dRST performs a filtering process to only keep the set of all combinations, which have the same dependency degrees, as the already selected dependency baseline value (M axDependency = 5), i.e., γ(AllComb (K) , D) = M axDependency = 5. By applying Algorithm 9, the following output is obtained:</p><formula xml:id="formula_25">• f, 5, 2 • e, f, 5, 2</formula><p>In fact, through these computations, the algorithm removes in each level the unnecessary attributes that may negatively influence the performance of any learning algorithm. At a final stage, by using the results generated from the previous step and applying Algorithm 10, LSH-dRST looks for the minimum number of features among all the Size (AllComb (K) ) . Once determined (minN bF = 1), the algorithm only keeps the set of combinations having the same minimum number of features as minN bF . The filtered selected features define the reduct set and describe all concepts in the initial Cl s=1 (K) training data set. The output of Algorithm 10, which presents the reduct (RED s (K, D)) for Cl s=1 (K), is the following:</p><formula xml:id="formula_26">• f</formula><p>The same calculations will be applied to the rest of Cl (Cl s=2 (k), Cl s=3 (k) and Cl s=4 (k)). At this stage, different reducts -Cl s=2 (k): RED = {d}, Cl s=3 (k): RED = {a, b}, Cl s=4 (k): RED = {c} -are generated from the different B partitions and from the different Cl s . With respect to Algorithm 1, line 15, a union of the obtained results is required to represent the initial big information table T RDD (C), i.e., Table <ref type="table" target="#tab_0">1</ref>. The final output is Reduct = {a, b, c, d, f }. LSH-dRST could reduce the big information table presented in Table <ref type="table" target="#tab_0">1</ref> from T RDD (C) to T RDD (Reduct). The output is presented in Table <ref type="table">6</ref>.</p><formula xml:id="formula_27">x ∈ U a b c d f X x 1 1 2 4 0 2 1 x 2 0 3 3 2 3 2 x 3 2 3 1 3 1 2 x 4 1 1 2 1 3 2 x 5 0 2 0 1 4 1 x 6 1 1 2 4 1 2 x 7 2 2 1 3 4 2 x 8 1 2 0 2 4 1</formula><p>Table <ref type="table">6</ref>. Reduct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setup</head><p>The main aim of our experimentation is to demonstrate that our proposed approach LSH-dRST preserves data dependency within the same generated buckets and within the distributed environment. We will show that by using a more intelligent partitioning of the universe, via the use of LSH, a more reliable process of gathering similar data instances based on their feature values can be reached; and hence better classification results can be obtained. Indeed, we will show that LSH-dRST is not only scalable but also more reliable for feature selection, making it more relevant to big data pre-processing. We, therefore, investigate different parameters of LSH-dRST and analyze how these affect execution time and stability of the feature selection, and hence data dependency. The LSH-dRST algorithm is implemented in Scala 2.11 within the Spark 2.1.1 framework. Based on the experiments conducted in <ref type="bibr" target="#b20">[11]</ref>, a maximum of 10 features per sub-information table Cl is used that can be processed by LSH-dRST. We therefore perform experiments for 2, 5, 10, 25 and 50 buckets (B), in Algorithm 1, each comprising sub-information tables of 4, 5, 8 and 10 features (F ), where F refers to the parameter K in Algorithm 1. For instance, for bucket (B = 2) and for a number of 4 features (F = 4) per sub-information table the algorithm generates 1250 Cl. We run all settings on 1, 2, 4, 8, and 16 nodes on the Grid5000 testbed <ref type="foot" target="#foot_12">21</ref> , a large-scale testbed for experiment-driven research. Within this testbed, we used dual 8 core Intel Xeon E5-2630v3 CPUs and 128 GB memory. Since the study does not require a scalable version of the classifiers, these experiments are run on a standard laptop configuration with a 3.1 GHz Dual-Core Intel Core i7 CPU, 16 GB 1867 MHz DDR3 RAM, SSD storage, 64-bit, macOS Catalina.</p><p>Our analysis first focuses on the number of features selected and the scalability of our algorithm. We evaluate the performance of LSH-dRST using the speedup, sizeup, and scaleup criteria introduced in <ref type="bibr" target="#b53">[45]</ref> (see Section 6.2).</p><p>We then show that the improvement in performance does not decrease the feature selection ability by performing model evaluation using a Naive Bayes and a Random Forest classifier on the original data set, the reduced data sets produced by LSH-dRST and some other feature selection techniques (see Section 6.3). We use the scikit-learn Random Forest implementation <ref type="foot" target="#foot_13">22</ref> with the following parameters: n_estimators = 1000, n_jobs = -1, and oob_score = True. A Stratified 10-Folds cross-validator <ref type="foot" target="#foot_14">23</ref> is used for all our conducted experiments. Moreover, we use the Naive Bayes implementation from Weka 3.8.2 <ref type="foot" target="#foot_15">24</ref> , again with 10-fold cross-validation.</p><p>LSH-dRST makes use of randomization in several places, e.g., LSH uses random projections, the construction of the sub-information tables starts with a randomly selected feature, and we select one reduct among the generated family of reducts randomly. For this reason, we always perform multiple runs of the algorithm and report appropriate statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Benchmark</head><p>To validate the efficiency of the LSH-dRST algorithm we require a data set with a large number of attributes as the advantage of the data partitioning scheme and the fact of looking on data dependencies via LSH will become more pronounced for data sets with a large set of features. Therefore, and for the sake of comparison with the standard distributed RST version <ref type="bibr" target="#b20">[11]</ref>, we chose the Amazon Commerce reviews data set from the UCI machine learning repository <ref type="bibr" target="#b54">[46]</ref>. This choice is based on the fact that this data set was the one with the largest number of features that still had a sufficiently large number of data items and as it was used in <ref type="bibr" target="#b20">[11]</ref>. This data set was derived from customer reviews on the Amazon commerce website by identifying a set of most active users and with the goal to perform authorship identification. The database includes 1 500 data items described through 10 000 features (linguistic style such punctuation, length of words, sentences, etc.) and 50 distinct classes (authors). Instances are identically distributed across the different classes, i. e., for each class there are 30 items.</p><p>We have discretized the data using the Equal-Width Intervals method. It separates all possible values into k intervals of the same width where width is defined as (max valuemin value)/k. We have used k = 10 in our experiments. Note that the number of distinct values for the conditional attributes in the data ranges from 3 (e. g., it+is_an) to 298 (e. g., _).</p><p>We demonstrate the scalability of our approach by considering subsets of this data set in terms of attributes. To be more precise, we have created five additional data sets by randomly choosing 1 000, 2 000, 4 000, 6 000, and 8 000 out of the original 10 000 attributes. We use these sets to evaluate our proposed method as discussed in Section 6.2 and refer to them as Amazon1000, Amazon2000, . . . , Amazon10000 in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation Metrics</head><p>To evaluate the scalability of the parallel LSH-dRST, we consider the standard metrics, which are the speedup, the scaleup, and the sizeup from literature <ref type="bibr" target="#b53">[45]</ref>. These are defined as follows:</p><p>• For the speedup, we keep the size of the data set constant (where size is measured by the number of features, i. e., we use the original data set with 10 000 features) and increase the number of nodes. For a system with m nodes, the speedup is defined as: Speedup(m) = runtime on one node runtime on m nodes An ideal parallel algorithm has linear speedup: the algorithm using m nodes solves the problem in the order of m times faster than the same algorithm using a single node. However, this is difficult to achieve in practice due to startup and communication cost as well as interference and skew <ref type="bibr" target="#b53">[45]</ref>, which may lead to a sub-linear speedup.</p><p>• The sizeup keeps the number of nodes constant and measures how much the runtime increases as the data set is increased by a factor of m:</p><p>Sizeup(m) = runtime for data set of size m • s runtime for baseline data set of size s To measure the sizeup, we use the smaller databases described in Section 6.1. We use 1 000 features as a baseline and consider 2 000, 4 000, 6 000, 8 000, and 10 000 features, respectively. A parallel algorithm with a linear sizeup has a very good sizeup performance: considering a problem that is m times larger than a baseline problem, the algorithm requires in the order of m times more runtime for the larger problem.</p><p>• The scaleup evaluates the ability to increase the number of nodes and the size of the data set simultaneously:</p><p>Scaleup(m) = runtime for data set of size s on 1 node runtime for data set of size s • m on m nodes Again, we use the sub-data set with 1 000 features as a baseline. Here, a scaleup of 1 implies 'linear' scaleup, which similarly to linear speedup is difficult to achieve.</p><p>For the model evaluation, we use the standard measures, which are the accuracy, precision, recall, and F1 score to compare the quality of the LSH-dRST selected feature set with other feature selection methods. Note that these metrics are formally defined for binary classification and thus, need to be adjusted to be used with the Amazon dataset, which has 30 classes.</p><p>In the following we provide the formal definitions for the binary case. For the case with more than two classes we use a standard weighted average approach. This approach first calculates the metrics for each class separately. Afterwards it determines the weighted average over all classes using the number of true instances for each class as weights. This has the advantage to take potential class imbalances into account.</p><p>The metrics definitions for the binary case are as follows (where TP: True positive, TN: True negative, FP: False positive, and FN: False negative):</p><p>• Precision: measures the ratio of correctly predicted positive observations to the total predicted positive observations, and is defined as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Other Feature Selection Techniques</head><p>We compare LSH-dRST with a number of other feature selection techniques from Weka 3.8.2 25   • CfsSubsetEval: considers the individual predictive ability of each feature along with the degree of redundancy between them</p><p>• ChiSquaredAttributeEval: computes the value of the chi-squared statistic with respect to the class • CVAttributeEval: first creates a ranking of attributes based on the Variation value, then divides into two groups, last using Verification method to select the best group</p><p>• GainRatioAttributeEval: measures the gain ratio with respect to the class 25 https://www.cs.waikato.ac.nz/~ml/weka</p><p>• InfoGainAttributeEval: measures the information gain with respect to the class</p><p>• ReliefFAttributeEval: repeatedly samples an instance and considers the value of the given attribute for the nearest instance of the same and different class</p><p>• SignificanceAttributeEval: computes the Probabilistic Significance as a two-way function (attributeclasses and classes-attribute association)</p><p>• SymmetricalUncertAttributeEval: evaluates the symmetrical uncertainty with respect to the class and Smile<ref type="foot" target="#foot_16">26</ref> </p><p>• SumSquaresRatio (AttributeEval): measures the ratio of between-groups to within-groups sum of squares</p><p>These methods include both, attribute and subset evaluation methods. For subset evaluation we use a 'Best First' greedy search method. For attribute evaluation, we need to either provide a threshold or a number of features to be selected. We set the number of features to be selected to a value comparable with LSH-dRST, i.e., the average number of features selected for each parameter setting of F and additionally use 0 as a threshold. We determine the sets of features selected by these methods and then perform model evaluation with a Random Forest and Naive Bayes classifier as discussed previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results and Analysis</head><p>In this section, we will discuss our results. We first consider the features selected by LSH-dRST (Section 7.1). After that we look into the runtime and scalability of LSH-dRST (Section 7.2). Finally, we investigate the quality of the feature selection and compare our results with the previously introduced algorithm creating random partitions (Sp-RST) and other feature selection techniques as discussed above (Section 7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Selected Features</head><p>We plot the number of features selected over 10 runs as boxplots and provide values for average and standard deviation in Figure <ref type="figure" target="#fig_14">5</ref>. We see that the number of features is very concentrated around its median, implying a low variance in the number of features selected. We select on average 3 145 features for F = 4, 2 555 for F = 5, 1 686 for F = 8, and 1 427 for F = 10. Again, the number of buckets hardly has any impact. For Sp-RST the results reported in <ref type="bibr" target="#b20">[11]</ref> were much more erratic with no clear tendency based on the parameter setting and numbers ranging between 1 600 and 6 200. We will use these observed numbers to parameterize other feature selection techniques when performing our model evaluation.</p><p>In a second step, we have a closer look at the features selected by LSH-dRST. For this, we perform 5 independent runs for each parameter setting and look at the number of times each of the 10 000 q q q q q q q q q q q F=4,B=2  features is selected. We plot the results in Figure <ref type="figure" target="#fig_10">6</ref> and see that only very few features are selected in every run while a considerable number of features is never or only once selected. This is a very different picture from our original algorithm Sp-RST where features were typically selected in all or none of the runs. It is also interesting to note that the number of buckets B has hardly any influence on this result-the shape of the curves in Figure <ref type="figure" target="#fig_10">6</ref> is mostly determined by the value of F . We therefore only provide additional averages for all values of F in Figure <ref type="figure" target="#fig_10">6</ref>. We emphasize that selecting different features in different runs is not a weakness of our proposed method. As pointed out in Section 3.2 more than one reduct may exist and selecting an arbitrary one among these is appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Runtime and Scalability</head><p>As discussed in the previous section, we consider standard measures for parallel algorithms to analyze the scalability of our approach, i. e., speedup (Section 7.2.1), sizeup (Section 7.2.2), and scaleup (Section 7.2.3). In addition, we consider the runtime of LSH-dRST in more detail, e. g., how the runtime is split between the LSH and the RST phase. We provide the actual runtimes for the different phases in Tables <ref type="table" target="#tab_0">11</ref> and<ref type="table" target="#tab_0">12</ref> in the appendix. The total runtimes for different numbers of nodes and the datasets described in Section 6.1 can be found in Tables <ref type="table">7,</ref><ref type="table">8</ref>, 9 and 10 in the appendix. Figures <ref type="figure">7</ref> and<ref type="figure">8</ref> visualize the runtimes for the different phases of LSH-dRST, both in terms of absolute and relative values. We observe that for some parameter settings the LSH part of LSH-dRST is more time-consuming than the RST part, but for others it is less time-consuming: For F = 10, the time for the LSH part of our algorithm is negligible for all settings of B and all numbers of nodes. For F = 8, this is only the case for more than 2 buckets (i.e., B = 2). For F = 4 and F = 5, the time taken for the LSH part is larger or equal to the time taken for RST in most cases. In general, we can observe that the share of the LSH part is large if more nodes are used, for smaller number of buckets B and for smaller values of F . We also observe that the runtime for the RST part generally peaks if two nodes are used and decreases for larger number of nodes. We conclude that the overhead caused by parallelization is not yet compensated for such a smaller number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.">Speedup Analysis</head><p>We plot the speedup in Figure <ref type="figure">9</ref> and the actual runtime for the six data sets discussed in Section 6.1 in Figure <ref type="figure" target="#fig_2">10</ref>. We see that the speedup for most parameter settings is very similar. However, setting F = 8 for the datasets with 8 000 and 10 000 features improves the speedup considerably-independently of the setting for B (where B = 5 and B = 10 yield the best overall results). Overall, we conclude that the number of buckets does not have a considerable influence on the speedup, but the number of subinformation tables F does. The latter is expected since the execution time grows exponentially with respect to the number of features and thus, using more nodes is more beneficial in cases with many features. It is therefore somewhat surprising that F = 10 does not exhibit a larger speedup. Note that an ideal parallel algorithm has linear speedup, which is, however, difficult to achieve in practice due to startup and communication cost as well as interference and skew <ref type="bibr" target="#b53">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.">Sizeup Analysis</head><p>To measure the sizeup we use 1 000 features as a baseline and consider 2 000, 4 000, 6 000, 8 000, and 10 000 features, respectively. We plot the sizeup for 1, 2, 4, 8, and 16 nodes in Figure <ref type="figure" target="#fig_2">11</ref> and the actual runtimes for different numbers of nodes in Figure <ref type="figure" target="#fig_8">12</ref>. We see that our method has sub-linear sizeup for most parameter settings if at least 4 nodes are used, i. e., for a 10-times larger data set it requires less than 10 times more time. The only two exceptions are F = 4 and F = 5 (i. e., small numbers of features) with only two buckets (B = 2). Results for 1 or 2 nodes look different, but it should be noted that these settings are less relevant in the context of parallel algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3.">Scaleup Analysis</head><p>We use the sub-data sets previously described with 1 000 features as a baseline and plot the results in Figure <ref type="figure" target="#fig_15">13</ref>. It should be noted that a scaleup of 1 implies linear scaleup, which similarly to linear speedup is difficult to achieve. Our scaleup is clearly smaller than 1 for all parameter settings, but fluctuates between 0.2 and 0.4 for most settings and 8 nodes, including the ones that exhibit the best speedup. The best scaleup is achieved for F = 5 and large values for B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Model Evaluation</head><p>We perform model evaluation with two classifiers, Random Forest and Naive Bayes as described previously. We present boxplots of the results for all metrics (accuracy, precision, recall, F1 score) </p><p>Amazon8000 </p><p>Figure <ref type="figure" target="#fig_2">10</ref>. Runtimes (in seconds) for the six data sets discussed in Section 6.1.  as well as runtimes in the main text. The corresponding raw data can be found in the appendix: Section 9.1 for runtimes and Sections 9.3 and 9.4 for classification metrics of Random Forest and Naive Bayes, respectively. As discussed in Section 6.3 some of the other feature selection methods have to be parameterized by either defining a threshold or the number of features to be selected. For these methods we have used five settings: a threshold of 0 (the standard setting) and four different numbers of features. The latter numbers are motivated by the number of features selected by LSH-dRST with different settings of F (see Section 7.1), i. e., 1 427, 1 686, 2 555, and 3 145 features. The relevant parameter settings are displayed in brackets after the name of the corresponding method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1.">Random Forest Classifier</head><p>We show the results of 10 runs of random forest on the different reduced data sets in Figures <ref type="figure" target="#fig_9">14,</ref><ref type="figure" target="#fig_6">15</ref>, 16 and 17 (classification metrics) as well as Figure <ref type="figure" target="#fig_2">18</ref> (runtime). The corresponding raw data for classification metrics can be found in Tables 13, 14 and 15 in the appendix. The raw data for runtimes is displayed in Tables 7, 8, 9 and 10 in the appendix.</p><p>We observe that LSH-dRST has comparable performance to most other feature selection techniques and outperforms some of them. 27 The only exception is the SumSquaresRatio method, which outperforms all other methods for 1 686, 2 555, and 3 145 features. The overall best result is achieved by SumSquaresRatio with 3 145 features (0.9513 accuracy, 0.9534 precision, 0.9577 recall, and 0.9345 F1 score). The best parameter setting for LSH-dRST is F = 4 and B = 50: 0.7402 accuracy, 0.7626 precision, 0.7402 recall, 0.7326 F1 score.</p><p>We also see that the classification result is quite stable with respect to the parameter settings in LSH-dRST. The slightly better results are generally achieved for smaller values of F , with all results for F = 4 (independent of the value for B) outperforming all other parameter settings. We conclude  that for a Random Forest classifier small values of F are clearly preferable while the concrete value of B makes less of a difference.</p><p>Concerning the runtime we have to consider both, the runtime for the actual feature selection and the runtime for the Random Forest on the reduced dataset. In general, the runtime for the feature selection is faster for small values of F , with the best runtime achieved for F = 4 and B = 50. Note that this is independent of the number of nodes used and that this setting also achieved the best classification result. The runtime for the Random Forest classifier ranges from 109.4952 seconds (F = 8, B = 50) to 196.8976 seconds (F = 4, B = 25), with the fastest times obtained for large values of F where fewer features are selected. For F = 4 and B = 50 (the fast setting for the feature selection) the runtime is 156.7633, a medium value. Given the considerable increase in runtime for the actual feature selection for larger values of F , we conclude that small values of F should be preferred as the overall runtime (selection+classification) is smaller and the classification performance is better. F = 4 and B = 50 appears to be a good choice for LSH-dRST if used with a Random Forest classifier.</p><p>Looking at the other feature selection methods, the runtimes for the classifiers vary more drastically (40.6593 to 443.0512), with the fast time achieved for the ConsistencySubsetEval (40.6593 seconds). However, this method also takes considerably longer than any of the other methods used for comparison to perform the feature selection task (608 seconds) and performs worse in terms of classification (0.6493 accuracy, 0.6500 precision, 0.6493 recall, 0.6395 F1 score). The fastest methods in terms of feature selection are ReliefF with threshold 0 (1 second), ChiSquared (2-3 seconds) and SumSquaresRatio (2 seconds). Note that the latter one was also the best in terms of classification while the other two achieve results comparable to LSH-dRST.</p><p>Finally, in comparison to our previous method (Sp-RST), runtimes for the classification after using LSH-dRST appear to be smaller (for some parameters much smaller) with slightly worse classification results (accuracy 0.7402 vs 0.7912 in the best parameter settings). Sp-RST in general selects more features than LSH-dRST, which is a possible explanation for this behavior. In terms of runtime for the feature selection, the runtime for both methods varies largely with the parameterization and the number of nodes used. For Sp-RST, we observe that the slower the method, the better the classification. This does not seem to be the case for LSH-dRST where the faster parameters also yield the better classification results. The runtimes for the fastest parameterization for both algorithms are comparable (F = 4, B = 50 for LSH-dRST and 2500 for SP-RST). In this case we achieve a classification accuracy of 0.7402 (LSH-dRST) vs 0.7710 (Sp-RST) and runtimes of less than 75 seconds (depending on the number of nodes used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.">Naive Bayes Classifier</head><p>Again, we show the results of 10 runs of Naive Bayes on the different reduced data sets: Figures 14, 15, 16 and 17 (classification metrics) as well as Table <ref type="table" target="#tab_17">18</ref> (runtime). The corresponding raw data for classification metrics can be found in <ref type="bibr">Tables 16,</ref><ref type="bibr" target="#b26">17</ref> and 18. The raw data for runtimes is again displayed in Tables 7, 8, 9 and 10 in the appendix.</p><p>We first observe that the overall classification performance of the Naive Bayes classifier is worse than for the Random Forest classifier. While this observation is independent from the feature selection method used, the overall results for LSH-dRST are somewhat disappointing as our method is outper- formed by many of the other feature selection techniques. <ref type="foot" target="#foot_17">28</ref> However, for some of these methods, the performance very much depends on the parameterization while the performance for LSH-dRST is again quite stable. Choosing the right parameters can be difficult in practice, thus it is promising to see the robustness of our method. Investigating the concrete reasons for this behavior and improving the overall performance when used with other classifiers such as Naive Bayes is subject to future research.</p><p>In terms of runtime for the classifier 29 , we observe that all runtimes for Naive Bayes are much smaller than the ones for Random Forest, ranging from 0.924 seconds for ConsistencySubsetEval and 135.5330 seconds for SignificanceEval with threshold 0. The fastest parameter setting for LSH-dRST is F = 10 and B = 50 (19.1884 seconds). Again, consistency is the fastest in this respect, but as discussed previously it requires a long time for the feature selection process.</p><p>In terms of classification, the best results for LSH-dRST are achieved for F = 4 and B = 25 (0.5103 accuracy, 0.5486 precision, 0.5103 recall, 0.5149 F1 score). While this is slightly different than for Random Forest, where B = 50 performed best, we again observe that smaller values for F are generally better and outperform all other parameter settings. Thus, very similar conclusions as for the Random Forest classifier can be drawn with an overall recommended parameter setting F = 4 and B = 25 in this case.</p><p>The overall best classification result is again achieved by SumSquaresRatio. However, here, fewer features seem to be better with the best result achieved for 1 427 features: 0.6680 accuracy, 0.6862 precision, 0.6679 recall, 0.6689 F1 score. This parameter setting requires a runtime for the classifier of 88.5070, a medium value.</p><p>Finally, in comparison to our previous method (Sp-RST), again very similar conclusions can be drawn: Runtimes for the classification after using LSH-dRST appear to be smaller with slightly worse classification results. And while Sp-RST benefits from investing more computational power this is not necessarily the case for LSH-dRST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and Emerging Trends</head><p>With the emergence of big data concept, various disciplines of granular computing have recently received more attention by several researchers. Among these disciplines, which are involved in the foundations of granular computing and within the big data context, specifically when it comes to information and knowledge processing, we mention the theory of fuzzy sets and the theory of rough sets. The adaptation of these disciplines to the context of big data, by revising the different granular notions and concepts, presents a big challenge in granular computing.</p><p>In this paper, we have investigated the performance of the distributed rough set theory approach for feature selection. The proposed method, named LSH-dRST, is based on an Apache Spark distributed architecture, and integrates a hashing component, which is the Locality Sensitive Hashing algorithm (LSH). In this version, the granular concepts of rough set theory have been revised and adapted to the big data context and, accordingly, implemented in a distributed way. The main idea behind this version is to ensure a more intelligent way on how to partition the feature search space to guarantee data dependency, which is also considered as a big challenge in a distributed environment.</p><p>We have investigated the number of features selected for different parameters as well as the scalability of the approach. Finally, we have performed model evaluation using two classifiers, Random Forest and Naive Bayes.</p><p>From the conducted deep analysis of LSH-dRST and the obtained results, we can highlight the different benefits and the impact of using the locality sensitive hashing in our proposed solution. Being incorporated into LSH-dRST, LSH could partition the high dimensional feature search space in a more reliable and intelligent way, and hence it can better preserve data dependency in the distributed environment. Another important benefit is that such an application can achieve a lower computational cost.</p><p>As future work, we aim to study the performance of the proposed distributed rough set theory versions for feature selection when these are applied to more complex real-world problems with more features and data items than the Amazon data set. Possible sources for such data sets include Kaggle data-science platforms.</p><p>It is also interesting to mention that up to now all of the proposed rough set theory versions dedicated to deal with the big data context focus on a single dimension of the input data set, i.e., the feature search space, to create the partitions. Therefore, it would be interesting to investigate other ways of partitioning the input data matrix where both dimensions of the input data set can be considered.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>B ⊂ A be a subset of attributes. The indiscernibility relation, denoted by IN D(B), is the central concept of RST and is an equivalence relation, which is defined as: xIN D(B)y if and only if a(x) = a(y) for every a ∈ B, where a(x) denotes the value of feature a of object x. The family of all equivalence classes of IN D(B), referring to a partition of U determined by B, is denoted by U/IN D(B). Each equivalence class may be viewed as a granule consisting of indistinguishable elements. It is also referred to as an equivalence granule. The granulation structure induced by an equivalence relation is a partition of the universe. Each element in U/IN D(B) is a set of indiscernible objects with respect to B. The equivalence classes U/IN D(C) and U/IN D(D) are called condition and decision equivalence classes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>,</head><label></label><figDesc>where P OS C (D) = X∈U/IN D(D) C(X) is called the positive region of the partition U/IN D(D) with respect to condition attributes C. The positive region is a set of objects of U that can be uniquely classified to blocks of the partition U/IN D(D), by means of C. If k = 1 we say that D depends totally on C, and if k &lt; 1, we say that D depends partially (to degree k) on C. If k = 0 then the positive region of the partition U/IN D(D) with respect to C is empty. The coefficient k, the dependency degree, expresses the ratio of all elements of the universe, which can be properly classified to blocks of the partition U/IN D(D), employing attributes C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The process of the MapReduce framework.</figDesc><graphic coords="11,51.02,145.47,442.21,226.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The process of generating the B data blocks based on the B generated LSH buckets.</figDesc><graphic coords="14,51.02,116.23,442.22,311.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The process of generating the sub-information tables Cl for each T RDD (b) bucket based on the K nearest neighbors approach.</figDesc><graphic coords="15,144.57,116.23,255.12,212.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 : 3 : 4 :</head><label>134</label><figDesc>LSH-dRST Inputs: T RDD : information table with: D: decision class K: number of nearest neighbors B: number of buckets Output: Reduct1: Generate the B LSH buckets: T RDD (b) , where b ∈ [1, . . . , B] 2: Calculate IN D(D) For each T RDD (b) -where b ∈ [1, . . . , B] Generate the set S of sub-information tables Cl based on K: Cl s (K) -where s ∈ [1, . . . , S] 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>15 :</head><label>15</label><figDesc>Union all the generated reducts RED s (K, D) from all the Cl s from all the T RDD (b) : Reduct = B b=1 S s=1 RED s (K, D) 16: Return (Reduct) //*Reduct is irreducible based on step 14*// reducts are irreducible within their Cl s (K). Nevertheless, as each Cl s is defined using distinct features within different T RDD (b) feature search spaces and with respect to T RDD (b) = S s=1 Cl s (K), a union operation is applied to merge all the RED s (K, D) from all the Cl s and from all the T RDD (b) . This is defined as Reduct = B b=1 S s=1 RED s (K, D) (Algorithm 1, line 15). The generated Reduct represents the minimal reduced set that represents the initial T RDD . Reduct is indeed irreducible as it is based on the irreducible reducts RED s (K, D) within their Cl s (K) (see Algorithm 1, line 14).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A high level description of LSH-dRST.</figDesc><graphic coords="17,151.65,116.23,240.95,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 2 :</head><label>2</label><figDesc>Generate Buckets(B) Inputs: T RDD : information table B: number of buckets Output: T RDD (b) : buckets 1: Generate the hash table based on the feature size of T RDD 2: Map the T RDD 3: Apply the LSH projection for each vector 4: Sort the buckets by B that, the f oldByKey() 18 transformation operation is applied to merge and reduce all values of each key in the transformed RDD output. This is to represent the sought IN D(D): IN D(d i ). Algorithm 3 describes the pseudo-code of this distributed job.Algorithm 3: Calculate IN D(D) Input: T RDD Output: IN D(D): [d i , List of id i of x i ] 1: Map the T RDD based on its format (id i of x i , List of the features of x i , Class d i of x i ) and generate the new format as a key-value pair Class d i of x i , List of id i of x i 2: Merge/Reduce the values of each generated key using the f oldByKey() operation 3: Return IN D(D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 4 :</head><label>4</label><figDesc>Generate Cl s (K) Inputs: T RDD (b) : bucket K: number of features Outputs: S: set of the sub-information tables Cl s (K)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Algorithm 6 :</head><label>6</label><figDesc>Calculate IN D(AllComb (K) ) Inputs: Cl s (K): sub-information tables AllComb (K) : set of all possible combinations of K Output: IN D(AllComb (K) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Algorithm 7 :Algorithm 8 :</head><label>78</label><figDesc>Generate DEP (AllComb (K) ) Inputs: AllComb (K) : set of all possible combinations of K IN D(D): indiscernibility relation for the decision class D IN D(AllComb (K) ): indiscernibility relation for every AllComb (K) Outputs: γ(AllComb (K) , D): dependency degrees of the feature combinations AllComb (K) Size (AllComb (K) ) : size of the dependency degrees of the feature combinations AllComb (K) for each element AllComb (K) i in AllComb (K) do for each element IN D(d i ) in IN D(D) do for each element IN D(AllComb (K) ) i in IN D(AllComb (K) ) do Apply the intersection() transformation over IN D(d i ) and IN D(AllComb (K) ) i Get the length of the resulting intersection that we name as LengthIntersect If LengthIntersect = length of IN D(AllComb (K) ) i Score = LengthIntersect Else Score = 0 End if end Apply a reduce function over IN D(D) based on a sum() function on the calculated scores, which are based on the elements having the same IN D(d i ) end Apply a reduce function over AllComb (K) based on a sum() function on the calculated previous results, which are based on the elements having the same AllComb (K) i end Return AllComb (K) : γ(AllComb (K) , D), Size (AllComb (K) ) γ(AllComb (K) , D). Select DEP max (AllComb (K) ) Input: RDD[AllComb (K) , Size (AllComb (K) ) , γ(K, AllComb (K) )]: RDD presenting all AllComb (K) and their characteristics Output: M axDependency: baseline value for feature selection 1: Apply the max() function on the third argument of the given RDD: γ(AllComb (K) , D) 2: Return M axDependency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>•</head><label></label><figDesc>Recall: measures the ratio of correctly predicted positive observations to the all observations in actual class -yes, and is defined as: Recall = TP TP + FN • Accuracy: measures the ratio of correctly predicted observation to the total observations, and is defined as follows: Accuracy = TP + TN TP + FN + TN + FP • F1 score: is the weighted average of Precision and Recall. F1 score is defined as follows: F1 score = 2 * Recall * Precision Recall + Precision</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>•</head><label></label><figDesc>ConsistencySubsetEval: considers the level of consistency in the class values when the training instances are projected onto the subset of attributes • CorrelationAttributeEval: measures the correlation (Pearson's) between it and the class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The number of features in the reduced data set. The boxplot visualizes the results over 10 independent runs for all parameter settings. Average and standard deviation for these runs are listed in the table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Scaleup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Toy dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Runtime results for other feature selection techniques (Part 1).</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Selection</cell><cell>Random Forest</cell><cell>Naive Bayes</cell></row><row><cell>ReliefF (t=0)</cell><cell>7957 (-)</cell><cell>1 (-)</cell><cell>363.0645 (2.4074)</cell><cell>106.4700 (6.8651)</cell></row><row><cell>ReliefF (1427)</cell><cell>1427 (-)</cell><cell>8 (-)</cell><cell>104.0523 (0.6797)</cell><cell>17.0170 (0.7430)</cell></row><row><cell>ReliefF (1686)</cell><cell>1686 (-)</cell><cell>8 (-)</cell><cell>115.5863 (0.8737)</cell><cell>20.2280 (0.7300)</cell></row><row><cell>ReliefF (2555)</cell><cell>2555 (-)</cell><cell>8 (-)</cell><cell>150.6049 (1.1557)</cell><cell>31.5970 (0.9336)</cell></row><row><cell>ReliefF (3145)</cell><cell>3145 (-)</cell><cell>8 (-)</cell><cell>175.4633 (2.7690)</cell><cell>39.1670 (1.4395)</cell></row><row><cell>Significance (t=0)</cell><cell>10000 (-)</cell><cell>3 (-)</cell><cell>443.0512 (7.1975)</cell><cell>135.7860 (5.4555)</cell></row><row><cell>Significance (1427)</cell><cell>1427 (-)</cell><cell>3 (-)</cell><cell>93.4174 (0.7614)</cell><cell>19.8110 (0.7376)</cell></row><row><cell>Significance (1686)</cell><cell>1686 (-)</cell><cell>3 (-)</cell><cell>104.0673 (0.8551)</cell><cell>23.3070 (0.8137)</cell></row><row><cell>Significance (2555)</cell><cell>2555 (-)</cell><cell>3 (-)</cell><cell>141.2948 (1.0859)</cell><cell>35.9880 (1.3315)</cell></row><row><cell>Significance (3145)</cell><cell>3145 (-)</cell><cell>3 (-)</cell><cell>165.8484 (0.8897)</cell><cell>45.0320 (1.5426)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Random Forest: Classification results for different parameters of LSH-dRST.</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>Original</cell><cell>10000 (-)</cell><cell>0.7997 (0.0068)</cell><cell>0.8151 (0.0068)</cell><cell>0.7997 (0.0068)</cell><cell>0.7925 (0.0072)</cell></row><row><cell>CV (t=0)</cell><cell>4149 (-)</cell><cell>0.6792 (0.0055)</cell><cell>0.7245 (0.0054)</cell><cell>0.6792 (0.0055)</cell><cell>0.6705 (0.0051)</cell></row><row><cell>CV (1427)</cell><cell>1427 (-)</cell><cell>0.5325 (0.0074)</cell><cell>0.6084 (0.0119)</cell><cell>0.5325 (0.0074)</cell><cell>0.5266 (0.0082)</cell></row><row><cell>CV (1686)</cell><cell>1686 (-)</cell><cell>0.5526 (0.0060)</cell><cell>0.6153 (0.0100)</cell><cell>0.5526 (0.0060)</cell><cell>0.5425 (0.0073)</cell></row><row><cell>CV (2555)</cell><cell>2555 (-)</cell><cell>0.6090 (0.0061)</cell><cell>0.6714 (0.0073)</cell><cell>0.6090 (0.0061)</cell><cell>0.6023 (0.0063)</cell></row><row><cell>CV (3145)</cell><cell>3145 (-)</cell><cell>0.6386 (0.0043)</cell><cell>0.6886 (0.0065)</cell><cell>0.6386 (0.0043)</cell><cell>0.6288 (0.0042)</cell></row><row><cell>Cfs (Greedy)</cell><cell>41 (-)</cell><cell>0.6857 (0.0049)</cell><cell>0.6864 (0.0058)</cell><cell>0.6857 (0.0049)</cell><cell>0.6772 (0.0048)</cell></row><row><cell>Chi Squared (t=0)</cell><cell>113 (-)</cell><cell>0.7115 (0.0034)</cell><cell>0.7187 (0.0037)</cell><cell>0.7115 (0.0034)</cell><cell>0.7074 (0.0034)</cell></row><row><cell>Chi Squared (1427)</cell><cell>1427 (-)</cell><cell>0.7312 (0.0043)</cell><cell>0.7386 (0.0063)</cell><cell>0.7312 (0.0043)</cell><cell>0.7214 (0.0049)</cell></row><row><cell>Chi Squared (1686)</cell><cell>1686 (-)</cell><cell>0.7322 (0.0058)</cell><cell>0.7407 (0.0064)</cell><cell>0.7322 (0.0058)</cell><cell>0.7224 (0.0063)</cell></row><row><cell>Chi Squared (2555)</cell><cell>2555 (-)</cell><cell>0.7399 (0.0030)</cell><cell>0.7508 (0.0037)</cell><cell>0.7399 (0.0030)</cell><cell>0.7293 (0.0032)</cell></row><row><cell>Chi Squared (3145)</cell><cell>3145 (-)</cell><cell>0.7529 (0.0044)</cell><cell>0.7668 (0.0066)</cell><cell>0.7529 (0.0044)</cell><cell>0.7432 (0.0049)</cell></row><row><cell>Consistency (Greedy)</cell><cell>30 (-)</cell><cell>0.6493 (0.0060)</cell><cell>0.6500 (0.0069)</cell><cell>0.6493 (0.0060)</cell><cell>0.6395 (0.0060)</cell></row><row><cell>Correlation (t=0)</cell><cell>10000 (-)</cell><cell>0.7988 (0.0050)</cell><cell>0.8167 (0.0043)</cell><cell>0.7988 (0.0050)</cell><cell>0.7921 (0.0053)</cell></row><row><cell>Correlation (1427)</cell><cell>1427 (-)</cell><cell>0.7818 (0.0033)</cell><cell>0.7940 (0.0038)</cell><cell>0.7818 (0.0033)</cell><cell>0.7744 (0.0037)</cell></row><row><cell>Correlation (1686)</cell><cell>1686 (-)</cell><cell>0.7860 (0.0032)</cell><cell>0.7977 (0.0027)</cell><cell>0.7860 (0.0032)</cell><cell>0.7789 (0.0033)</cell></row><row><cell>Correlation (2555)</cell><cell>2555 (-)</cell><cell>0.7872 (0.0040)</cell><cell>0.8026 (0.0056)</cell><cell>0.7872 (0.0040)</cell><cell>0.7802 (0.0046)</cell></row><row><cell>Correlation (3145)</cell><cell>3145 (-)</cell><cell>0.7936 (0.0067)</cell><cell>0.8097 (0.0065)</cell><cell>0.7936 (0.0067)</cell><cell>0.7861 (0.0071)</cell></row><row><cell>Gain Ratio (t=0)</cell><cell>113 (-)</cell><cell>0.7061 (0.0045)</cell><cell>0.7141 (0.0037)</cell><cell>0.7061 (0.0045)</cell><cell>0.7020 (0.0043)</cell></row><row><cell>Gain Ratio (1427)</cell><cell>1427 (-)</cell><cell>0.7337 (0.0057)</cell><cell>0.7402 (0.0062)</cell><cell>0.7337 (0.0057)</cell><cell>0.7243 (0.0064)</cell></row><row><cell>Gain Ratio (1686)</cell><cell>1686 (-)</cell><cell>0.7343 (0.0045)</cell><cell>0.7418 (0.0051)</cell><cell>0.7343 (0.0045)</cell><cell>0.7243 (0.0049)</cell></row><row><cell>Gain Ratio (2555)</cell><cell>2555 (-)</cell><cell>0.7405 (0.0040)</cell><cell>0.7502 (0.0043)</cell><cell>0.7405 (0.0040)</cell><cell>0.7305 (0.0044)</cell></row><row><cell>Gain Ratio (3145)</cell><cell>3145 (-)</cell><cell>0.7543 (0.0037)</cell><cell>0.7675 (0.0040)</cell><cell>0.7543 (0.0037)</cell><cell>0.7450 (0.0038)</cell></row><row><cell>Info Gain (t=0)</cell><cell>113 (-)</cell><cell>0.7089 (0.0034)</cell><cell>0.7174 (0.0033)</cell><cell>0.7089 (0.0034)</cell><cell>0.7053 (0.0035)</cell></row><row><cell>Info Gain (1427)</cell><cell>1427 (-)</cell><cell>0.7335 (0.0046)</cell><cell>0.7404 (0.0041)</cell><cell>0.7335 (0.0046)</cell><cell>0.7240 (0.0042)</cell></row><row><cell>Info Gain (1686)</cell><cell>1686 (-)</cell><cell>0.7305 (0.0056)</cell><cell>0.7415 (0.0060)</cell><cell>0.7305 (0.0056)</cell><cell>0.7213 (0.0061)</cell></row><row><cell>Info Gain (2555)</cell><cell>2555 (-)</cell><cell>0.7408 (0.0048)</cell><cell>0.7502 (0.0063)</cell><cell>0.7408 (0.0048)</cell><cell>0.7308 (0.0057)</cell></row><row><cell>Info Gain (3145)</cell><cell>3145 (-)</cell><cell>0.7559 (0.0051)</cell><cell>0.7692 (0.0057)</cell><cell>0.7559 (0.0051)</cell><cell>0.7461 (0.0055)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Random Forest: Classification results for other feature selection techniques (Part 1).</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>Relief F (t=0)</cell><cell>7957 (-)</cell><cell>0.8005 (0.0042)</cell><cell>0.8174 (0.0046)</cell><cell>0.8005 (0.0042)</cell><cell>0.7941 (0.0045)</cell></row><row><cell>Relief F (1427)</cell><cell>1427 (-)</cell><cell>0.7813 (0.0028)</cell><cell>0.7955 (0.0033)</cell><cell>0.7813 (0.0028)</cell><cell>0.7735 (0.0029)</cell></row><row><cell>Relief F (1686)</cell><cell>1686 (-)</cell><cell>0.7889 (0.0050)</cell><cell>0.8005 (0.0064)</cell><cell>0.7889 (0.0050)</cell><cell>0.7816 (0.0061)</cell></row><row><cell>Relief F (2555)</cell><cell>2555 (-)</cell><cell>0.7909 (0.0058)</cell><cell>0.8065 (0.0058)</cell><cell>0.7909 (0.0058)</cell><cell>0.7844 (0.0063)</cell></row><row><cell>Relief F (3145)</cell><cell>3145 (-)</cell><cell>0.7944 (0.0045)</cell><cell>0.8103 (0.0049)</cell><cell>0.7944 (0.0045)</cell><cell>0.7878 (0.0049)</cell></row><row><cell>Significance (t=0)</cell><cell>10000 (-)</cell><cell>0.7988 (0.0067)</cell><cell>0.8159 (0.0073)</cell><cell>0.7988 (0.0067)</cell><cell>0.7914 (0.0079)</cell></row><row><cell>Significance (1427)</cell><cell>1427 (-)</cell><cell>0.7314 (0.0052)</cell><cell>0.7392 (0.0059)</cell><cell>0.7314 (0.0052)</cell><cell>0.7224 (0.0056)</cell></row><row><cell>Significance (1686)</cell><cell>1686 (-)</cell><cell>0.7330 (0.0038)</cell><cell>0.7421 (0.0046)</cell><cell>0.7330 (0.0038)</cell><cell>0.7238 (0.0039)</cell></row><row><cell>Significance (2555)</cell><cell>2555 (-)</cell><cell>0.7426 (0.0038)</cell><cell>0.7524 (0.0055)</cell><cell>0.7426 (0.0038)</cell><cell>0.7329 (0.0046)</cell></row><row><cell>Significance (3145)</cell><cell>3145 (-)</cell><cell>0.7538 (0.0046)</cell><cell>0.7675 (0.0047)</cell><cell>0.7538 (0.0046)</cell><cell>0.7442 (0.0052)</cell></row><row><cell>Sp-RST (1000)</cell><cell>6171.333 (26.01282)</cell><cell>0.7912 (0.0063)</cell><cell>0.8095 (0.0064)</cell><cell>0.7912 (0.0063)</cell><cell>0.7838 (0.0069)</cell></row><row><cell>Sp-RST (1250)</cell><cell>5566.000 (23.89142)</cell><cell>0.7902 (0.0077)</cell><cell>0.8059 (0.0079)</cell><cell>0.7902 (0.0077)</cell><cell>0.7826 (0.0083)</cell></row><row><cell>Sp-RST (2000)</cell><cell>4117.000 (18.95257)</cell><cell>0.7784 (0.0067)</cell><cell>0.7951 (0.0070)</cell><cell>0.7784 (0.0067)</cell><cell>0.7697 (0.0071)</cell></row><row><cell>Sp-RST (2500)</cell><cell>3205.333 (24.35296)</cell><cell>0.7710 (0.0065)</cell><cell>0.7869 (0.0070)</cell><cell>0.7710 (0.0065)</cell><cell>0.7627 (0.0070)</cell></row><row><cell>Sum Squares Ratio (1427)</cell><cell>1427 (-)</cell><cell>0.7139 (0.0022)</cell><cell>0.7310 (0.0028)</cell><cell>0.7251 (0.0023)</cell><cell>0.6582 (0.0031)</cell></row><row><cell>Sum Squares Ratio (1686)</cell><cell>1686 (-)</cell><cell>0.8719 (0.0008)</cell><cell>0.8865 (0.0016)</cell><cell>0.8866 (0.0098)</cell><cell>0.8338 (0.0089)</cell></row><row><cell>Sum Squares Ratio (2555)</cell><cell>2555 (-)</cell><cell>0.8585 (0.0004)</cell><cell>0.8219 (0.0249)</cell><cell>0.8642 (0.0005)</cell><cell>0.8025 (0.0010)</cell></row><row><cell>Sum Squares Ratio (3145)</cell><cell>3145 (-)</cell><cell>0.9513 (0.0000)</cell><cell>0.9534 (0.0000)</cell><cell>0.9577 (0.0000)</cell><cell>0.9345 (0.0000)</cell></row><row><cell>Symmetrical Uncert (t=0)</cell><cell>113 (-)</cell><cell>0.7099 (0.0036)</cell><cell>0.7168 (0.0032)</cell><cell>0.7099 (0.0036)</cell><cell>0.7055 (0.0033)</cell></row><row><cell>Symmetrical Uncert (1427)</cell><cell>1427 (-)</cell><cell>0.7309 (0.0041)</cell><cell>0.7383 (0.0051)</cell><cell>0.7309 (0.0041)</cell><cell>0.7215 (0.0041)</cell></row><row><cell>Symmetrical Uncert (1686)</cell><cell>1686 (-)</cell><cell>0.7345 (0.0060)</cell><cell>0.7418 (0.0062)</cell><cell>0.7345 (0.0060)</cell><cell>0.7250 (0.0066)</cell></row><row><cell>Symmetrical Uncert (2555)</cell><cell>2555 (-)</cell><cell>0.7425 (0.0039)</cell><cell>0.7521 (0.0035)</cell><cell>0.7425 (0.0039)</cell><cell>0.7325 (0.0043)</cell></row><row><cell>Symmetrical Uncert (3145)</cell><cell>3145 (-)</cell><cell>0.7533 (0.0049)</cell><cell>0.7657 (0.0068)</cell><cell>0.7533 (0.0049)</cell><cell>0.7432 (0.0049)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 .</head><label>15</label><figDesc>Random Forest: Classification results for other feature selection techniques (Part 2).9.4. Classification Results (Naive Bayes)</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>LSH (F=4,B=2)</cell><cell>3162.6 (20.93562)</cell><cell>0.5025 (0.0085)</cell><cell>0.5385 (0.0102)</cell><cell>0.5025 (0.0085)</cell><cell>0.5069 (0.0084)</cell></row><row><cell>LSH (F=4,B=5)</cell><cell>3153.4 (28.18333)</cell><cell>0.5037 (0.0099)</cell><cell>0.5413 (0.0114)</cell><cell>0.5037 (0.0099)</cell><cell>0.5087 (0.0104)</cell></row><row><cell>LSH (F=4,B=10)</cell><cell>3136.6 (20.59854)</cell><cell>0.5065 (0.0103)</cell><cell>0.5410 (0.0136)</cell><cell>0.5065 (0.0103)</cell><cell>0.5098 (0.0120)</cell></row><row><cell>LSH (F=4,B=25)</cell><cell>3129.6 (30.31171)</cell><cell>0.5103 (0.0070)</cell><cell>0.5486 (0.0100)</cell><cell>0.5103 (0.0070)</cell><cell>0.5149 (0.0078)</cell></row><row><cell>LSH (F=4,B=50)</cell><cell>3143.8 (27.93206)</cell><cell>0.5004 (0.0140)</cell><cell>0.5354 (0.0180)</cell><cell>0.5004 (0.0141)</cell><cell>0.5047 (0.0152)</cell></row><row><cell>LSH (F=5,B=2)</cell><cell>2562.6 (6.94982)</cell><cell>0.4770 (0.0108)</cell><cell>0.5132 (0.0096)</cell><cell>0.4769 (0.0108)</cell><cell>0.4810 (0.0105)</cell></row><row><cell>LSH (F=5,B=5)</cell><cell>2549.2 (27.28919)</cell><cell>0.4837 (0.0074)</cell><cell>0.5174 (0.0075)</cell><cell>0.4837 (0.0073)</cell><cell>0.4873 (0.0068)</cell></row><row><cell>LSH (F=5,B=10)</cell><cell>2579.0 (13.83835)</cell><cell>0.4804 (0.0070)</cell><cell>0.5156 (0.0085)</cell><cell>0.4804 (0.0070)</cell><cell>0.4845 (0.0071)</cell></row><row><cell>LSH (F=5,B=25)</cell><cell>2548.2 (17.93600)</cell><cell>0.4854 (0.0164)</cell><cell>0.5142 (0.0194)</cell><cell>0.4854 (0.0164)</cell><cell>0.4876 (0.0171)</cell></row><row><cell>LSH (F=5,B=50)</cell><cell>2535.4 (30.44339)</cell><cell>0.4807 (0.0132)</cell><cell>0.5134 (0.0144)</cell><cell>0.4808 (0.0132)</cell><cell>0.4839 (0.0132)</cell></row><row><cell>LSH (F=8,B=2)</cell><cell>1700.8 (10.32957)</cell><cell>0.4550 (0.0138)</cell><cell>0.4848 (0.0172)</cell><cell>0.4550 (0.0138)</cell><cell>0.4592 (0.0148)</cell></row><row><cell>LSH (F=8,B=5)</cell><cell>1709.8 (18.28114)</cell><cell>0.4496 (0.0078)</cell><cell>0.4794 (0.0092)</cell><cell>0.4496 (0.0077)</cell><cell>0.4535 (0.0077)</cell></row><row><cell>LSH (F=8,B=10)</cell><cell>1684.8 (14.09610)</cell><cell>0.4513 (0.0153)</cell><cell>0.4776 (0.0179)</cell><cell>0.4512 (0.0153)</cell><cell>0.4537 (0.0159)</cell></row><row><cell>LSH (F=8,B=25)</cell><cell>1673.0 (34.72751)</cell><cell>0.4538 (0.0144)</cell><cell>0.4835 (0.0153)</cell><cell>0.4539 (0.0144)</cell><cell>0.4580 (0.0142)</cell></row><row><cell>LSH (F=8,B=50)</cell><cell>1659.8 (12.11198)</cell><cell>0.4550 (0.0169)</cell><cell>0.4846 (0.0179)</cell><cell>0.4551 (0.0169)</cell><cell>0.4586 (0.0167)</cell></row><row><cell>LSH (F=10,B=2)</cell><cell>1452.2 (14.04279)</cell><cell>0.4538 (0.0080)</cell><cell>0.4797 (0.0113)</cell><cell>0.4537 (0.0080)</cell><cell>0.4564 (0.0086)</cell></row><row><cell>LSH (F=10,B=5)</cell><cell>1438.8 (27.55358)</cell><cell>0.4391 (0.0111)</cell><cell>0.4657 (0.0136)</cell><cell>0.4391 (0.0111)</cell><cell>0.4433 (0.0123)</cell></row><row><cell>LSH (F=10,B=10)</cell><cell>1423.6 (21.19670)</cell><cell>0.4427 (0.0112)</cell><cell>0.4678 (0.0105)</cell><cell>0.4426 (0.0111)</cell><cell>0.4449 (0.0105)</cell></row><row><cell>LSH (F=10,B=25)</cell><cell>1407.0 (16.00000)</cell><cell>0.4543 (0.0139)</cell><cell>0.4829 (0.0156)</cell><cell>0.4543 (0.0140)</cell><cell>0.4576 (0.0138)</cell></row><row><cell>LSH (F=10,B=50)</cell><cell>1414.2 (21.62637)</cell><cell>0.4443 (0.0120)</cell><cell>0.4660 (0.0116)</cell><cell>0.4442 (0.0120)</cell><cell>0.4467 (0.0120)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 .</head><label>16</label><figDesc>Naive Bayes: Classification results for different parameters of LSH-dRST.</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>Original</cell><cell>10000 (-)</cell><cell>0.6099 (0.0080)</cell><cell>0.6451 (0.0082)</cell><cell>0.6098 (0.0080)</cell><cell>0.6118 (0.0087)</cell></row><row><cell>CV (t=0)</cell><cell>4149 (-)</cell><cell>0.5152 (0.0067)</cell><cell>0.5703 (0.0082)</cell><cell>0.5152 (0.0068)</cell><cell>0.5213 (0.0067)</cell></row><row><cell>CV (1427)</cell><cell>1427 (-)</cell><cell>0.3777 (0.0053)</cell><cell>0.4137 (0.0068)</cell><cell>0.3776 (0.0054)</cell><cell>0.3804 (0.0053)</cell></row><row><cell>CV (1686)</cell><cell>1686 (-)</cell><cell>0.3997 (0.0037)</cell><cell>0.4382 (0.0065)</cell><cell>0.3997 (0.0037)</cell><cell>0.4022 (0.0040)</cell></row><row><cell>CV (2555)</cell><cell>2555 (-)</cell><cell>0.4528 (0.0068)</cell><cell>0.5050 (0.0079)</cell><cell>0.4528 (0.0066)</cell><cell>0.4576 (0.0070)</cell></row><row><cell>CV (3145)</cell><cell>3145 (-)</cell><cell>0.4843 (0.0083)</cell><cell>0.5342 (0.0092)</cell><cell>0.4844 (0.0083)</cell><cell>0.4883 (0.0087)</cell></row><row><cell>Cfs (Greedy)</cell><cell>41 (-)</cell><cell>0.6083 (0.0039)</cell><cell>0.6156 (0.0049)</cell><cell>0.6084 (0.0039)</cell><cell>0.6053 (0.0043)</cell></row><row><cell>Chi Squared (t=0)</cell><cell>113 (-)</cell><cell>0.5771 (0.0039)</cell><cell>0.5870 (0.0055)</cell><cell>0.5771 (0.0038)</cell><cell>0.5705 (0.0043)</cell></row><row><cell>Chi Squared (1427)</cell><cell>1427 (-)</cell><cell>0.5670 (0.0053)</cell><cell>0.5880 (0.0059)</cell><cell>0.5670 (0.0053)</cell><cell>0.5703 (0.0055)</cell></row><row><cell>Chi Squared (1686)</cell><cell>1686 (-)</cell><cell>0.5609 (0.0065)</cell><cell>0.5782 (0.0071)</cell><cell>0.5610 (0.0066)</cell><cell>0.5628 (0.0067)</cell></row><row><cell>Chi Squared (2555)</cell><cell>2555 (-)</cell><cell>0.5303 (0.0045)</cell><cell>0.5484 (0.0040)</cell><cell>0.5302 (0.0045)</cell><cell>0.5317 (0.0045)</cell></row><row><cell>Chi Squared (3145)</cell><cell>3145 (-)</cell><cell>0.5027 (0.0043)</cell><cell>0.5261 (0.0057)</cell><cell>0.5027 (0.0041)</cell><cell>0.5049 (0.0046)</cell></row><row><cell>Consistency (Greedy)</cell><cell>30 (-)</cell><cell>0.5443 (0.0044)</cell><cell>0.5514 (0.0050)</cell><cell>0.5443 (0.0045)</cell><cell>0.5391 (0.0044)</cell></row><row><cell>Correlation (t=0)</cell><cell>10000 (-)</cell><cell>0.6147 (0.0059)</cell><cell>0.6512 (0.0063)</cell><cell>0.6147 (0.0059)</cell><cell>0.6170 (0.0069)</cell></row><row><cell>Correlation (1427)</cell><cell>1427 (-)</cell><cell>0.6409 (0.0056)</cell><cell>0.6614 (0.0056)</cell><cell>0.6409 (0.0057)</cell><cell>0.6420 (0.0057)</cell></row><row><cell>Correlation (1686)</cell><cell>1686 (-)</cell><cell>0.6363 (0.0058)</cell><cell>0.6647 (0.0049)</cell><cell>0.6363 (0.0058)</cell><cell>0.6401 (0.0055)</cell></row><row><cell>Correlation (2555)</cell><cell>2555 (-)</cell><cell>0.6211 (0.0059)</cell><cell>0.6553 (0.0069)</cell><cell>0.6211 (0.0058)</cell><cell>0.6254 (0.0061)</cell></row><row><cell>Correlation (3145)</cell><cell>3145 (-)</cell><cell>0.6239 (0.0061)</cell><cell>0.6593 (0.0067)</cell><cell>0.6240 (0.0061)</cell><cell>0.6275 (0.0065)</cell></row><row><cell>Gain Ratio (t=0)</cell><cell>113 (-)</cell><cell>0.5771 (0.0039)</cell><cell>0.5870 (0.0055)</cell><cell>0.5771 (0.0038)</cell><cell>0.5705 (0.0043)</cell></row><row><cell>Gain Ratio (1427)</cell><cell>1427 (-)</cell><cell>0.5670 (0.0053)</cell><cell>0.5880 (0.0059)</cell><cell>0.5670 (0.0053)</cell><cell>0.5703 (0.0055)</cell></row><row><cell>Gain Ratio (1686)</cell><cell>1686 (-)</cell><cell>0.5609 (0.0065)</cell><cell>0.5782 (0.0071)</cell><cell>0.5610 (0.0066)</cell><cell>0.5628 (0.0067)</cell></row><row><cell>Gain Ratio (2555)</cell><cell>2555 (-)</cell><cell>0.5303 (0.0045)</cell><cell>0.5484 (0.0040)</cell><cell>0.5302 (0.0045)</cell><cell>0.5317 (0.0045)</cell></row><row><cell>Gain Ratio (3145)</cell><cell>3145 (-)</cell><cell>0.5027 (0.0043)</cell><cell>0.5261 (0.0057)</cell><cell>0.5027 (0.0041)</cell><cell>0.5049 (0.0046)</cell></row><row><cell>Info Gain (t=0)</cell><cell>113 (-)</cell><cell>0.5771 (0.0039)</cell><cell>0.5870 (0.0055)</cell><cell>0.5771 (0.0038)</cell><cell>0.5705 (0.0043)</cell></row><row><cell>Info Gain (1427)</cell><cell>1427 (-)</cell><cell>0.5670 (0.0053)</cell><cell>0.5880 (0.0059)</cell><cell>0.5670 (0.0053)</cell><cell>0.5703 (0.0055)</cell></row><row><cell>Info Gain (1686)</cell><cell>1686 (-)</cell><cell>0.5609 (0.0065)</cell><cell>0.5782 (0.0071)</cell><cell>0.5610 (0.0066)</cell><cell>0.5628 (0.0067)</cell></row><row><cell>Info Gain (2555)</cell><cell>2555 (-)</cell><cell>0.5303 (0.0045)</cell><cell>0.5484 (0.0040)</cell><cell>0.5302 (0.0045)</cell><cell>0.5317 (0.0045)</cell></row><row><cell>Info Gain (3145)</cell><cell>3145 (-)</cell><cell>0.5027 (0.0043)</cell><cell>0.5261 (0.0057)</cell><cell>0.5027 (0.0041)</cell><cell>0.5049 (0.0046)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17 .</head><label>17</label><figDesc>Naive Bayes: Classification results for other feature selection techniques (Part 1).</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>ReliefF (t=0)</cell><cell>7957 (-)</cell><cell>0.6087 (0.0061)</cell><cell>0.6476 (0.0069)</cell><cell>0.6087 (0.0060)</cell><cell>0.6120 (0.0067)</cell></row><row><cell>ReliefF (1427)</cell><cell>1427 (-)</cell><cell>0.6231 (0.0061)</cell><cell>0.6475 (0.0055)</cell><cell>0.6231 (0.0060)</cell><cell>0.6253 (0.0060)</cell></row><row><cell>ReliefF (1686)</cell><cell>1686 (-)</cell><cell>0.6199 (0.0055)</cell><cell>0.6506 (0.0057)</cell><cell>0.6198 (0.0055)</cell><cell>0.6234 (0.0060)</cell></row><row><cell>ReliefF (2555)</cell><cell>2555 (-)</cell><cell>0.6093 (0.0071)</cell><cell>0.6409 (0.0077)</cell><cell>0.6093 (0.0071)</cell><cell>0.6118 (0.0074)</cell></row><row><cell>ReliefF (3145)</cell><cell>3145 (-)</cell><cell>0.6004 (0.0073)</cell><cell>0.6367 (0.0069)</cell><cell>0.6005 (0.0071)</cell><cell>0.6046 (0.0074)</cell></row><row><cell>Significance (t=0)</cell><cell>10000 (-)</cell><cell>0.6139 (0.0065)</cell><cell>0.6475 (0.0065)</cell><cell>0.6138 (0.0066)</cell><cell>0.6151 (0.0068)</cell></row><row><cell>Significance (1427)</cell><cell>1427 (-)</cell><cell>0.5670 (0.0053)</cell><cell>0.5880 (0.0059)</cell><cell>0.5670 (0.0053)</cell><cell>0.5703 (0.0055)</cell></row><row><cell>Significance (1686)</cell><cell>1686 (-)</cell><cell>0.5609 (0.0065)</cell><cell>0.5782 (0.0071)</cell><cell>0.5610 (0.0066)</cell><cell>0.5628 (0.0067)</cell></row><row><cell>Significance (2555)</cell><cell>2555 (-)</cell><cell>0.5303 (0.0045)</cell><cell>0.5484 (0.0040)</cell><cell>0.5302 (0.0045)</cell><cell>0.5317 (0.0045)</cell></row><row><cell>Significance (3145)</cell><cell>3145 (-)</cell><cell>0.5027 (0.0043)</cell><cell>0.5261 (0.0057)</cell><cell>0.5027 (0.0041)</cell><cell>0.5049 (0.0046)</cell></row><row><cell>Sp-RST (1000)</cell><cell>6171.333 (26.01282)</cell><cell>0.5934 (0.0067)</cell><cell>0.6321 (0.0065)</cell><cell>0.5934 (0.0067)</cell><cell>0.5966 (0.0069)</cell></row><row><cell>Sp-RST (1250)</cell><cell>5566.000 (23.89142)</cell><cell>0.5893 (0.0066)</cell><cell>0.6304 (0.0067)</cell><cell>0.5893 (0.0066)</cell><cell>0.5929 (0.0067)</cell></row><row><cell>Sp-RST (2000)</cell><cell>4117.000 (18.95257)</cell><cell>0.5736 (0.0058)</cell><cell>0.6156 (0.0071)</cell><cell>0.5736 (0.0058)</cell><cell>0.5772 (0.0061)</cell></row><row><cell>Sp-RST (2500)</cell><cell>3205.333 (24.35296)</cell><cell>0.5634 (0.0061)</cell><cell>0.6050 (0.0073)</cell><cell>0.5635 (0.0061)</cell><cell>0.5681 (0.0068)</cell></row><row><cell>SumSquaresRatio (1427)</cell><cell>1427 (-)</cell><cell>0.6680 (0.0018)</cell><cell>0.6862 (0.0032)</cell><cell>0.6679 (0.0017)</cell><cell>0.6689 (0.0021)</cell></row><row><cell>SumSquaresRatio (1686)</cell><cell>1686 (-)</cell><cell>0.6667 (0.0032)</cell><cell>0.6857 (0.0047)</cell><cell>0.6666 (0.0032)</cell><cell>0.6675 (0.0038)</cell></row><row><cell>SumSquaresRatio (2555)</cell><cell>2555 (-)</cell><cell>0.6420 (0.0051)</cell><cell>0.6680 (0.0059)</cell><cell>0.6420 (0.0051)</cell><cell>0.6439 (0.0054)</cell></row><row><cell>SumSquaresRatio (3145)</cell><cell>3145 (-)</cell><cell>0.6480 (0.0037)</cell><cell>0.6726 (0.0032)</cell><cell>0.6480 (0.0038)</cell><cell>0.6493 (0.0039)</cell></row><row><cell>Symmetrical Uncert (t=0)</cell><cell>113 (-)</cell><cell>0.5771 (0.0039)</cell><cell>0.5870 (0.0055)</cell><cell>0.5771 (0.0038)</cell><cell>0.5705 (0.0043)</cell></row><row><cell>Symmetrical Uncert (1427)</cell><cell>1427 (-)</cell><cell>0.5670 (0.0053)</cell><cell>0.5880 (0.0059)</cell><cell>0.5670 (0.0053)</cell><cell>0.5703 (0.0055)</cell></row><row><cell>Symmetrical Uncert (1686)</cell><cell>1686 (-)</cell><cell>0.5609 (0.0065)</cell><cell>0.5782 (0.0071)</cell><cell>0.5610 (0.0066)</cell><cell>0.5628 (0.0067)</cell></row><row><cell>Symmetrical Uncert (2555)</cell><cell>2555 (-)</cell><cell>0.5303 (0.0045)</cell><cell>0.5484 (0.0040)</cell><cell>0.5302 (0.0045)</cell><cell>0.5317 (0.0045)</cell></row><row><cell>Symmetrical Uncert (3145)</cell><cell>3145 (-)</cell><cell>0.5027 (0.0043)</cell><cell>0.5261 (0.0057)</cell><cell>0.5027 (0.0041)</cell><cell>0.5049 (0.0046)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 .</head><label>18</label><figDesc>Naive Bayes: Classification results for other feature selection techniques (Part 2).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://spark.apache.org/docs/2.2.0/ml-features.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/triguero/MR-EFS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/sramirez/spark-infotheoretic-feature-selection</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/sramirez/fast-mRMR</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://hadoop.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://storm.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://samza.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://spark.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://flink.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_9"><p>https://spark.apache.org/docs/0.7.3/api/core/spark/PairRDDFunctions.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_10"><p>https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/rdd/RDD.html# mapPartitionsWithIndex(scala.Function2,%20boolean,%20scala.reflect.ClassTag)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_11"><p>https://spark.apache.org/docs/latest/rdd-programming-guide.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_12"><p>https://www.grid5000.fr/mediawiki/index.php/Grid5000:Home</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_13"><p>http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier. html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_14"><p>http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold. html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_15"><p>http://weka.sourceforge.net/doc.dev/weka/classifiers/bayes/NaiveBayes.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_16"><p>https://haifengl.github.io/smile/feature.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_17"><p>Again all observations hold for all evaluation metrics used, i.e., accuracy, recall, precision, and F1 score.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This work is part of a project that has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Skłodowska-Curie</rs> grant agreement No <rs type="grantNumber">702527</rs>.</p><p><rs type="person">Z. Chelly Dagdia</rs>, <rs type="person">C. Zarges / LSH</rs>-RST for an Efficient Big Data Pre-processing liable way; hence better preserving data dependency in the distributed environment and ensuring a lower computational cost.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kKsR6vn">
					<idno type="grant-number">702527</idno>
					<orgName type="grant-name">Marie Skłodowska-Curie</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>://spark.apache.org/sql/ 11 https://spark.apache.org/streaming/ 12 https://spark.apache.org/mllib/ 13 https://spark.apache.org/graphx/ 14 https://mapr.com/blog/installation-guide-rhadoop-mapr-now-available/assets/rhadoop_and_mapr.pdf 15 http://spark.apache.org/docs/latest/api/python/ 16 https://www</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Runtime (in seconds) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Appendix</head><p>We provide more detailed experimental results in the appendix for the sake of completeness. We particularly include tables with numeric runtime and classification results. In the tables, we highlight the best results (smallest runtime, highest classification metric) in bold. We do so separately for the best parameter setting of LSH-dRST and other feature selection techniques. Whenever applicable, tables display averages and standard deviation (in brackets) over the runs performed in our experiments.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Original LSH-dRST (F=10, B=10) LSH-dRST (F=10, B=2) LSH-dRST (F=10, B=25) LSH-dRST (F=10, B=5) LSH-dRST (F=10, B=50) LSH-dRST (F=4, B=10) LSH-dRST (F=4, B=2) LSH-dRST (F=4, B=25) LSH-dRST (F=4, B=5) LSH-dRST (F=4, B=50) LSH-dRST (F=5, B=10) LSH-dRST (F=5, B=2) LSH-dRST (F=5, B=25) LSH-dRST (F=5, B=5) -dRST (F=5, B=50)</title>
		<imprint>
			<biblScope unit="page" from="0" to="100" />
		</imprint>
	</monogr>
	<note>-dRST (F=8, B=10) -dRST (F=8, B=2-dRST (F=8, B=25-dRST (F=8, -dRST (F=8, B=50</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Sp-RST</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4 0</biblScope>
			<date type="published" when="1250">1250. 2000. 2500</date>
			<publisher>Sp-RST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">7 Original LSH-dRST (F=10, B=10) LSH-dRST (F=10, B=2) LSH-dRST (F=10, B=25) LSH-dRST (F=10, B=5) LSH-dRST (F=10, B=50) LSH-dRST (F=4, B=10) LSH-dRST (F=4, B=2) LSH-dRST (F=4, B=25) LSH-dRST (F=4, B=5) LSH-dRST (F=4, B=50) LSH-dRST (F=5, B=10) LSH-dRST (F=5, B=2) LSH-dRST (F=5, B=25) LSH-dRST (F=5, B=5) LSH-dRST (F=5, B=50) -dRST (F=8</title>
		<imprint/>
	</monogr>
	<note>B=10) -dRST (F=8, B=2-dRST (F=8, B=25-dRST (F=8, -dRST (F=8, B=50</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Sp-RST</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4 0</biblScope>
			<date type="published" when="1250">1250. 2000. 2500</date>
			<publisher>Sp-RST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m">7 Original LSH-dRST (F=10, B=10) LSH-dRST (F=10, B=2) LSH-dRST (F=10, B=25) LSH-dRST (F=10, B=5) LSH-dRST (F=10, B=50) LSH-dRST (F=4, B=10) LSH-dRST (F=4, B=2) LSH-dRST (F=4, B=25) LSH-dRST (F=4, B=5) LSH-dRST (F=4, B=50) LSH-dRST (F=5, B=10) LSH-dRST (F=5, B=2) LSH-dRST (F=5, B=25) LSH-dRST (F=5, B=5) LSH-dRST (F=5, B=50) -dRST (F=8</title>
		<imprint/>
	</monogr>
	<note>B=10) -dRST (F=8, B=2-dRST (F=8, B=25-dRST (F=8, -dRST (F=8, B=50</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Sp-RST</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4 0</biblScope>
			<date type="published" when="1250">1250. 2000. 2500</date>
			<publisher>Sp-RST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">7 Original LSH-dRST (F=10, B=10) LSH-dRST (F=10, B=2) LSH-dRST (F=10, B=25) LSH-dRST (F=10, B=5) LSH-dRST (F=10, B=50) LSH-dRST (F=4, B=10) LSH-dRST (F=4, B=2) LSH-dRST (F=4, B=25) LSH-dRST (F=4, B=5) LSH-dRST (F=4, B=50) LSH-dRST (F=5, B=10) LSH-dRST (F=5, B=2) LSH-dRST (F=5, B=25) LSH-dRST (F=5, B=5) LSH-dRST (F=5, B=50) -dRST (F=8</title>
		<imprint/>
	</monogr>
	<note>B=10) -dRST (F=8, B=2-dRST (F=8, B=25-dRST (F=8, -dRST (F=8, B=50</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Sp-RST</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4 0</biblScope>
			<date type="published" when="1250">1250. 2000. 2500</date>
			<publisher>Sp-RST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m">7 Original LSH-dRST (F=10, B=10) LSH-dRST (F=10, B=2) LSH-dRST (F=10, B=25) LSH-dRST (F=10, B=5) LSH-dRST (F=10, B=50) LSH-dRST (F=4, B=10) LSH-dRST (F=4, B=2) LSH-dRST (F=4, B=25) LSH-dRST (F=4, B=5) LSH-dRST (F=4, B=50) LSH-dRST (F=5, B=10) LSH-dRST (F=5, B=2) LSH-dRST (F=5, B=25) LSH-dRST (F=5, B=5) LSH-dRST (F=5, B=50) -dRST (F=8</title>
		<imprint/>
	</monogr>
	<note>B=10) -dRST (F=8, B=2-dRST (F=8, B=25-dRST (F=8, -dRST (F=8, B=50</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Original LSH-dRST (F=10, B=10) LSH-dRST (F=10, B=2) LSH-dRST (F=10, B=25) LSH-dRST (F=10, B=5) LSH-dRST (F=10, B=50) LSH-dRST (F=4, B=10) LSH-dRST (F=4, B=2) LSH-dRST (F=4, B=25) LSH-dRST (F=4, B=5) LSH-dRST (F=4, B=50) LSH-dRST (F=5, B=10) -dRST (F=5, B=2) -dRST (F=5, B=25</title>
		<imprint>
			<publisher>Sp-RST</publisher>
			<date type="published" when="2000">2000. 2500</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="50" to="100" />
		</imprint>
	</monogr>
	<note>-dRST (F=5, B=5-dRST (F=5, B=50-dRST (F=8, B=10-dRST (F=8, B=2-dRST (F=8</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Handbook of granular computing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kreinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Granular computing: basic issues and possible solutions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th joint conference on information sciences</title>
		<meeting>the 5th joint conference on information sciences</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="186" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data mining, rough sets and granular computing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Workshop on Rough Set Theory and Granular Computingâ ȂŤ Summary</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inuiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Japanese Society for Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="239" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy sets and systems</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="127" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An overview on the roles of fuzzy set techniques in big data processing: Trends, challenges and opportunities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="15" to="30" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fuzzy-set based sentiment analysis of big social data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mukkamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vatrapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 18th International Enterprise Distributed Object Computing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel rough set based knowledge acquisition using MapReduce from big data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications</title>
		<meeting>the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rough set theory as a data mining technique: A case study in epidemiology and cancer incidence prediction</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Dagdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zarges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Micalef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Galiana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>De Fresnoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benchoufi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="440" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Rough Set Based Feature Selection Approach to Analyse Deep and Hand-crafted Features for Mammography Mass Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hamidinekoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Dagdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zwiggelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A distributed rough set theory based algorithm for an efficient big data pre-processing under the spark framework</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Dagdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zarges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lebbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="911" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nouveau Modèle de Sélection de Caractéristiques basé sur la Théorie des Ensembles Approximatifs pour les Données Massives: Méthode de sélection de caractéristiques pour les données massives</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Dagdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zarges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lebbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conférence Internationalle sur l&apos;Extraction et la Gestion des Connaissances</title>
		<imprint>
			<biblScope unit="page" from="377" to="378" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rough set data analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Düntsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gediga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="281" to="301" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Scalable and Effective Rough Set Theory based Approach for Big Data Pre-processing</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Dagdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zarges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lebbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On resilient feature selection: Computational foundations of rC-reducts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grzegorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ślęzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">499</biblScope>
			<biblScope unit="page" from="25" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality reduction based on rough set theory: A review</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thangavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pethalakshmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="2074" to="2081" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A revisit of hashing algorithms for approximate nearest neighbor search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07545</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Distributed Rough Set Theory Algorithm based on Locality Sensitive Hashing for an Efficient Big Data Pre-processing</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Dagdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zarges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azzag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lebbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2597" to="2606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature selection: An ever evolving frontier in data mining</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Setiono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Feature Selection in Data Mining</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Manipulating data and dimension reduction methods: feature selection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Complexity</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1790" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the scalability of feature selection methods on high-dimensional data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bolón-Canedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rego-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peteiro-Barral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonso-Betanzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guijarro-Berdiñas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sánchez-Maroño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MapReduce: a flexible data processing tool</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discovering outlying aspects in large datasets</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1520" to="1555" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiple Bayesian discriminant functions for high-dimensional massive data classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="465" to="501" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Classification of high-dimensional evolving data streams via a resourceefficient online ensemble</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large scale distributed data science using apache spark</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2323" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evolutionary feature selection for big data classification: A mapreduce approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramírez-Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Problems in Engineering</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information criteria of max-dependency, maxrelevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Locality-sensitive Hashing Scheme Based on P-stable Distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
		<idno type="DOI">10.1145/997817.997857</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Annual Symposium on Computational Geometry, SCG &apos;04</title>
		<meeting>the Twentieth Annual Symposium on Computational Geometry, SCG &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Similarity Estimation Techniques from Rounding Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
		<idno type="DOI">10.1145/509907.509965</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thiryfourth Annual ACM Symposium on Theory of Computing, STOC &apos;02</title>
		<meeting>the Thiryfourth Annual ACM Symposium on Theory of Computing, STOC &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<idno type="DOI">10.1145/276698.276876</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, STOC &apos;98</title>
		<meeting>the Thirtieth Annual ACM Symposium on Theory of Computing, STOC &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the Resemblance and Containment of Documents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Compression and Complexity of Sequences 1997</title>
		<meeting>the Compression and Complexity of Sequences 1997<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>SEQUENCES &apos;97</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral Hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Neural Information Processing Systems, NIPS&apos;08</title>
		<meeting>the 21st International Conference on Neural Information Processing Systems, NIPS&apos;08<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scalable Similarity Search with Optimized Kernel Hashing</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/1835804.1835946</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;10</title>
		<meeting>the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1129" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Compact Hashing with Joint Optimization of Search Accuracy and Time</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2011.5995518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hashing for Similarity Search: A Survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">J</forename></persName>
		</author>
		<idno>CoRR, 2014. abs/1408.2927. 1408.2927</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rudiments of rough sets</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pawlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information sciences</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey of large scale data management approaches in cloud environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alomari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="336" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">MPI-the Complete Reference: the MPI core</title>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Big Data with Cloud Computing: an insight on the computing environment, MapReduce, and programming frameworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bawakid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benítez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="409" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A fast parallel clustering algorithm for large spatial databases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jäger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="263" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Challenges and opportunities with big data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Labrinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2032" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mining big data: current status, and forecast to the future</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM sIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Data mining methods for omics and knowledge of crude medicinal plants toward big data biology</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Afendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Darusman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kibinge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Horai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Altaf-Ul-Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Structural Biotechnology Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Data mining with big data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="107" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Big data: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Networks and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="209" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Discovering knowledge in data: an introduction to data mining</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Larose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Data Mining and Rough Set Theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Grzymala-Busse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ziarko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="108" to="109" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A distributed approach for graph mining in massive networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Talukder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1024" to="1052" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scalable time series classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Positive approximation: An accelerator for attribute reduction in rough set theory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="597" to="618" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scalable density-based clustering with quality guarantees using random projections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Hadoop: The definitive guide</title>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards ultrahigh dimensional feature selection for big data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1371" to="1429" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003-03">2003. Mar</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Feature selection for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent data analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Curse of dimensionality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="257" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Rough sets: Theoretical aspects of reasoning about data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pawlak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Feature selection based on rough sets and particle swarm optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="471" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rough set methods in feature selection and recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Swiniarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="833" to="849" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">An introduction to feature extraction</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Feature extraction</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning: proceedings of the eleventh international conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Random forests for multiclass classification: Random multinomial logit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prinzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Den Poel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert systems with Applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1721" to="1732" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Enhanced feature selection for biomarker discovery in LC-MS data using GP</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Computation (CEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="584" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Text feature selection using ant colony optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Aghdam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghasem-Aghaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Basiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert systems with applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6843" to="6853" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Self-adaptive differential evolution for feature selection in hyperspectral image data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1969" to="1977" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Unsupervised rough set classification using GAs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lingras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="228" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Rough set clustering for web mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lingras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 IEEE International Conference on</title>
		<meeting>the 2002 IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1039" to="1044" />
		</imprint>
	</monogr>
	<note>Fuzzy Systems</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Integrating sustainability into supplier selection with grey system and rough set methodologies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Production Economics</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="252" to="264" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Big Data Analytics with Spark: A Practitioner&apos;s Guide to Using Spark for Large Scale Data Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thiryfourth annual ACM symposium on Theory of computing</title>
		<meeting>the thiryfourth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Ferret: a toolkit for content-based similarity search of feature-rich data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2927</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
