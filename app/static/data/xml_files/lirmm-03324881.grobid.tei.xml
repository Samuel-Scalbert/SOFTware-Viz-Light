<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Workflow Provenance in the Lifecycle of Scientific Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Renan</forename><surname>Souza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonardo</forename><forename type="middle">G</forename><surname>Azevedo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vítor</forename><surname>Lourenço</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elton</forename><surname>Soares</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Thiago</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Brandão</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Civitarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emilio</forename><forename type="middle">Vital</forename><surname>Brazil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcio</forename><surname>Moreno</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Inria</orgName>
								<orgName type="institution" key="instit1">Univ. Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS &amp; LIRMM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vítor</forename><surname>Lourenc</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Brand Ão</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renato</forename><surname>Cerqueira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><forename type="middle">A S</forename><surname>Netto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Workflow Provenance in the Lifecycle of Scientific Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B2EED4B2DA727F7AFA88B4FEDD3857B9</idno>
					<idno type="DOI">10.1002/cpe.6544</idno>
					<note type="submission">Submitted on 24 Aug 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-23T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scientific Machine Learning</term>
					<term>Machine Learning Lifecycle</term>
					<term>Artificial Intelligence</term>
					<term>Data Science</term>
					<term>Provenance</term>
					<term>Lineage</term>
					<term>Reproducibility</term>
					<term>Explainability</term>
					<term>Scientific Workflow</term>
					<term>Data lake</term>
					<term>e-Science</term>
					<term>Design Principles</term>
					<term>Taxonomy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Machine Learning (ML) has been fundamentally transforming several industries and businesses in numerous ways. More recently, it has also been impacting computational science and engineering domains, such as geoscience, climate science, material science, and health science. Scientific ML, i.e., ML applied to these domains, is characterized by the combination of data-driven techniques with domain-specific data and knowledge to obtain models of physical phenomena <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Obtaining models in scientific ML works similarly to conducting traditional large-scale computational experiments <ref type="bibr" target="#b5">[6]</ref>, which involve a team of scientists and engineers that formulate hypotheses, design the experiment and predefine parameters and input datasets, analyze the experiment data, do observations, and calibrate initial assumptions in a cycle until they are satisfied with the results. Scientific ML is naturally large-scale because multiple people collaborate in a project, using their multidisciplinary domain-specific knowledge to design and perform data-intensive tasks to curate (i.e., understand, clean, enrich with observations) datasets and prepare for learning algorithms. They then plan and execute compute-intensive tasks for computational simulations or training ML models affected by the scientific domain's constraints. They utilize specialized scientific software tools running either on their desktops, on cloud clusters (e.g., Docker-based), or large HPC machines.</p><p>Other works propose an ML lifecycle <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Although they might apply for scientific ML, in our view, there are still gaps in these lifecycle proposals to properly address scientific ML characteristics, particularly the need for deeper integration with scientific domain data and specialized knowledge on a domain. Our proposed model for the lifecycle of scientific ML has three phases (explained in detail later in this paper): data curation -to curate raw data; learning data preparation -to prepare the curated data for learning; and the learning itself -aware of the constraints of a scientific domain. In each of these phases, there may be multiple workflows. Each workflow is a set of chained data transformations consuming and producing datasets, and a workflow may consume the datasets produced by another workflow. For instance, there may be multiple workflows only in the learning data preparation phase to transform curated data into learning datasets. These datasets may then be consumed by multiple workflows in the learning phase, transforming the datasets into different ML models. Therefore, we propose modeling these workflows as multiple interconnected workflows <ref type="bibr" target="#b8">[9]</ref>. From now on, we refer to workflows as these multiple interconnected workflows in all phases of the lifecycle of scientific ML.</p><p>Our primary goal in this paper is to support this lifecycle by enabling scientists and engineers to perform comprehensive, i.e., end-to-end data analyses that integrate the data consumed and generated in these workflows, from raw domain data to learned models. The importance of these data analyses is that they are enablers to meet critical requirements in ML, such as model reproducibility and explainability, and experiment data understanding.</p><p>The main problem to achieve this goal is to deal, in an integrated and comprehensive way, with the high heterogeneity of different contexts (e.g., data, software, environments, persona) involved in this lifecycle. For example, the analyses need to be aware of the (hyper)parametrization of different data transformations in various workflows, how the transformations affect the experiment results (e.g., quality of the ML models), and the relationships between parameters, results, and domain-specific data and knowledge. For instance, one may ask: "what happened to the model performance when the parameters varied from X to Y when the datasets had a specific characteristic in the domain?". To allow for such analyses, tracking how the data are transformed throughout the workflows in an integrated and holistic way is necessary. Not having such holistic integration is critical for several reasons. To exemplify, it compromises experiment reproducibility from a scientific perspective. From a business perspective, stakeholders may be less likely to apply an ML model, even with the best performance, if they do not understand the transformations that led to the best model) <ref type="bibr" target="#b9">[10]</ref>.</p><p>Provenance (also referred to as lineage) data management techniques help reproduce, trace, assess, understand, and explain data, models, and their transformation processes <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The provenance research community has evolved significantly in recent years to provide for several strategic capabilities, including experiment reproducibility <ref type="bibr" target="#b13">[14]</ref>, user steering (i.e., runtime monitoring, interactive data analysis, runtime fine-tuning) <ref type="bibr" target="#b14">[15]</ref>, raw data analysis <ref type="bibr" target="#b15">[16]</ref>, and our previous work, which helps data integration for multiple workflows generating data in a data lake <ref type="bibr" target="#b8">[9]</ref>. Furthermore, other works contribute to support provenance tracking specifically for ML workflows <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>, including reproducible models and explainability <ref type="bibr" target="#b19">[20]</ref>. These related works are essential building blocks to be leveraged towards supporting the lifecycle.</p><p>Nevertheless, scientists and engineers still face difficulties in performing comprehensive data analyses that would help them meet those critical requirements in ML. Tracking provenance in those workflows could be used as a tool to provide for a holistic view, hence enabling the data analyses. However, the problem caused by the high heterogeneity in the lifecycle arises several challenges. For example, the workflows are highly heterogeneous and with distributed execution control: there may not be one single Workflow Management System (WMS) orchestrating all workflows; instead, there may be multiple WMSs, scripts, programs, and ML and data processing frameworks without a single unified execution orchestrator. Further, these workflows manage domain-and ML-specific data and knowledge stored in various distributed data stores and run on various execution environments. Hence, strategies to track data in multiple data stores are needed. Also, another complicating factor is that efficiency is a common requirement, especially in HPC executions. Thus the systems supporting the lifecycle need to scale and not add significant tracking overhead. Designing a system to efficiently track provenance in such heterogeneous scenarios has been recently acknowledged as a research challenge by leading data management researchers <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this paper, our focus is to support the lifecycle of scientific ML by proposing an approach for comprehensive data analyses, addressing the problem of high heterogeneity of different contexts. Particularly, we contribute with:</p><p>(i) A comprehensive characterization of the lifecycle, from raw domain data to learned models passing through the processes that manipulate these data, and a taxonomy (detailing e.g., data, execution timing, and training timing classes) positioning the role of provenance analysis to support the lifecycle (Sec. 2); (ii) Data design decisions to build and query a provenance-based holistic data view to integrate data, processed by workflows in the lifecycle, aware of the heterogeneous dimensions and enable comprehensive analyses; and architecture decisions that guide how to build a provenance system to efficiently track and integrate data in distributed executions (Sec. 3); (iii) PROV-ML, a new provenance data representation for scientific ML leveraging W3C PROV <ref type="bibr" target="#b21">[22]</ref> and MLS <ref type="bibr" target="#b22">[23]</ref> (Sec. 4); (iv) Lessons learned after applying the design decisions in a system's implementation and evaluating it in a real case in the Oil &amp; Gas (O&amp;G) industry in a testbed with 3 environments, including an HPC cluster with 393 computing nodes and 946 GPUs. We found that the decisions enabled comprehensive queries with rich semantics about the application domain and ML while maintaining low tracking overhead (&lt;1%), near-linear scalability, and efficient query performance (over an order of magnitude compared with a provenance representation without PROV-ML for certain workloads) (Sec. 5) Finally, we present the related work in Section 6, and conclude in Section 7.</p><p>The major extensions related to our work published on IEEE WORKS@SC19 <ref type="bibr" target="#b23">[24]</ref> are as follows: (i) We detail the explanations of the decisions that drive the data and architectural design of a system to support data analyses in the lifecycle. These details better highlight the lessons learned so that other researchers and practitioners can learn from them; (ii) We refine and extend PROV-ML to better represent subparts of the learning process and to make the model clearer by using more accurate names for the concepts, refining descriptions, and changing a few structures of the representation;</p><p>(iii) We include a new set of experiments, which demonstrate that PROV-ML can yield query acceleration allowing better experience while using the system for data analyses; and (iv) We discuss how the proposed approach can be customized to different applications and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CHARACTERIZING THE LIFECYCLE OF SCIENTIFIC ML</head><p>Existing works describe an ML lifecycle <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, but such descriptions focus on business domains and do not address the high heterogeneity problem of the lifecycle of scientific ML. Since our main goal is to support this lifecycle by enabling scientists and engineers to perform comprehensive data analyses, we begin with a proposal of a model for this lifecycle and a thorough characterization. This is the first work that proposes a lifecycle focused on scientific ML to the best of our knowledge. We first characterize the personas and describe the lifecycle (Sec. 2.1), then we present our motivating use case (Sec. 2.2), and finally, we characterize the data analyses using provenance (Sec. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Lifecycle of Scientific ML</head><p>Multidisciplinary personas, with different skills in the domain and ML techniques, participate in the lifecycle phases. In our previous work, we presented a spectrum of expertise and personas in scientific ML <ref type="bibr" target="#b23">[24]</ref>, depicted in Figure <ref type="figure" target="#fig_0">1</ref> and briefly summarized here. The spectrum ranges from only scientific-domain (fully white on the left) to ML only (fully black on the right), with the following personas: (i) Domain scientists, who have in-depth knowledge of the domain data and use specialized tools to interpret, visualize, and clean the scientific data; (ii) Computational scientists and engineers, who have high computational skills, often with abilities to develop parallel scripts and execute them in HPC clusters; (iii) ML scientists and engineers, who have in-depth knowledge of statistics, ML algorithms, and software engineering. In an orthogonal sense, Provenance specialists design the provenance schema for applications and guide other users to add provenance capture hooks to the workflows.    Data curation. It is the most complex phase of the lifecycle, mainly because of the nature of the scientific data. Much manual and highly specialized work are performed by the users (primarily domain scientists) to achieve automated knowledge extraction from scientific data promoted by ML. There is a significant gap between raw scientific data and useful data for consumption (e.g., data to serve as input to train ML models). Datasets can be huge, typically containing geospatialtemporal data stored in scientific formats, like HDF5, NetCDF, SEG-Y. Specialized formats in scientific domains may require industry-specific software and domain-specific knowledge to inspect, visualize, and understand the data. In addition, users can use metadata and textual reports to annotate the data with extra domain-specific knowledge, without which would be nearly impossible to make the data useful for ML algorithms. Considering the heterogeneous nature of the data, "it is unreasonable to assume that data lives in a single source" (e.g., a single file system or DBMS) <ref type="bibr" target="#b9">[10]</ref>. For instance, raw files can be stored in file systems or cloud stores, domain-specific annotations can be stored in a Knowledge Base System (e.g., Triple Store) with domain ontologies, and curated data can be stored in a NoSQL DBMS. Then, computational scientists and engineers develop data-intensive scripts to clean, filter, and validate the data. Each of these steps inside the data curation phase is highly interactive, manual, and may execute independently. In other words, users may run different scripts to perform these phases, several times, in an ad-hoc way, in any order, and on different machines. These phases occur in a cycle, which stops when the users consider the data "curated". In the context of ML, it is ready to be transformed into learning data.</p><p>Learning data preparation. Model trainers select relevant parts of the curated data to be used for learning. For instance, if the ML task is to classify geological structures <ref type="bibr" target="#b4">[5]</ref>, seismic images will need to be correlated with seismic interpretation, creating labeled samples. After selecting the data, model designers develop scripts, typically using domain-specific libraries to manipulate the raw scientific data, to transform (e.g.,image cropping, quantization, scaling) the data into learning datasets. Due to data complexity, frequently, data need to be manually inspected before it can be used as input for the learning phase.</p><p>Learning. The learning contemplates training, validation, and evaluation. In this phase, model trainers select the input learning datasets, optionally they choose validation datasets, and choose learning parameters (e.g., in deep learning they can choose ranges of epochs and learning rates) that will be optimized. Trainers can use their domain knowledge to discard learning datasets that will unlikely provide good results. The learning process is compute-intensive, typically executed in an HPC machine. One single learning process often generates multiple learned models, among which one is chosen as the "best" depending on evaluation metrics (e.g., MSE, accuracy, or any other user-defined metric). Moreover, trainers need to monitor the learning process by, e.g., inspecting how the evaluation metrics are evolving while the learning process iterates. They can wait until completion or interrupt the learning process, change parameters, iteratively re-submit the learning until satisfied with results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivating Use Case: The Lifecycle of a Deep Learning Classifier in Geoscience</head><p>We explore a motivating use case in the O&amp;G industry to illustrate the classes of data analyses driven by data integration via provenance capture. Finding new reservoirs is a demanding task in the O&amp;G industry and involves a broad spectrum of actions, such as the interpretation of seismic surveys. These surveys are indirect measures of the earth subsurface that can be organized into slices (images). They cover hundreds of square kilometers and help to interpret the geology by identifying geological structures, like salt bodies, and find possible hydrocarbons accumulations. Processing seismic data imposes complex chained data transformations and can suffer from many problems, like noise and shadows (regions with low signal). Trying to automate such activity is of high interest in academia and industry and deep learning is a promising machine learning technique for this <ref type="bibr" target="#b4">[5]</ref>. However, the geological structures vary geographically, from point to point in the subsurface, imposing significant challenges to the ML algorithms. Thus, it requires specialized knowledge to prepare, clean, and understand the data processed in the workflows.</p><p>To cope with this, often different teams in an interdisciplinary group composed of geoscientists, computational scientists, engineers, statisticians, and others decompose the problem into parts so that each can address different facets of the problem. Nonetheless, each team has a preferred way to automate tasks and store data, and a team consumes data generated by another. Despite decomposing the problem into parts makes the problem feasible, it creates a new problem: how to analyze the data in an integrated way <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Therefore, a unified view over those multiple parts is required so users with various personas can use it to analyze the data. Table <ref type="table" target="#tab_2">1</ref> shows seven exemplary data analyses (via queries) that integrate the phases of the lifecycle. , what was the seismic data file used, along with its number of slices, related oil basin, and field?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q7</head><p>Considering only the learning workflows that used the learning dataset associated to a given range of seismic slices, list the minimum batch loss per model obtained in the learning stage, also listing the model's hyperparameters and evaluation measurements jointly with the hyperparameters and measurements for associated model obtained in the validation stage, ordered by the best learned models.</p><p>Although this use case is in the O&amp;G industry, we observe a similar demand in several other domains. For instance, designing ML algorithms to handle problems in other industries, such as health, high-energy physics, bioinformatics, and manufacturing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In these areas, the development of ML models typically requires complex computational experiments designed as multiple workflows, executed on different HPC clusters, and also involve collaboration among various experts. Therefore, being able to analyze the experiment data associated with the ML models through all lifecycle phases is essential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A Taxonomy of Data Analyses in the Lifecycle of Scientific ML using Provenance</head><p>Provenance data in workflows contain a structured record of the data derivation paths within chained data transformations and their parameterizations <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Provenance data are usually represented as a directed graph where vertices are instances of entities (data) or activities (the data transformations) or agents (e.g., users); and, edges are instances of relationships between vertices <ref type="bibr" target="#b21">[22]</ref>. Comprehensive data analysis using provenance has been used as an enabler for several key capabilities:</p><p>• Experiment reproducibility <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b11">[12]</ref> • AI explainability <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b8">[9]</ref>; • Experiment fine-tuning and what-if analyses <ref type="bibr" target="#b14">[15]</ref>; • Uncertainty quantification <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>;</p><p>• Hypothesis testing <ref type="bibr" target="#b5">[6]</ref>; and • Real-time monitoring, and interactive data analysis. <ref type="bibr" target="#b28">[29]</ref> Based on a literature analysis <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b24">[25]</ref> and on our own experience to leverage provenance to support workflows for scientific ML <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, we propose here a taxonomy (Fig. <ref type="figure">3</ref>) to classify workflow provenance analysis in support of ML, by considering three classes: data, execution timing, and training timing. Next, we characterize the data involved in the lifecycle. Data class includes domain-specific, machine learning, and execution Provenance data may be augmented with these data, increasing the scope of the analysis.</p><p>Domain-specific data are the main data processed in the data curation phase (Sec. 2.1). Approaches to add domain data into provenance analysis include, e.g., raw data extraction <ref type="bibr" target="#b33">[34]</ref> and utilization of domain-specific knowledge databases associated to provenance databases <ref type="bibr" target="#b8">[9]</ref>. For raw data extraction, quantities of interest are extracted from raw data files. For domain databases, domain scientists may provide relevant information and metadata about the raw data and store them in knowledge graphs.</p><p>Machine learning data include learning data and generated learned models, which are more related to the learning data preparation (e.g., Q1) and learning (e.g., Q2, Q3, Q7) phases (Fig. <ref type="figure" target="#fig_1">2</ref>). These queries exemplify that the parametrization within the data transformations and relevant metadata of the generated data is important for provenance analysis.</p><p>Execution data. Besides model performance metrics (e.g., accuracy), users need to assess workflow execution time and resource consumption. They need to inspect if a critical block in their workflow (e.g., one demanding high parallelism) is taking longer than usual or if other parts are consuming more memory than expected. For this, provenance systems can capture system performance metrics and timestamps (e.g., Q4). Metadata, such as data store metadata (e.g., host address), HPC cluster name, and nodes in use, can be captured and associated with the provenance of the data transformations for extended analysis.</p><p>Hybrid. These data can be combined. In Q5 and Q7, the analysis queries data processed in workflows in the learning data preparation and learning phases, whereas Q6 uses the same dataset to analyze the raw files curated in the data curation phase. Execution timing refers to if the analysis is done online, i.e., while at least a workflow is running, or offline.</p><p>Offline analysis. The typical use of offline provenance analysis is to support reproducibility and historical data understanding, e.g., understand the curation of raw files and relate with the ML models. For example, the queries Q1-Q7 can be executed offline.</p><p>Online analysis. Users can use online provenance analysis to monitor, debug or inspect the data transformations while they are still running (e.g., see the status, see how the intermediate results are evolving as the input parameters vary). The problem of adding low provenance data capture overhead is more challenging for provenance systems that allow for online analysis <ref type="bibr" target="#b8">[9]</ref>. Queries Q3-Q5 and Q7 exemplify queries that can be executed online, e.g., while a training process is running.</p><p>Training timing refers to whether the analysis performs intra-training-i.e., to inspect one training process, e.g., a training job running on an HPC cluster, or inter-training-i.e., analyses comprehending results of several training processes.</p><p>Intra-training. In an offline intra-training analysis, users are interested in understanding how well-trained models generated in a given training process perform. The queries Q1-Q7 can be executed either online or offline, but Q3 and Q4 are more likely to be performed as online intra-training analysis.</p><p>Inter-training. This analysis refers to comprehensive queries to understand multiple training processes, e.g., how each of them performed, which learning datasets were used, how the training processes were parameterized. It supports activities like Model Validation, Management, Training, and Design. Usually, they are used offline, but may also be performed online. Queries Q1-Q7 fit this class when analyzing multiple trained models generated in different training processes. Fig. <ref type="figure">3</ref>: A taxonomy for workflow provenance analysis of the lifecycle of scientific ML.</p><p>Further characterization. Other classes worth mentioning for provenance analysis are: data store-data are distributed onto multiple stores, like file systems, cloud stores (e.g., IBM Cloud Object Storage, AWS S3), Relational or NoSQL DBMSs <ref type="bibr" target="#b8">[9]</ref>; execution environment-where the workflows execute, such as HPC clusters, Kubernetes clusters, Standalone server; execution orchestration software-each workflow may be executed as a standalone script, or as a workflow in a WMS, or a composition of microservice calls, or as a pipeline in a data processing (e.g., Spark) and ML frameworks (e.g., Tensorflow); provenance data granularity-provenance of files (i.e., references to files consumed and generated in a script), functions calls (arguments and outputs), blocks of code, and stack traces <ref type="bibr" target="#b29">[30]</ref>; and provenance analysis direction-forward or backward: generally, forward queries analyze from raw scientific files or learning datasets to trained models (e.g., Q3-Q5, Q7), whereas backward queries analyze from trained models to learning datasets or raw files (e.g., Q1, Q2, Q6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROVENANCE IN THE LIFECYCLE OF SCIENTIFIC ML</head><p>This section presents the fundamental design decisions for effective and efficient management of workflow provenance data in the lifecycle of scientific ML to provide for comprehensive data analyses. Although some of these decisions, individually, have been presented in related works <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b33">[34]</ref>, together they compose the building blocks of our approach and we assemble them as one unified set and describe how they support the lifecycle. They are organized as: (i) Data Design (Sec. 3.1), which contains the decisions and key concepts that drive the contents of our holistic data view, whose a resulting artifact is PROV-ML, a new provenance data representation; and, (ii) System Design (Sec. 3.2), which contain the decisions that determine how the provenance data are captured in a scalable and portable manner, whose resulting artifact is a reference system architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Design</head><p>D1: Data Integration with a Holistic Data View. The primary design decision is that to be able to manage effectively (i.e., capture, integrate, store, and query) provenance data in the interconnected workflows in all lifecycle phases, a provenance system must implement techniques to provide for an integrated, unified, and holistic data view. Also, it has to be aware of the contexts of the data transformations in the multiple workflows that consume and generate these data, their (hyper)parameterization, and output values, where these transformations run, where the generated data are stored, who are the involved personas, and how they interact with the workflows. This design decision builds on a multi workflow data view concept proposed in our previous work <ref type="bibr" target="#b8">[9]</ref>. It extends it to support the lifecycle comprehensively, with specializations to address ML-specific data and knowledge related to domain-specific data and knowledge. Let us call this data view as the Provenance-based Holistic Data View of the Lifecycle of Scientific ML (MLHolView). The contents and the granularity of the MLHolView are driven by the relevant queries for a project, and the view can be materialized as the database that integrates data from several sources, while the workflows run <ref type="bibr" target="#b8">[9]</ref>. D2: Context-awareness using Knowledge Graphs: Domain, ML, and Hybrid environments and Data stores. Extending provenance with domain-specific data for data analysis has been explored before <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b14">[15]</ref>. However, in scientific ML, it is required to go a step further into the details of domain-specific knowledge, including how key domain concepts relate to each other. Thus, it is important to relate the data in the workflows with as much knowledge as possible available about the project's key concepts. To be able to integrate with domain-specific knowledge databases, one needs to design the workflows aware that files (or data in other data stores, like DBMSs or object stores) are associated with concepts defined elsewhere. Then the provenance system needs to provide the proper links between the file and the domain-specific concepts.</p><p>Similarly, the MLHolView needs ML-specific concepts and relationships. Although modeling ML-specific concepts could be seen as modeling data for a specific domain (in this case, ML would be the domain), ML, by itself, is a distinguished domain, which crosses many industries and scientific domains. Thus, the MLHolView should have a built-in ML-specific schema, tightly coupled with the rest of the provenance data schema, to provide ML-specific context to support the comprehensive analyses. In certain cases, such specialized schema modeling might be even helpful to accelerate queries that require them <ref type="bibr" target="#b37">[38]</ref>.</p><p>In addition to the domain-and ML-specific context awareness, since the workflows can be executed within heterogeneous frameworks, scripts, or WMSs and on heterogeneous environments, the MLHolView needs to be aware of such hybrid (i.e., heterogeneous) execution by containing the track of the execution environment and software, and associated metadata. These data and their relationships, with pointers to domain-specific knowledge graphs and large data stored in other stores, are all materialized using provenance data in the knowledge graph that forms the MLHolView. Figure <ref type="figure" target="#fig_2">4</ref> illustrates the MLHolView and its awareness of data coming from the ML phases and the dimensions of heterogeneity (illustrated as layers) it addresses: software, data, data stores, and infrastructure (execution environment). The figure also shows the kind of provenance analysis (top-left) and the key capabilities the MLHolView enables (top-right) (Sec. 2.3). D3: Provenance of Multiple Workflows on Data Lakes meets ML Provenance Following W3C Standards. To be able to implement the context-awareness for the domain, ML, and hybrid environments , the MLHolView needs a comprehensive data representation. Data lake provenance builds on workflow provenance to enable the awareness of the location of each data item generated by chained data transformations in a data lake, even if there are multiple data items dispersed in hybrid environments and data stores <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, making it a good alternative to address such heterogeneity of data, store, and environments. However, it is not enough to support the lifecycle, as it requires provenance of ML-specific data and learning processes. The provenance data community has significantly evolved in recent years, oftentimes leveraging the PROV <ref type="bibr" target="#b21">[22]</ref> family of documents, a W3C recommendation, making it a de facto standard that provides the building blocks, in terms of data representation, for any provenance-based approach, allowing for compatibility among different solutions <ref type="bibr" target="#b39">[40]</ref>. The PROV-Wf <ref type="bibr" target="#b40">[41]</ref> workflow provenance data representation and its derivatives <ref type="bibr" target="#b15">[16]</ref> have also been used and evolved by several initiatives <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Our previous work builds on W3C PROV and PROV-Wf to propose PROVLake, a first provenance data representation for workflows on data lakes <ref type="bibr" target="#b8">[9]</ref>. Concerning ML-specific data modeling, there is a W3C community group developing a data representation with specific ML vocabulary, the W3C ML Schema (MLS) <ref type="bibr" target="#b22">[23]</ref>. Therefore, this data design decision proposes that the data representation for the MLHolView should be comprehensive, with detailed semantics about the workflows, where they execute, the data they process, and where they are stored, combining and extending a data lake provenance representation with ML-specific data representation, following standards and reusing existing representations, such as W3C PROV, PROV-Wf, PROVLake, and MLS. D4: Keeping Prospection and Retrospection Related but Separated. <ref type="bibr">Davidson and</ref> Freire explain that prospective provenance captures the specification of a workflow, i.e., the recipe of which data transformations will be processed and their inputs and outputs. In contrast, retrospective provenance captures the data that was consumed and produced, along with a detailed execution log about the computational tasks and execution environment <ref type="bibr" target="#b24">[25]</ref>. The prospective provenance provides the abstraction layer to specify provenance analyses, often giving semantics to the retrospective provenance data generated during the workflows' execution. Also, there are cases that the provenance analysis uses only one kind of provenance data. Therefore managing both kinds of provenance data, and more importantly, with a strong connection between each related kind, is essential for the MLHolView, which should be reflected in the provenance data modeling. D5: Designing a Focused Conceptual Data Schema. To provide the specialized semantics needed by the MLHolView, we propose a conceptual data schema focusing on the key concepts identified by the characterization in Section 2. The concepts are driven by the lifecycle phases and the data they manipulate: the phases are illustrated with a gray background and the and four main kinds of data are illustrated in white background in the UML class diagram in Figure <ref type="figure" target="#fig_3">5</ref>.  On the four data concept classes, each instance represents one dataset, i.e., a set of data elements that combined form one meaningful set of data for a given application. As with any dataset, we may have a data schema that varies depending on the application; it may be further decomposed into several interrelated sub-datasets (or subconcepts for a given application), and there may be related metadata such as where it is physically stored and data sizes. Concerning the three phases' classes, each can be further decomposed into workflows with associated execution data. A Learning instance can be qualified into training, validation, and evaluation. With respect to relationships, each Data Curation instance consumes a Raw Domain Data instance and generates a Curated Domain Data instance. Then, each Curated Domain Data instance may be consumed by one or more Data Preparation instances, which in turn may generate one or multiple Curated Domain Data instances (i.e., a n:m relationship). For instance, a learning algorithm may require the preparation of well log data and seismic data, jointly, and thus two sets of Curated Domain Data would need to be related to the Data Preparation instance. Finally, each Data Preparation instance generates a Learning Data instance to be be consumed by one Learning process that generates one Model instance. Typically, during a learning phase, there are multiple Learning instances, each generating a Model instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Architecture Design</head><p>S1: Portable and Distributed Capture Control. As discussed, the workflows execute in highly distributed, heterogeneous environments processing data in heterogeneous data stores, executing within heterogeneous software and on heterogeneous environments. To address this distributed execution control, the provenance system should be portable with distributed capture control so that there may be multiple provenance data capturers spread out across the multiple workflows executing. To address the heterogeneity of how workflows are executed, the provenance system cannot be tightly coupled with a specific workflow tool, but rather it should be pluggable to any of these aforementioned heterogeneous ways of executing workflows. The distributed captured data are ultimately integrated into the unified MLHolView. S2: Specialized Microservices in a Distributed Architecture. In addition to the distributed capture control, designing a provenance system using a microservices architecture allows for the flexibility needed for large-scale deployments in hybrid environments. The provenance system can be decomposed into smaller, stateless microservices with specialized functions and, more importantly, it enables that components of the provenance system architecture are deployed wherever best fits for the workflow having provenance being captured. For instance, provenance capture components can be deployed geographically near (or inside) the machine where the workflow runs, to reduce latency caused by communication costs, and other heavy-weight provenance-specific processes (e.g., creating the linkages, inserting in the DBMS) and the DBMS itself can be deployed elsewhere, to reduce concurrency with the running workflows. A real deployment exploring the flexibility to place the architectural components to reduce communication costs and concurrency is shown in Section 5.1. S3: Strategies for a Scalable Capture. Since many of these workflows require HPC, the provenance capture system should not add significant performance penalties to the running workflows, requiring designing strategies for a scalable data capture. In addition to reducing concurrency, as described in S2, which is one of these strategies, other strategies to reduce performance overhead are as follows. During capture, the provenance persistence requests coming from the running workflows to the provenance system should be asynchronous and do not need to wait for complete processing, avoiding adding periods of waiting in the running workflow. Also, batches of data capture requests from the running workflows can be queued in the local memory of the host processing the workflow and sent to the provenance system at once, avoiding keeping open multiple communication channels between the running workflow and the provenance system. These batches are then received in the provenance system, which should process each request in the batch in a parallel manner, to reduce the time between the provenance capture in the workflow and the data to be readily available for queries in the MLHolView. Moreover, during capture, the provenance system responsible for creating the data linkages should avoid doing read operations to the underlying DBMS, but should only do appends to the data in the DBMS. This is because the read operations on the DBMS inevitably have to be waited for the query response, thus potentially increasing latency in the provenance capture. Finally, the only component that is in direct contact with a running workflow should be a lightweight provenance capture library shielding the workflows from possible slowness from other components. The key for such a lightweight library is to significantly reduce provenance-specific code in a workflow, consequently reducing provenance-specific calls during execution, and strictly follow the insert-only policy, so that no queries to the DBMS are made by the library, avoiding waits. Such provenance-specific descriptions, essential for the specification of the workflows, are stored as prospective provenance data externally to the actual workflow. The provenance library (in the client-side of the system) does not need these specifications, which are essential for the server-side of the system, so the linkages that form the MLHolView can be provided. A side-effect of reducing provenance-calls in a workflow is that it also reduces the changes needed to be done, making it look as similar as possible to the original workflows without the hooks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>. S4: Easing Data Linkage with Unique Data Identifiers. The concept of using unique identifiers is useful for keeping track of data in provenance systems <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Existing approaches keep track of data files consumed and produced in the workflows, and here we extend this concept to keep track of every data value that participates in the MLHolView, even scalar values. Thus, every attribute-value pair that are consumed or produced in any data transformation participating in any workflow receives a unique identifier. So, whenever an attribute-value generated by one data transformation is consumed by another, the provenance system can reuse the value keeping track of the paths between transformations and, thus, keeping the workflows interconnected. S5: Workflow Design and Adding the Provenance Capture Hooks. To enable the context-awareness (D2), the first step is to design the workflows with context awareness. For this, for each workflow in those multiple workflows for a given project, one needs to specify its data transformations with input datasets, parameters, and expected outputs. Each computational process (data transformations) and the datasets they transform are qualified according to the MLHolView's conceptual data schema (D5). When specifying data references, the physical location where the data reference is expected to be stored should be provided, as well as metadata about the execution environment where the workflow will execute. Finally, the relationships between the workflows and the data in the distributed data stores need to be specified. Such specification can be maintained in configuration files, which will inform the provenance capture system to enable it to create the linkages to provide the context-aware integration of domain, ML, and hybrid environments and stores using provenance. After the specification, hooks can be added to the workflows before and after each data transformation, informing the key concept (following the MLHolView's conceptual data schema) in each data transformation and data reference. A data transformation execution is encapsulated by a provenance capture task, which typically occurs in a function call, a program execution, a web service call, or an iteration in an iterative workflow. Reference System Architecture. Based on these system design decisions, our proposed reference architecture is illustrated in Figure <ref type="figure" target="#fig_4">6</ref> and is described as follows. There are M environments (e.g., HPC clusters, Kubernetes clusters) and N workflows in all phases of the lifecycle, distributed on these environments. Each workflow may use heterogeneous data stores and may be implemented as a standalone script, or as a workflow in a WMS, or a composition of microservice calls, or as a pipeline in a data processing or ML framework. Provenance capture hooks, through a lightweight ProvLib, are added to capture provenance data at each data transformation in each of these workflows. At the beginning and end of each (potentially parallel) data transformation executions for each (potentially parallel) workflow, a provenance capture event is emitted from the ProvLib. Thus a provenance capture event has the granularity of a data transformation execution, with their corresponding input data (at the beginning) and output data (at the end). These events are asynchronously sent to a Message Broker, such as Apache Kafka, or any lightweight repository that persists these events in a queue. Then, the ProvConsumer, which is a lightweight service that runs on the background, consumes from this queue and sends the requests to the ProvManager, which is aware of the prospective provenance data and can create the context-aware linkages using W3C PROV-based relationships and the reuse of unique identifiers (D4), and sends the data to the MLHolView, which is managed by a DBMS, typically a knowledge graph DBMS. The (Message Broker, ProvConsumer) pair is instantiated at each environment to reduce communication costs between the ProvLib. The ProvManager is a RESTful, stateless service and can receive provenance capture requests in any order. Thus it uses a lightweight Key Value DBMS (e.g., Redis) to manage state when needed (e.g., to create a link with a just received request with another request sent before). During the execution of these workflows, users or applications may submit provenance analysis through a Query API that communicates with the Prov query component, which is a RESTful service responsible for implementing query building strategies using the query language of the MLHolView's DBMS and returning the results to the requesting client. Analyzing Design Decisions Based on Existing Approaches. To the best of our knowledge, the design decisions D2 (integrating domain, ML, and hybrid environments and stores in a knowledge graph) and D3 (combining multi workflow, data lake, and ML provenance schemas) are new for general provenance management approaches. Regarding the decision D1 (holistic data view), existing approaches <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b33">[34]</ref> also propose provenance as a view over datasets, but cannot cope with data being generated by multiple workflows, such as the ones in the ML lifecycle; also, there is no ML-specific schema in their views. The decision D4 (prospective and retrospective provenance separation) is traditionally followed by most general provenance approaches, but it is not used in the existing approaches for ML provenance <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The system design decision S1 (portable and distributed control) is not often adopted by existing approaches <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>, as typically they propose "all-in-one" ML data management systems that require the workflows to be executed within such a platform and thus cannot address highly heterogeneous environments, parallel software, and data stores. The approaches that follow a more portable system design decision either cannot deal with multiple workflows <ref type="bibr" target="#b33">[34]</ref> or does not follow the decisions D2 and D3 <ref type="bibr" target="#b38">[39]</ref>. The other system design decisions S2-S5 can be found in existing provenance-based approaches, nevertheless since they do not follow the data design decisions we are proposing, particularly D2 and D3, they can only partially support the lifecycle of scientific ML. In summary, we only found approaches that partially follow some of these decisions, which is not enough to answer complex end-to-end queries that integrate all lifecycle phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROV-ML: A PROVENANCE DATA REPRESENTATION FOR THE LIFECYCLE OF SCIENTIFIC ML</head><p>In this section, we propose PROV-ML, the first generic provenance data representation for the lifecycle of scientific ML to the best of our knowledge. PROV-ML extends PROVLake <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b8">[9]</ref> as its underlying metamodel and employs elements of W3C ML Schema (MLS) <ref type="bibr" target="#b22">[23]</ref>. PROVLake is an extension of the W3C PROV data representation, specializing PROV for multiple workflows that process data in data lakes (see D3). However, as with other workflow provenance data representations, PROVLake alone lacks ML-specific semantics. PROV-ML bridges this gap between workflows and ML concepts by building on PROVLake and MLS representations. PROV-ML is depicted in Figure <ref type="figure" target="#fig_5">7</ref>, where the light-color classes represent prospective provenance, and dark-color, retrospective. PROV-ML provides rich semantics and details based on the conceptual data model of the lifecycle's fundamental concepts (D5), especially the ones in the learning phase.</p><p>The colors in the figure map to these concepts: the blue-shaded classes account for the Learning Data; the gray-shaded, for the Learning; and the yellow-shaded, for the Model. The stereotypes indicated in the figure represent the classes inherited from PROVLake, which has subclasses that extend W3C PROV classes. This is how W3C PROV recommends how it should be extended 1 . All classes illustrated in the figure are individually described in Table <ref type="table">2</ref>. We briefly discuss the 1. https://www.w3.org/TR/prov-dm/#section-prov-extended-mechanisms PROV-ML classes here, and further details on the classes and on how PROVLake classes are extended from PROV classes are available online <ref type="bibr" target="#b46">[47]</ref>. In PROV-ML, the Study class introduces a series of experiments, portrayed by the LearningExperiment class, which defines one of the three major phases in the lifecycle, the Learning phase. A learning experiment comprises a set of learning stages, represented by the BaseLearningStage class, which are the primary data transformation within the Learning phase and with whom the agent (Persona class) is associated. The BaseLearningStage is as an abstract class from which LearningStage and LearningStageSection classes inherit. Also, it relates the ML algorithm, evoked through Algorithm class, used in the stage might be defined in the context of a specific ML task (e.g., classification, regression), represented in the LearningTask class. This approach favors both the learning stage and learning stage section to conserve the relationships among other classes while grant them to have special characteristics discussed in the following. A learning stage varies regarding its type, i.e., Training, Validation, and Evaluation classes. The provision of a specific class for the learning stage allows the explicit representation of the relationship between the Learning Data Preparation phase, through its Learning Data, and the Learning phase of an ML lifecycle. The LearningStageSection class introduces the sectioning semantics that grant capabilities of referencing subparts of the learning stage and the data, respectively. An example of sectioning elements relevance is the ability to reference a specific epoch within a training stage, or mentioning a set of batches within a specific epoch. The Learning Data appears in the model over the LearningDataSetReference class. Another data transformation specified in PROV-ML is the Feature Extraction class, which represents the process that transforms the learning dataset into a set of features, represented by FeatureSet class. This modeling favors the ML experiment to be reproducible since it relates the dataset with the feature extraction process and the resulting feature set. TABLE 2: PROV-ML data representation classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Description Study</head><p>Investigation (e.g., research hypothesis) leading ML workflow definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LearningExperiment</head><p>The set of analyses (e.g., research questions) that drives the ML workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LearningProcessExecution</head><p>An ML workflow execution. This is equivalent to mls:Run and was renamed to explicitly preserve the aspects of retrospective provenance, which are not explicitly handled in MLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LearningTask and Learning-TaskValue</head><p>Defines the goal of a learning process, i.e., the ML task (e.g., LearningTask: Classification; LearningTaskValue: Seismic Stratigraphic Classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BaseLearningStage and Base-LearningStageExecution</head><p>Abstract classes of LearningStage and LearningStageSection, and their execution counterparts. It is used to conserve the relationships among other classes while granting them to have special characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Persona</head><p>The personas associated with the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LearningStage and Learn-ingStageExecution</head><p>Defines a stage in the learning process (Training or Validation or Evaluation) and its execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LearningStageSection and LearningSectionExecution</head><p>Introduces the sectioning semantics, i.e., capabilities for provenance of subparts of the learning stage and corresponding data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LearningDatasetReference and LearningDataset</head><p>Defines the dataset to be used by a LearningStage or LearningStageSection. In the last case, it is a section of a LearningDatasetReference. LearningDataset is the dataset used in the execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DatasetCharacteristic and DatasetCharacteristcValue</head><p>Defines metadata about the LearningDatasetReference (e.g., #instances), and DatasetCharacteristcValue relates with a LearningDataset (e.g., #instances =8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FeatureExtraction and Fea-tureExtractionExecution</head><p>Defines the prospective and retrospective feature retrieval process, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ModelSchema</head><p>The scope of the resulting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ModelProspection and Model</head><p>The resulting model a LearningStage or a LearningStageSection should generate, and the generated value (e.g., the trained model after the training stage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ModelHyperparameter and ModelHyperparameterValue</head><p>Hyperparameters a LearningStage or a LearningStageSection generate, and their values corresponding to the resulting model (e.g., the epoch which the resulting model was generated).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DataStoreInstance</head><p>Storage of the resulting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EvaluationMeasure and ModelEvaluation</head><p>A measure a LearningStage or a LearningStageSection should evaluate and the generated value (e.g., the precision of classifier model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EvaluationSpecification and EvaluationProcedure</head><p>Classes directly inherited from MLS, with their semantics preserved.</p><p>Further fundamental aspects regarding the Learning phase are the outputs and the parametrization used to produce these outputs. Like so, The ModelSchema class describes the characteristic of the models produced in a learning stage or learning stage section, such as the number of layers of a neural network or the number of trees in a random forest. The ModelProspection class represents the prospected ML models, i.e., the reference for the ML models learned during a learning stage or learning stage section of a training stage. In addition to the data produced in the Learning phase is the EvaluationMeasure class. This class, combined with EvaluationProcedure and EvaluationSpecification classes, provide the representation of evaluation mechanisms of the produced ML models during any stage of learning, specifically: an evaluation measure defines an overall metric used to evaluate a learning stage (e.g., accuracy, F1-score, area under the curve); an evaluation specification defines the set of evaluation measures used in the evaluation of learned models; and, an evaluation procedure serves as the model evaluation framework, i.e., it details the evaluation process and used methods. On the parametrization aspect, PROV-ML afford two classes LearningHyperparameter and ModelHyperparameter. The first hyperparameter-related class represents the hyperparameter used in a learning stage or learning stage section (e.g., max training epochs, weights initialization). The second class is used in the representation of the models' hyperparameters (e.g., network weights). Finally, PROV-ML addresses the retrospective counterpart of the classes mentioned above. The classes ending in Execution and Value are the derivative retrospective analogous of data transformations and the attributes, respectively.</p><p>Comparing with the paper <ref type="bibr" target="#b23">[24]</ref> being extended here, we make the following improvements in PROV-ML. We change the representation of learning stage types (i.e., training, evaluation, and validation) from an enumeration to a hierarchy to make explicit relationships specific to learning phases. Also, towards better defining the stages within a learning experiment, we introduce the Learning Stage Section (LearningStageSection and LearningStageSectionExecution which is crucial to represent the specific characteristics sub-parts of a learning process, when it is required. Combining these representations, we enable the representation of specific events as sub-sections in a learning stage. For instance, in the training stage we now may represent mini-batch iterations, which is a common sub-section to enhance stochastic optimization <ref type="bibr" target="#b47">[48]</ref>, and thereafter make references to the mini-batch representations. Likewise, we introduce the Persona class, subclass of PROV Agent, and associate it with BaseLearningStage and BaseLearningExecution. Further, we refined the names of elements related to hyperparameters and model besides description adjustments due to learning stage section creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM IMPLEMENTATION AND EXPERIMENTAL EVALUATION</head><p>In this section, we provide experimental validation of the design decisions to build and query the MLHolView to support the lifecycle of scientific ML in a real case study in the O&amp;G industry. First, we explain how we implement and deploy the system used in the evaluation (Sec. 5.1). Then, we show a running example of which data are captured during execution of the workflows to answer the exemplary queries Q1-Q7 (Sec. 5.2). After, we present performance and scalability analyses of the system (Sec. 5.3). Then, we discuss the benefits of PROV-ML both in terms of easing queries and query performance (Sec. 5.4). Finally, we discuss how our approach can be customized (Sec. 5.5) and conclude with lessons learned after this evaluation (Sec. 5.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation and Deployment</head><p>ProvLake <ref type="bibr" target="#b46">[47]</ref> is a provenance system capable of capturing, integrating, and querying data across distributed services, programs, scripts, and data stores used by multiple computational workflows using provenance data management techniques <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In this section, we explain how we implement the design decisions to enable ProvLake to build the MLHolView and how it is deployed to support the lifecycle in our case study. ProvLake Architecture. ProvLake architecture is an implementation of the reference architecture (Fig. <ref type="figure" target="#fig_4">6</ref>). Details about this architecture can be found in our previous work <ref type="bibr" target="#b23">[24]</ref>. Here we give a summary, highlighting how its components are mapped to the reference architecture proposed in this paper. The ProvLake Library (PLLib) [49] maps to the ProvLib. ProvTracker implements simple queue management to receive the provenance capture events coming from the lib and also implements a queue consumer, thus working both as the message broker and the provenance consumer in the reference architecture. ProvManager maps like the reference architecture and the PolyProvQueryEngine is the component for building the provenance queries and sending them to the DBMS managing the MLHolView. As described in the decision S5, the workflows are specified using prospective provenance data stored as configuration files. Data transformations that are specific and standard in ML workflows, e.g., training, validation, and evaluation are defined beforehand following the conceptual data schema for the key concepts (D5) and the PROV-ML (Sec. 4) for attributes, such as hyperparameters and model evaluation attributes. ProvTracker uses the specified prospective provenance data to provide for the tracking by creating the relationships of retrospective provenance data being continuously sent by PLLib added to the workflows. ProvTracker gives unique identifiers (S4) to every data value captured and when there are data references (e.g., references to files or identifiers in a database table or any analogous data reference), it creates a knowledge graph relationship between the data value and the data store <ref type="bibr" target="#b8">[9]</ref>. ProvManager transforms the captured data into RDF triples (the data model of the DBMS in use by ProvLake in this implementation) following the PROV-ML ontology (when capturing data in the learning phase) and PROVLake ontology (when capturing data in the previous phases of the lifecycle).</p><p>ProvLake Deployment in the Case Study. The deployment of our case study also follows the system design decisions (Sec. 3.2). It uses two clusters: a Kubernetes cloud cluster for data curation and learning data preparation workflows, and the other is a large HPC cluster with CPUs and GPUs for the workflows in the learning phase. PLLib is the only component in direct contact with the users' workflows running in the clusters (S3). This deployment is illustrated in detail in our previous paper <ref type="bibr" target="#b23">[24]</ref>.</p><p>Hardware Setup. The experiments use three environments. An HPC cluster for learning workflows, which has 393 Intel and Power8 nodes, each with 24 to 48 CPU cores, 256 to 512 GB RAM, interconnected via InfiniBand, sharing about 3.45 PB in a GPFS, and using in total 946 GPUs (NVIDIA Tesla K40 and K80, each with 2880 and 4992 CUDA cores respectively); a Kubernetes cloud cluster for data processing, which has 4 nodes, each with 16 GB RAM and 8 cores; and a server machine Intel Core i7-7700T CPU 2.40 GHz, 8 GB DDR4 RAM, 128 GB SSD Liteon. Software Setup. ProvManager, PolyProvQueryEngine, and Prov DBMS are deployed on a virtual Kubernetes cluster with two nodes with 4 vCores, 16 GB RAM each, virtualized on top of the data processing cluster. ProvManager's queue is set to 50, and ProvTracker threads are set to 120. The workflow scripts of our use case are implemented in Python using different libraries to manipulate raw seismic files and for designing and training the ML algorithms (PyTorch V1.1) that execute in the HPC cluster. For the query performance tests, we deployed three different DBMSs on the server machine: Apache Jena TDB 3.12, Allegro 6.6.0, and Blazegraph 2.1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Use Case Validation</head><p>In this section, we investigate whether our approach supports the lifecycle by enabling users to perform comprehensive, i.e., end-to-end analyses that integrate the data consumed and generated in the workflows, from raw domain data to learned models. More specifically, we investigate if the proposed data design decisions (Sec. 3.1) can be applied to answer queries that do such integration of the data. We explore the O&amp;G use case described in Section 2 and validate if the data tracked by ProvLake, inserted in the MLHolView implementing the PROV-ML data, can answer the queries Q1-Q7. Fig. <ref type="figure" target="#fig_6">8</ref> shows the phases of the lifecycle in this use case. Next, we describe the workflows of the use case and how ProvLake tracks the data.  <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Curation</head><note type="other">Triple Store</note><p>In the data curation phase, ProvLake tracks provenance while data-intensive scripts run. When processing raw files, essential data that will help answer the queries are extracted, associated with the file's URI, and stored in the provenance database. One example of such data is the embedded geographic coordinates in raw SEG-Y seismic files. Additionally, geoscientists add relevant information, based on their specialized knowledge, as input to some of those scripts to be loaded into a domain-specific knowledge graph database, external to the provenance database, but also tracked by ProvLake through links between the workflows and the domain knowledge in the graph. Relevant information includes associated oil fields, basins, oil wells, and pieces of text from PDF documents with survey information related to the geological data acquisition process. These annotations are stored in triple stores in a domain-specific database, externally to the provenance database.</p><p>The learning data preparation phase includes several data transformations in a pipeline that converts the curated and annotated scientific data into training, validation, and evaluation datasets. Each transformation contains parameters that specify, for instance, noise filter thresholds, input shape, or the selected seismic lines (inlines, or crosslines) of the seismic cube that constitutes the training dataset. Each value of these parameters, the name of the transformation, the execution data, and the references to input and output data are captured and represented in ProvLake's provenance data graph.</p><p>The entire process is interconnected, where each phase produces data and passes it forward for the consumption of the next one. Essentially, ProvLake tracks and maintains such interconnections in a provenance data graph composed of RDF triples. Such structures describe chained data transformations in the multiple workflows that constitute the inner phases of the major ones of the lifecycle run. RDF resources represent the data in Fig. <ref type="figure" target="#fig_6">8</ref>, i.e., instances that extend prov:Entity and PROV-ML specializations. Each of these instances receives a URI, which works as a global identifier throughout the lifecycle D4. Examples of RDF resources are learned models produced in the learning phase, a model's hyperparameters, evaluation metrics, and references (file path) to actual model files stored in the file system. Provenance data graphs also associate execution data with learned models. Execution data may include file system metadata, a cluster's hostname and node names used in the HPC jobs, job ids in the cluster scheduler, or start and end timestamps of each block of provenance capture events.</p><p>ProvLake can keep track of data distributed in multiple stores. Such ability helps to maintain data relationships between raw files in the file system and structured knowledge stored in another database. Auxiliary data, such as polygons in the seismic cube, are stored in the Document DBMS. The system similarly tracks data references and related to the raw files. Other data, such as implementation details, software name, and version, are captured and stored in the provenance database, following the PROV-ML, but, for simplicity, we do not show them in the figure. Finally, since the system tracks every data and their relationships while the workflows execute, ProvLake enables answering online, offline, intra-and inter-training provenance queries to analyze ML data, domain-specific data, and execution data throughout the phases of the lifecycle, exemplified by the queries Q1-Q7.</p><p>To submit queries, the user sends a GET or POST request to one of PolyProvQueryEngine's endpoints. Then, PolyProv-QueryEngine sends requests to ProvManager. Most of the queries are answered with simple graph traversals using standard SPARQL features. For instance, to answer Q1, the user provides a learned model URI (generated in the learning phase), and the query should traverse in the provenance data graph backward until the raw seismic file's URI (processed in the data curation phase). One can get the geographic coordinates and number of seismic slices by querying the extracted data related to the seismic file. In turn, to obtain the oil basin and oil field information, the query retrieves data from the resource, in the Triple Store, which represents structured knowledge about the seismic file. For Q2 and Q6, one can execute a similar graph traversal. Other queries require analytical operators, such as Q3, which requires finding the learned model with least (using min() native SPARQL operator) loss and returning its hyperparameters. Q4 and Q5 make use of execution data to provide basic statistics (min(), max(), avg() operators) about the execution time of training iterations and Q7 retrieves the models, their hyperparameters, their evaluation measures, and minimum batch loss per model generated when a specific learning dataset was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Analysis on Capturing Data during Learning Workflows</head><p>In our use case for training an autonomous identifier of geological structures (c.f. Sec 2), the learning phase generates a large amount of provenance data at a high frequency to stress ProvLake services. In the deep learning model training, there are two provenance capture calls (for the beginning and end) at each batch iteration in each learning epoch. In this test, each learning workflow executes about 35 iterations for each learning epoch and up to 300 epochs, generating about 15,000 provenance capture events per workflow run. ProvTracker runs on one node in the learning cluster with 24 CPU cores, whereas the learning workflows run in parallel and distributed on up to 8 nodes, each with 28 Intel CPU cores and 6 GPUs (K80). While running the workflows, PLLib captures data at runtime and sends them to ProvTracker, which in turn sends them to ProvManager service deployed externally on the virtual Kubernetes cluster, which finally stores them in the Prov DBMS. A provenance capture overhead analysis of ProvLake using synthetic workloads to highly stress the system and comparison with a competing system has been presented in a previous work <ref type="bibr" target="#b8">[9]</ref>. Here, we evaluate the system design decisions that focus on providing distributed capture control and scalable architecture (S1-S3). We test different settings for provenance analysis and then test the scalability using real ML workloads in both cases. We measure the overall execution time of the learning workflow script, repeating each test at least 10 times and we plot the boxplots of the repetitions and the numeric values used in-text refer to the medians. Varying Provenance Capture Settings. The PLLib allows customizing provenance capture settings, such as the queue size and whether the provenance capture events should be persisted to the local disk, rather than sending to ProvTracker. Then, if disk only is not specified, when the scripts execute, provenance data are captured and sent to ProvTracker.</p><p>For a baseline, we first execute the training without any provenance capture, then we vary the queue size in PLLib (i.e., amount of provenance capture requests accumulated in PLLib), diskless vs. diskful (i.e., saving or not provenance data in a log file on disk), and online vs. offline (i.e., storing or not provenance data in the DBMS, available for online provenance queries during the execution). As for the training datasets, we use a curated and labeled real seismic dataset using a specific range of seismic slices (corresponding to a regional section of a seismic cube) defined by the model trainer. The results are in Fig. <ref type="figure" target="#fig_7">9 (a)</ref>, where the fastest result is for Queue Size = 50, Diskless, Online (Setting D). Comparing with the setting with no provenance capture, the added execution overhead, in this case, is only 8.6 seconds on top of 21.3 minutes, i.e., 0.67%, which is considered negligible. Although these workflows execute in parallel in a total of around 21.3 minutes, in this experiment, there are 8 workflows concurrently running using 228 CPUs and 48 GPUs. It is only a snapshot of the whole lifecycle, whereas in practice, this set of 8 (or more) workflows needs to be executed hundreds of times as users adjust the experiment setting and assess the model performance. They only stop when they are satisfied with the results, which may take weeks of experimentation. Also, this is a critical part of the lifecycle, responsible for stressing the provenance capture system while training in an HPC cluster.</p><p>To analyze the queue size, we compare Settings A-C with D-F, and we see larger queues provide faster provenance capture since there is less but larger communication with ProvTracker service. For instance, Setting A is about 7% slower than D. Persistence latency (i.e., time taken between data capture in the running workflow and the actual persistence in the database) is not the focus of this experiment, as we are interested in understanding whether our approach adds significant capture overhead, which is what impacts the running user workflow performance. However, we briefly discuss it here as very large queues can introduce higher persistence latency and may impact the user experience, especially for monitoring. This latency may occur for many reasons, including network traffic, from the running workflow to the DBMS passing through the provenance system, and queue throttling at the DBMS receiving the persistence events, as DBMSs typically also implement queuing mechanisms. In the settings with queue size 50 (D-F), a persistence latency of less than 5 seconds was empirically observed, which is considered good enough for training monitoring. To analyze diskless vs. diskful settings, we compare Setting A with B and C; and D with E and F. Diskless is faster than diskful, as the latter introduces more I/O operations at runtime. However, comparing only the medians, the difference is negligible (less than 0.1%). Thus, because of a higher fault tolerance provided by a diskful setting, it may be useful to append provenance data onto a file on disk, locally in the cluster where the workflow runs. Similarly, comparing the medians, we observe that the  difference between online vs. offline (e.g., setting B vs. C or E vs. F) is also small, about 1%. Therefore, despite (D) being the fastest setting, (E) may be preferred because its performance is nearly the same as (D), and it has the advantage of backup storage for provenance data, which is quite important as provenance is used for reproducibility. Scalability Analysis. In this experiment, we want to confirm if the execution strategies on an HPC cluster are keeping the overhead low in a real ML workload, running multiple learning workflows in parallel. We run a weak scalability test by increasing the number of processing units while increasing the data size. We use the fastest setting of the previous experiment (i.e., D) and the same seismic cube. To set up the training datasets, the trainer selects up to 8 different sets of seismic slices, where each set has the same length (i.e., nearly the same data size). Thus, for x ∈ {1, 2, 4, 8}, there are x workflows running on x nodes in parallel, summing 28x Intel CPU cores, 6x GPUs, 4992 * 6x CUDA GPU cores, using in total an input dataset with size x * datasize, where datasize is the size of a dataset formed by 1 set of seismic slices. The results are in Fig. <ref type="figure" target="#fig_7">9</ref> (b), where we illustrate the linear scalability as a horizontal line passing through the median of the smallest setting (x = 1). Ideally, the medians should be near this line. If they are not, it means that ProvTracker is taking too long to answer, caused by high stress in the system due to too many provenance capture requests, adding latency to the training. However, we see that even in the largest setting (i.e., x = 8), the execution time remains close to the linear curve. The boxes remain within a small margin of 0.2 min (or 0.9% of the x = 1 median) between 21.4 and 21.6 min, meaning that the system delivers a constant and predictable behavior even at larger scales. We note though that the variance grows with the scale, caused by the larger number of parallel tasks. Therefore, we conclude that at least for this scale (up to 48 K80 GPUs), the provenance capture system delivers good scalability.</p><formula xml:id="formula_0">) (A) (B) (C) (D) (E) (F)<label>(a</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Query Comparisons With and Without PROV-ML</head><p>In this experiment, we analyze the benefits of PROV-ML, both qualitatively and quantitatively. We begin with a qualitative comparison of queries that use PROV-ML, highlighting its expressiveness and the complexity of building queries that use or not PROV-ML. Then, we further provide a quantitative analysis to investigate whether using PROV-ML can help to accelerate queries and, if it can, how much it helps. Among the queries Q1-Q7, we select three to compare in detail: Q1, Q5, Q7, and the reason for this choice is that they increase in complexity and how much they make use of the concepts modeled specifically in the PROV-ML ontology (i.e., emphasis on the learning phase, c.f. Sec. 4). Q1 is the simplest query and makes the least use of PROV-ML specific concepts, Q7 is the most complex query with the heaviest use of PROV-ML, and Q5 is in between these two. We write the selected queries both with and without the PROV-ML ontology (written in OWL) using SPARQL 1.1. The query complexity stems from the number of clauses to filter, patterns to match in the graph traversal, aggregations and sorting, and amount of triples that satisfy the patterns to match; and the number of clauses that make use of the PROV-ML ontology defines how much each query makes use of the PROV-ML ontology. Qualitative comparison. Since Q7 is the most complex query and makes heavy use of PROV-ML, it helps us to illustrate whether PROV-ML eases query building, especially when there is heavy use of Learning phase concepts. Excerpts of Q7 in SPARQL with and without PROV-ML are available in the Listings 1 and 2, respectively. Comparing both, since PROV-ML has specialized concepts for the Learning phase, it requires fewer clauses to be matched to express the same concept. For instance, to match triples in the training stage only, with PROV-ML, we just write one clause (Lst. 1#L2), whereas without PROV-ML we need to write four clauses to qualify the data transformation needed for the query (Lst. 2#L1-6). This is because there are other stages (evaluation, validation), and since without PROV-ML there are no specific types for each stage, we need to qualify the variable ?training to determine the correct stage. Without PROV-ML, the only resource we have to do this is to tag the data transformations that are related to training. In PROVLake ontology, tagging of workflows, data transformations, and attributes is possible with the property provlake:tag, but since naming, schema definitions, and tagging are available only in the prospective part, we need three more clauses: one to relate the retrospective instance with its prospective instance (Lst. 2#L4), other to give the type of the prospective instance (#L6), and a third clause to qualify the data transformation as "Training" using the tag property (#L7). A similar fact happens for the epoch iteration part of the query (Lst. 1#L3-6 and Lst. 2#L8-12) and also for the model reference (Lst. 1#L16-19 and Lst. 2#L26-33) and for model evaluation (Lst. 1#L20-24 and Lst. 2#L34-42). However, the model hyperparameters part of Q7 differs from the others because Q7 requires the name of the hyperparameters, in addition to the values, and names are stored within the prospective portion of the data. In this case, Q7 demands a relationship between prospective and retrospective, regardless it is with PROV-ML or not. The only difference between with and without PROV-ML for such cases is that with PROV-ML, we do not need tags to qualify the attributes of interest (Lst. 2#L24, as we can specify hyperparameters using the specialized type (Lst. 1#L14). As a result, in these cases, only one extra clause is needed.</p><p>Therefore, we found that when the parts of the query do not demand prospective provenance data, one needs to write three extra clauses when not using PROV-ML (a clause to relate the retrospective with prospective, another to give the prospective instance type, and a third clause to qualify this instance, often using tags or labels). However, when the query demands prospective provenance data, one needs only an extra clause (to qualify the instance), because the relationship and types will be required regardless it uses PROV-ML or not. These observations are in Table <ref type="table" target="#tab_7">3</ref>. We verified the same behavior in the queries Q1 and Q5. Thus, we conclude that PROV-ML's ability to qualify specific ML data transformations and attributes using direct types eases query building, as it reduces the number of clauses required to express ML-specific concepts compared to a data representation that does not use PROV-ML. A reduction of one to three clauses per query part was observed in all queries. Quantitative comparison. We discussed in Section 3.1 that certain data design decisions might accelerate queries that make use of the defined concepts, and it is known that design choices when modeling an ontology may impact the performance of queries. Also, a recent work that evaluated schema optimization to speed up queries in knowledge graphs <ref type="bibr" target="#b37">[38]</ref>, showed that this is still a relevant topic to be investigated. Therefore, in addition to the qualitative gains discussed previously, we conduct a quantitative evaluation experiment to verify how much (if any) PROV-ML impacts query performance.</p><p>We generate 2 synthetic datasets that mimic the real use case evaluated in Sections 5.2 and 5.3. With the synthetic dataset, we can control experiment variables, such as the numbers of parallel learning workflows, hyperparameters, epochs, and batches per epoch, as well as the model evaluation metrics. We can also generate one dataset that uses PROV-ML and another that does not. If using only the real data for this experiment, it would be much harder to reproduce and control these conditions, whereas with these two synthetic datasets, it is more cost effective and feasible to switch between with and without PROV-ML, rather than having to implement, deploy, and run the real learning workflows without PROV-ML. Both datasets are as follows: 8 parallel learning workflows, each with the 3 stages (training, validation, and evaluation), 300 epochs,and 200 batches per epoch (i.e., 60,000 batches), where each batch is associated with batch losses and hyperparameters, and each epoch uses hyperparameters and generates models and model evaluations. In total, each dataset has 10,168,890 triples.</p><p>The performance impact depends on the number of clauses to be matched in the query and on the number of triples matched by the Triple Store DBMS. However, the performance depends on the underlying DBMS that manages the MLHolView, since the DBMS might implement efficient indexing mechanisms, parallelism techniques, or data transformation strategies. Therefore, for this experiment, we analyze the three queries (Q1, Q5, Q7). Q1 does a simple graph traversal with simple pattern matching. Q5 does more complex graph traversals and needs to calculate aggregates (average of time difference per batch, per epoch), but for training stages only. Q7 also does complex traversals and needs to calculate aggregates (minimum batch loss per epoch), but for three stages (training, validation, and evaluation), in addition to listing hyperparameters and model performance. Since the choice of the underlying DBMS may impact the results, we analyze three different DBMSs: AllegroGraph, Blazegraph, and Jena TDB running on the same hardware and same conditions, with their default settings (no special fine-tuning are performed in any DBMS). We analyze the query execution time, which is measured in the requesting client subtracting the timestamp obtained immediately after the response has arrived in the client from the timestamp obtained immediately before sending the request. Results are in Figure <ref type="figure" target="#fig_0">10</ref>, where we plot the medians of the query execution time over a hundred repetitions or when the confidence interval of the medians was below 5%. The numeric values reported in-text also refer to the median of the repetitions, we do not remove outliers, the height of the confidence intervals are the error bars, and Q7 results are in log scale.  prov : wasInfluencedBy ? t r a i n i n g p r o s p ; 5 ? t r a i n i n g p r o s p 6 a provlake : DataTransformation ; 7 provlake : t a g " T r a i n i n g " . 8 --Epoch iteration (Training Section) 9 ? e p o c h e x e c t r a i n i n g 10 prov : wasInformedBy ? t r a i n i n g ; 11 a provlake : Da ta Tr an sf or ma ti on Ex ec ut io n ;    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q7</head><p>With PROV-ML Without PROV-ML Fig. <ref type="figure" target="#fig_0">10</ref>: Execution time comparison of queries using PROV-ML vs. queries without PROV-ML. Q7 is in log scale.</p><p>The results show that for Q1, PROV-ML does not significantly impact query performance since the queries using PROV-ML are only 1.17x and 1.05x slower in AllegroGraph and BlazeGraph, respectively; and the difference for Jena is within the error bars, i.e., no statistically significant difference. However, Q1 is only the simplest query, with trivial graph traversals and little use of PROV-ML specific concepts, and with query times up to a hundred milliseconds (very fast queries). The DBMSs are likely spending more time doing data transfers than actually computing the query, which would explain the higher error bars for Q1 than for Q5 and Q7, for which the error bars are so small (&lt;1%) that can barely be seen in the plot. Nevertheless, Q5 and Q7 show a different behavior, particularly for the DBMSs AllegroGraph and BlazeGraph. In Q5, the queries with PROV-ML are 1.22x, 2.9x, and 2x faster in Jena, AllegroGraph, and BlazeGraph, respectively. In Q7, they are 3x faster in AllegroGraph and 16.6x (an order of magnitude) faster in BlazeGraph, whereas in Jena, the results are nearly the same. Since Q5 and Q7 need to aggregate over 60,000 batches per learning stage, and Q7 has over three learning stages, these queries are considerably more complex than Q1, make much heavier use of PROV-ML specific concepts, and require complex operations in the DBMS. We analyze the system statistics while these two queries execute, and we observe full CPU utilization. With these queries, we can see that those clauses that are reduced because of PROV-ML types make a difference in the query performance, with significant gains in AllegroGraph and BlazeGraph. In Jena, there were gains (more clearly seen in Q5), but they were not as relevant as in the other two DBMSs. Therefore, we conclude that PROV-ML not only has qualitative advantages, such as expressiveness for ML-specific concepts and reduced complexity to write queries, but it can also help the performance of the queries, as it shows acceleration in almost all cases, particularly an order of magnitude faster for complex queries in certain workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Utilization and Customizing the Approach to other Applications and Domains</head><p>Our approach is driven by queries, meaning that depending on the classes of queries (Sec. 2.3) the users involved in the lifecycle of the ML models want to execute over the MLHolView (D1), they will know what to capture in their workflows. Then, they can declaratively design each workflow in the lifecycle by specifying the data transformations they want to make explicit, along with the input and output data of each transformation. In our current implementation, this specification is done using configuration files. To enable runtime data capture and integration in their workflows, users import a lightweight library (PLLib) and add the data capture calls into the workflow codes (see S1, S5). The PLLib is publicly available on GitHub <ref type="bibr">[49]</ref>, with examples for data transformation instrumentation. During the specification, users also inform which data transformations are for training, validation, and evaluation, and which attributes of the transformation are hyperparameters and evaluation metrics. Such specification is used by ProvManager to create the nodes, their properties, and their relationships in the knowledge graph MLHolView using PROV-ML (D2, D3). Also, in this specification, users inform (and add metadata about) the data stores being used by the workflows and the execution environments. Users can always revisit the set of queries they want to execute to either improve the queries (e.g., adding new fields in the projection or new filters) or add new queries, then they revisit the workflows specification and check if the hooks they add will capture the data they need for the queries.</p><p>In addition, as we observe in Section 5.2, by leveraging a domain-specific knowledge graph (D2), our approach can provide queries with rich semantics about the domain data. To enable this, during the specification phase, users inform which data transformations and attributes are known concepts in the domain (i.e., have been previously designed in a domain ontology), so that during the capture, will create the relationships between the captured provenance data with the domain-specific concepts in the knowledge graph. This is an optional step, although it improves the overall users' understanding of the data flowing in the lifecycle, as the users typically are familiarized with the domain. For each application and domain, there is a specific ontology and knowledge graph with concepts and instances that make sense to that application or domain. The process of building a domain-specific ontology is out of the scope of this work. Our approach focuses on creating the relationships, in the MLHolView, between the heterogeneous workflow data and the domain-specific knowledge graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Lessons Learned</head><p>We draw a set of lessons learned after the practical experience of implementing the data and system design decisions to support the lifecycle in a real deployment in an O&amp;G industry case that uses heterogeneous environments, i.e., a Kubernetes cluster and a large HPC cluster with CPUs and GPUs. The key findings for the success of the experiments are the following:</p><p>(i) Characterizing the lifecycle and identifying the main classes for data analysis using provenance allowed the understanding of the different needs in scientific ML (Sec. 2). Particularly, it helped to understand the different personas driving the provenance capture to answer key online and offline, intra-and inter-training provenance queries. The queries were capable of analyzing ML data, domain-specific data, and execution data, throughout the data curation, data preparation, and learning phases of the lifecycle in an integrated way. We observed that the data curation phase is the most complex. One needs to address it carefully to take advantage of domain-specific knowledge, which highly benefits trainers in the learning phase.</p><p>(ii) Employing provenance tracking and a data representation that allows data integration of multiple workflows helped to address the highly heterogeneous nature of the lifecycle. To accomplish such integration, it was key to promote a holistic view of the lifecycle, end-to-end, which we called MLHolView, as described in the data design decision D1, which enabled the comprehensive data analyses (e.g., Q1-Q7), thus supporting the lifecycle, which is our main motivation. Due to the highly heterogeneous nature, the context-awareness using domain-specific and ML data and knowledge materialized in a knowledge graph leveraging provenance-based relationships (D2) enabled tracking, persisting, and querying interconnections between heterogeneous data with details about localization and data access. Furthermore, it enabled queries with rich semantics about the application domain and ML, exploring new data relationships that would not be possible without such context awareness.</p><p>(iii) Designing a conceptual data schema focused on the key concepts enabled the design and implementation of the system facilitating query building, and the ML-specialized schema modeling. The key concepts are described in D5. It enabled query acceleration and facilitated query building for queries that make heavy use of ML-specific concepts compared with a schema that does not have such specializations. Yet, the focused schema was the basis for PROV-ML (Sec. 4), which served as the underlying schema for the provenance system. PROV-ML combines the provenance of data lakes to address integration, embracing the heterogeneity nature, with concepts for ML (D3). PROV-ML leverages W3C contributions for provenance, W3C PROV <ref type="bibr" target="#b21">[22]</ref>, and for ML, ML Schema <ref type="bibr" target="#b22">[23]</ref>. We hope other systems with similar purposes can adopt such representation.</p><p>(iv) The system design decisions enabled data capture and integration in a highly heterogeneous and distributed setting adding negligible overhead. Particularly, S1 and S2 provided the portability and flexibility needed in such deployments. The scalable strategies (S3) allowed the system to do this while incurring low overhead as well, even in HPC workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>The interest in workflow provenance management has increased in the recent years, driven by a major effort by the provenance community <ref type="bibr" target="#b48">[50]</ref>, <ref type="bibr" target="#b49">[51]</ref>, <ref type="bibr" target="#b50">[52]</ref>, <ref type="bibr" target="#b51">[53]</ref>, <ref type="bibr" target="#b52">[54]</ref>, <ref type="bibr" target="#b53">[55]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b54">[56]</ref>, <ref type="bibr" target="#b55">[57]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b56">[58]</ref>, <ref type="bibr" target="#b57">[59]</ref>, <ref type="bibr" target="#b58">[60]</ref>, <ref type="bibr" target="#b59">[61]</ref>, <ref type="bibr" target="#b60">[62]</ref> , particularly to explore possibilities of optimizing workflows with the data captured by provenance tools and as a response to the urgent need for reproducible science, which is critical in scientific ML <ref type="bibr" target="#b61">[63]</ref>. To exemplify, Thavasimani et al. <ref type="bibr" target="#b13">[14]</ref> investigate provenance traces recorded during workflow executions to observe differences in results with minor workflow configuration differences. Other works have advanced provenance tracking techniques on heterogeneous data, stores, and environments <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b62">[64]</ref>, <ref type="bibr" target="#b63">[65]</ref> and others have explored the intersection of provenance and blockchain <ref type="bibr" target="#b64">[66]</ref>, <ref type="bibr" target="#b65">[67]</ref>.</p><p>On the intersection between ML and provenance, other works have explored provenance to support ML workflows <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b66">[68]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b67">[69]</ref> and Deelman et al. <ref type="bibr" target="#b68">[70]</ref> characterized provenance analysis to leverage ML in support of scientific workflows. On reproducible ML models, another aspect that has been explored is the use of provenance as an essential tool to help create explainable artificial intelligence <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b61">[63]</ref>. In addition, some works addressed the gap between the experiments of an ML workflow execution and a standard representation to provide reproducible experiments <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Esteves et al. <ref type="bibr" target="#b44">[45]</ref> provide a machine-readable vocabulary and a common schema for reproducibility of ML experiments in various frameworks and workflow systems. Publio et al. <ref type="bibr" target="#b45">[46]</ref> present a new ML data representation based on MEX vocabulary <ref type="bibr" target="#b44">[45]</ref> to improve processes on ML workflows, despite not having a clear separation between prospective and retrospective provenance. Samuel <ref type="bibr" target="#b69">[71]</ref> propose ProvBook, for reproducibility of ML experiments using Jupyter notebooks applying FAIR data principles. Moreno et al. proposed MLWfM <ref type="bibr" target="#b70">[72]</ref> to provide data concepts for ML and domain-specific awareness, but without provenance concepts and a data representation. Brandao et al. <ref type="bibr" target="#b71">[73]</ref> proposed a knowledge-based workflow management approach aiming at broadening user collaboration over ML experiments. It provided a semantic structure for computational workflows allowing rich querying at different levels of provenance.</p><p>These works are important building blocks to support the lifecycle of scientific ML using provenance management techniques. Nevertheless, they still lack a holistic view capable of comprehensively integrating the data in the whole lifecycle, end-to-end, from raw domain data to learned models. Without such a holistic view, the ML-specific concepts cannot integrate with the specific concepts about the scientific domain, jeopardizing the comprehensive end-to-end analyses that require richer semantics about the domain integrated with rich semantics about ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this work, we aimed at enabling scientists and engineers to perform comprehensive data analyses in the lifecycle of scientific ML. We proposed workflow provenance techniques to address the problem of dealing, in an integrated and comprehensive way, with the high heterogeneity of different contexts (e.g., data, software, environments, persona) involved in the lifecycle, to enable such analyses. We proposed modeling the workflows in all phases of this lifecycle as multiple interconnected workflows. A holistic view of the data processed in these workflows should be built as the workflows execute. In this way, the collaborating teams can use it as their primary source of data analyses that integrate that from raw data to learned ML models. We called it as Provenance-based Holistic Data View of the Lifecycle of Scientific ML (MLHolView). It is materialized as a knowledge graph with provenance-based relationships. It is aware of the contexts of the data transformations in the workflows, their (hyper)parameterizations and model metrics, which computational environments they run and data stores they use, the involved personas, and how they interact with the workflows.</p><p>To be able to build this view, aware of these many dimensions of heterogeneity, we first characterized the lifecycle and proposed a taxonomy for the classes of data analyses (e.g., data, execution timing, and training timing). Then, we proposed design principles for the effective and efficient management of provenance data from these workflows. From this understanding and design principles, we derived the PROV-ML data representation, promoting such a holistic view of data in workflows in the lifecycle, which is the first one to the best of our knowledge. We also proposed system design principles and a reference system architecture to provide the view with efficient provenance capture adding significant data capture overhead (&lt;1%). It allows for portable and flexible deployments required because of the heterogeneous executions. We obtained these results after implementing the design principles and the PROV-ML in the ProvLake system and deploying it in a real case in the O&amp;G industry. Altogether, based on our studies, our major finding is that the design principles enabled comprehensive queries, with rich semantics about the domain and ML, by exploring the data view while maintaining high scalability even in HPC workloads. Finally, ongoing work is towards applying our approach in the finance industry, to integrate workflows in cloud and HPC clusters to train ML models for credit risk assessment. The preliminary results indicate that our approach can be customized with the effort described in Section 5.5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Spectrum of expertise and personas in the lifecycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The Lifecycle of Scientific ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The Provenance-based Holistic Data View of the Lifecycle of Scientific ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Conceptual data schema of the key concepts of the lifecycle and their supporting graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Reference system architecture to manage workflow provenance in the lifecycle of scientific ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: PROV-ML: a W3C PROV-and W3C ML Schema-compliant provenance data representation for scientific ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FilteringFig. 8 :</head><label>8</label><figDesc>Fig.8: Summarized example of provenance tracking in an O&amp;G use case. Details on the captured data, contents, stores, and the dataflow used to answer the queries Q1-Q7 are in Table4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>)Fig. 9 :</head><label>9</label><figDesc>Performance analysis results. Figure9(a) shows the variation of provencance capture settings, where Setting D adds 0.67% overhead. Figure9(b) shows the scalability results, a near-linear scalability with up to 48 GPUs and 228 CPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 6 a</head><label>6</label><figDesc>--Training stage 2 ? t r a i n i n g a provml : T r a i n i n g E x e c u t i o n . 3 --Epoch iteration (Training section) 4 ? e p o c h e x e c t r a i n i n g 5 prov : wasInformedBy ? t r a i n i n g ; provml : T r a i n i n g S e c t i o n E x e c u t i o n . 7 --Model hyperparameters 8 ? epoch training hyperparam 9 prov : wasGeneratedBy ? e p o c h e x e c t r a i n i n g ; 10 a provml : ModelHyperparameterValue ; 11 prov : value ? epoch training hyperparam v ; 12 prov : wasDerivedFrom ? epoch training hpram psp . 13 ? epoch training hpram psp 14 a provml : LearningHyperparameterSetting ; 15 r d f s : l a b e l ? epoch training hyperparam name . 16 --Model 17 ? m o d e l t r a i n i n g 18 prov : wasGeneratedBy ? e p o c h e x e c t r a i n i n g ; 19 a provml : Model . 20 --Model evaluation 21 ? m o d e l t r a i n i n g e v a l 22 prov : wasGeneratedBy ? e p o c h e x e c t r a i n i n g ; 23 a provml : ModelEvaluation ; 24 prov : value ? m o d e l t r a i n i n g e v a l v a l u e . Listing 1: Excerpt of Q7 with PROV-ML. 1 --Training stage 2 ? t r a i n i n g a 3 provlake : Da ta Tr an sf or ma ti on Ex ec ut io n ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>12 prov</head><label>12</label><figDesc>: wasInfluencedBy e p o c h e x e c t r a i n i n g p s p .<ref type="bibr" target="#b12">13</ref> ? e p o c h e x e c t r a i n i n g p s p 14 a provlake : DataTransformation ; 15 r d f s : l a b e l "Epoch E x e c u t i o n " . 16 --Model hyperparameters 17 ? epoch training hyperparam 18 prov : wasGeneratedBy ? e p o c h e x e c t r a i n i n g ; 19 a provlake : A t t r i b u t e V a l u e ; 20 prov : value ? epoch training hyperparam v ; 21 prov : wasDerivedFrom ? epoch training hpram psp . 22 ? epoch training hpram psp 23 a provlake : A t t r i b u t e ; 24 provlake : t a g " Hyperparameter " ; 25 r d f s : l a b e l ? epoch training hyperparam name . 26 --Model 27 ? m o d e l t r a i n i n g 28 a provlake : A t t r i b u t e V a l u e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>29 prov</head><label>29</label><figDesc>: wasGeneratedBy ? e p o c h e x e c t r a i n i n g ; 30 prov : wasDerivedFrom ? m o d e l t r a i n i n g p r o s p . 31 ? m o d e l t r a i n i n g p r o s p 32 a provlake : A t t r i b u t e ; 33 provlake : t a g "Model" . 34 --Model evaluation 35 ? m o d e l t r a i n i n g e v a l 36 prov : wasGeneratedBy ? e p o c h e x e c t r a i n i n g ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>37 a</head><label>37</label><figDesc>provlake : A t t r i b u t e V a l u e ; 38 prov : value ? m o d e l t r a i n i n g e v a l v a l u e ; 39 prov : wasDerivedFrom ? m o d e l t r a i n i n g e v a l p s p . 40 ? m o d e l t r a i n i n g e v a l p s p 41 a provlake : A t t r i b u t e V a l u e ; 42 provlake : t a g "Model E v a l u a t i o n " .Listing 2: The same excerpt of Q7, without PROV-ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Examples of provenance queries in the lifecycle of scientific ML.</figDesc><table><row><cell>with the trained model</cell></row></table><note><p><p><p>ID Description</p>Q1</p>Given a trained model, what are the geographic coordinates, oil basin and field, and the number of seismic slices of the seismic in the training dataset? Q2 Given a trained model, what is the tile size, the noise filter threshold, and the ranges of seismic slices that were selected to generate the training set used to adjust this model? Q3 Given a training set, what are the values for all hyperparameters and the evaluation measure values associated</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Workflow Provenance Analysis in the Lifecycle of Scientific ML Execution Timing</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Data</cell><cell></cell><cell cols="2">Training Timing</cell><cell></cell><cell>Execution Environment</cell><cell></cell><cell></cell><cell>Data Store</cell><cell></cell><cell></cell><cell cols="2">Execution Software</cell><cell></cell></row><row><cell>Online</cell><cell>Offline</cell><cell>Machine Learning</cell><cell>Domain-specific</cell><cell>Execution Performance</cell><cell>Inter-training</cell><cell>Intra-training</cell><cell>HPC cluster</cell><cell>Cloud cluster</cell><cell>Standalone machine</cell><cell>File System</cell><cell>DBMS</cell><cell>Object Store</cell><cell>Workflow MS</cell><cell>Big Data Framework</cell><cell>ML Framework</cell><cell>Script</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 :</head><label>3</label><figDesc>Qualitative comparison of Q7 in terms of number of clauses with and without PROV-ML.</figDesc><table><row><cell>Q7 Query part</cell><cell cols="2">#Clauses w/ PROV-ML #Clauses w/o PROV-ML</cell></row><row><cell>Training stage</cell><cell>1</cell><cell>4</cell></row><row><cell>Epoch iteration</cell><cell>2</cell><cell>5</cell></row><row><cell>Model hyperparameters</cell><cell>6</cell><cell>7</cell></row><row><cell>Model</cell><cell>2</cell><cell>5</cell></row><row><cell>Model evaluation</cell><cell>3</cell><cell>6</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hesthaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://icerm.brown.edu/events/ht19-1-sml" />
		<title level="m">Scientific machine learning workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intelligent systems for geosciences: an essential research agenda</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Babaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheatham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ebert-Uphoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Horel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lermusiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pankratius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Restrepo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ridley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Physics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepDownscale: a deep learning strategy for high-resolution weather forecast</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient classification of seismic textures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chevitarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szwarcman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards supporting the life cycle of large-scale scientific experiments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Travassos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Martinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJBPIM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelhub: Deep learning lifecycle management</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatically tracking metadata and provenance of machine learning experiments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Boese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirschnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seufert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLS@NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient runtime capture of multiworkflow data using provenance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Netto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data lifecycle challenges in production machine learning: a survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Rec</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on provenance: What for? what form? what from?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Diestelkämper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Lahmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Special issue: The first provenance challenge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludäscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Barga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen-Boulakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Digiampietri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Futrelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krenek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mcphillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Munroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Podhorszki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schuchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Simmhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Slaughter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CCPE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data provenance: What next?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Rec</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facilitating reproducible research by investigating computational metadata</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thavasimani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Big Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Keeping track of user steering actions in dynamic workflows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Camata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Analyzing related raw data files through dataflows</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CCPE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards unified data and lifecycle management for deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accelerating the machine learning lifecycle with MLflow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nykodym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parkhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bulletin</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Análise de hiperparâmetros em aplicac ¸ões de aprendizado profundo por meio de dados de proveniência</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kunstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SBBD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring explainable artificial intelligence and autonomy through provenance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lucero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coronado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XAI@IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The next 5 years: what opportunities should the database community seize to maximize its impact?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<ptr target="https://www.w3.org/TR/prov-overview" />
		<title level="m">W3C PROV: an overview of the prov family of documents</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lawrynowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esteves</surname></persName>
		</author>
		<ptr target="https://www.w3.org/community/ml-schema/" />
		<title level="m">Mls: Machine learning schema community group</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Provenance data in the machine learning lifecycle in computational science and engineering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lourenc ¸o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Civitarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WORKS@Supercomputing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Provenance and scientific workflows: challenges and opportunities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going beyond provenance: explaining query answers with pattern-based counterbalances</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glavic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A characterization of workflow management systems for extreme-scale applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Filgueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pietri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Uncertainty quantification in computational predictive models for fluid dynamics using a workflow management engine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Rochinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Coutinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Uncertain. Quantif</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data reduction in scientific workflows using provenance monitoring and user steering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on collecting, managing, and analyzing provenance from scripts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Surv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Provenance-aware knowledge representation: a survey of data models and contextualized knowledge graphs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Sikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Philp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Sci. Eng</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Managing data lineage of o&amp;g machine learning models: the sweet spot for shale use case</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F D S</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Bayser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EAGE Digital</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Supporting the training of physics informed neural networks for seismic inversion using provenance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Codas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quinones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>AAPG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DfAnalyzer: runtime dataflow analysis of scientific applications using provenance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FAIR computational workflows</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen-Boulakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soiland-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Crusoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient provenance management via clustering and hybrid storage in big data environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How much domain data should be in provenance databases?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TaPP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Property graph schema optimization for domain-specific knowledge graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alotaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özcan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crossing analytics systems: A case for integrated provenance in data lakes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Suriarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Linking multiple workflow provenance traces for interoperable collaborative science</title>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludäscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WORKS@Supercomputing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Capturing and querying workflow runtime provenance with prov: a practical approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oca Ña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT/ICDT workshops</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Provenance of dynamic adaptations in user-steered dataflows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPAW</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distributed in-memory data management for workflow executions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJCS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bridging workflow and data provenance using strong links</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Troyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSDBM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mex vocabulary: a lightweight interchange format for machine learning experiments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Soru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSS. ACM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ML Schema: exposing the semantics of machine learning with schemas and ontologies</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Publio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ławrynowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soldatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Soru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zafar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<ptr target="https://ibm.biz/provlake" />
		<title level="m">Provlake website</title>
		<imprint/>
		<respStmt>
			<orgName>IBM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient mini-batch training for stochastic optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623612</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623612" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploiting execution provenance to explain difference between two data-intensive computations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thavasimani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cała</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Abstracting prov provenance graphs: A validity-preserving approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bryans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gamble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Curcin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bugdoc: algorithms to debug computational processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lourenc ¸o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On efficiently processing workflow provenance queries in spark</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rajmohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lohia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDCS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Integrating models through knowledge-powered data and process composition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Cobourn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferreira Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Peckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AGU Fall Meeting</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Vamsa: tracking provenance in data science scripts</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Namaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Floratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Psallidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Active provenance for data-intensive workflows: engaging users and developers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spinuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Magnoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Capturing and analyzing provenance from spark-based scientific workflows with SAMbA-RaP</title>
		<author>
			<persName><forename type="first">T</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L F</forename><surname>Falci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>Oca Ña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V N</forename><surname>Bedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">DARE to perform seismological workflows</title>
		<author>
			<persName><forename type="first">F</forename><surname>Magnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Casarotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rietbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Klampanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davvetas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spinuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Filgueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gemund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karkaletsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AGU Fall Meetings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Toward enabling reproducibility for data-intensive research using the whole tale platform</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hategan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kowalik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludascher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mcphillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nabrzyski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stodden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">YesWorkflow: A user-oriented, language-independent tool for recovering workflow information from scripts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mcphillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kolisnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aulenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belhajjame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Bocinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chirigati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Kintigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Macklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schildhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schwalm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bieda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludäscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">noWorkflow: a tool for collecting, analyzing, and managing provenance from python scripts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving reproducibility of data science pipelines through transparent provenance capture</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhagwat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Factsheets: Increasing trust in ai services through supplier&apos;s declarations of conformity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K E</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mojsilovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Piorkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Research &amp; Development</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Polyflow: a soa for analyzing workflow heterogeneous provenance data in distributed environments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Braga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Str</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>SBSI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Data lake management: challenges and opportunities</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nargesian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Arocena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Provchain: A blockchain-based data provenance architecture in cloud environment with enhanced privacy and availability</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kamhoua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Njilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCGrid</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fine-grained, secure and efficient data provenance on blockchain systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T A</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Model selection management systems: the next frontier of advanced analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Rec</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards a human-in-the-loop library for tracking hyperparameter tuning in deep learning development</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azeredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LaDaS@VLDB</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The role of machine learning in scientific workflows</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. HPC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Machine learning pipelines: provenance, reproducibility and FAIR data principles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Önig-Ries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Managing machine learning workflow components</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lourenc ¸o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Civitarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICSC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A knowledge-based approach for structuring cyclic workflows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lourenc ¸o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
