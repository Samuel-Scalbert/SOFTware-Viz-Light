<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine</title>
				<funder>
					<orgName type="full">DRIM PAMIR</orgName>
				</funder>
				<funder>
					<orgName type="full">DIM MAP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Thibault</forename><surname>Clérice</surname></persName>
							<email>thibaultclérice@chartes.psl.eu</email>
							<affiliation key="aff0">
								<orgName type="department">Centre Jean-Mabillon</orgName>
								<orgName type="institution">PSL Research University</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hisoma (UMR 5189)</orgName>
								<orgName type="institution" key="instit1">Université Lyon 3</orgName>
								<orgName type="institution" key="instit2">Université de Lyon (UDL)</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04F0DCF6152734DD9A80D7D7A7475962</idno>
					<idno type="DOI">10.46298/jdmdh.9806</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-23T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>kraken</term>
					<term>layout segmentation</term>
					<term>yolo</term>
					<term>htr</term>
					<term>ocr</term>
					<term>object detection</term>
					<term>historical document</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Layout Analysis (the identification of zones and their classification) and line segmentation are the first steps in Optical Character Recognition and similar tasks. The ability of identifying the main body of text from marginal text or running titles makes the difference between extracting the full text of a digitized book and noisy outputs. We show that most segmenters focus on pixel classification and that polygonization of this output has not been used as a target for the latest competitions on historical documents (ICDAR 2017 and onwards), despite being the focus in the early 2010s. We suggest that transitioning the task from pixel classification-based polygonization to object detection using isothetic rectangles might improve results in terms of speed and accuracy. We compare the output of Kraken and YOLOv5 in terms of segmentation and show that the latter severely outperforms the first on small datasets (1110 samples and below). We release two datasets for training and evaluation on historical documents as well as a new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of Kraken 4.1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I INTRODUCTION</head><p>In recent years, automatic text extraction has become an important activity in digital philology and, in general, in corpus creation for historical documents. A few different tools share the software market: Transkribus and eScriptorium (with its underlying and autonomous Kraken engine) are probably the two best known platforms. As model recognition improves over the years, the question of quasi-automatic corpus extraction from digitized manuscripts and early printed material has become more of a question of appropriate tools rather than HTR performances for specific fonts or languages and periods. On the side of Kraken, which has the advantage of being fully open-source, unlike Transkribus, the layout segmentation <ref type="bibr">[Kiessling et al., 2019a]</ref> has suffered from poor performances user-perception, specifically with small dataset (around a thousand samples for train/dev/test) or very small dataset (less than 300 for train and dev).</p><p>This results perception has slowed the improvement of automatic information extraction, where the main body of text could be drawn from a printed book or a manuscript by ignoring marginal texts such as footnotes or running title. If the segmenter is not able to recognize columns (or separate them) in medieval manuscripts or does not distinguish main bodies of text from foot notes, the co-occurrences of the extracted text are fooled by a wrong interpretation of the layout.</p><p>Extracting main body of works could result in creating data minable corpora of unprecedented sizes. We know of three research projects whose goals are hindered by the inability of Kraken to be trained with small datasets:</p><p>• Pierre-Carl Langlais, Jean-Baptiste Camps, Nicolas Baumard and Olivier Morin's work on the evolution of French literary fictions from 1050 to the 1920s; • Ariane Pinche's research project on medieval vernacular Légendiers, compilations of Saint's Lives in Old French, with manuscripts split in multiple columns; • Simon Gabay and Béatrice Joyeux-Prunelle's work on manuscripts sales and art exhibition catalogues where the identification of each entry in the dataset leads to a lower amount of data to parse through natural language processing for denoising. Layout Analysis has been one of the most frequent tasks in competition in conferences such as International Conference on Document Analysis and Recognition (ICDAR) and the International Workshop on Historical Document Imaging and Processing (HIP). However, historical document layout analysis competition saw the light of the day in 2011, as a joint venture from ICDAR2011 and HIP2011. The 2011 event limited itself to print documents and focused on both the region segmentation as well as the region classification operations. In the context of ICDAR2011's Historical Document Layout Analysis competition (HDLA2011) and in general, layout segmentation is understood as the operation of recognizing blocks of content from the background in a digitized document. Region classification, on the other hand, is understood as the qualification of the blocks found during layout segmentation into various different categories. During ICDAR2011, documents for the competition spanned from the 17 th to the early 20 th century and were segmented using polygons, which is the norm for this competition.</p><p>In 2011, methods for segmentation would mostly use a binarization step followed by some form of separator or content detection. ICDAR2017 had three relevant competitions:</p><p>• the competition on Page Object Detection (POD2017), whose dataset is composed of scientific papers in which the objective is to detect figures, tables, equations, etc.; • the competition on Historical Book Analysis (HBA2017), which has been renewed in 2019 (HBA2019); • the competition on Layout Analysis for Challenging Medieval Manuscripts (HisDoc-Layout-Comp). While POD2017 sets the objective as a bounding box detection task, both HBA and HisDoc-Layout-Comp treats the task as a pixel labelling one, a pixel can be assigned to multiple classes. Since 2017, deep learning models have emerged in the competition, specifically in HBA and HisDoc-Layout-Comp where CNN models shined.</p><p>However, the shift from polygon detection at ICDAR2011 to pixel-based categorization at IC-DAR 2019 creates a bias for the downstream task such as main body extraction: pixel categorization ignores the necessary serialization and clustering of said pixels into various zones that can then be filled with lines and content. This approach is specifically damageful in tools such as <ref type="bibr">Kraken (v4.1.2)</ref> as it led to the creation of "hacks", which zones being qualified with ColumnEven and ColumnOdd systems in order to ensure the differentiation of columns in the clustering step<ref type="foot" target="#foot_2">1</ref> . Note that even stronger models such as ARU-NET <ref type="bibr" target="#b8">[Grüning et al., 2018]</ref> only computes pixel classification and does not polygonize the output in its official implementation<ref type="foot" target="#foot_3">2</ref> .</p><p>With the specific purpose of training models on small datasets and document text extraction, we propose to approach the document segmentation as an object detection task, following the bounding box approach of ICDAR2017 instead of the polygon detection of ICDAR2011 or the pixel categorization of HBA or HisDoc-Layout-Comp. In this context, we propose two new datasets, a historical tabular document dataset (YALTAi-Tables <ref type="bibr">[Clérice, 2022b]</ref>) and a manuscripts and early printed book dataset (YALTAi-MSS-EPB <ref type="bibr">[Clérice, 2022a]</ref>). We do not propose a specific model but instead reuse You Only Look Once (YOLOv5)'s tools and models. Results show up to 100 times better results on column detection for tabular documents as well as double the score of main body document detection. As YOLOv5 base version is limited to "straight" boxes, it limits the application to lightly or unskewed documents. Bounding box can also overlap easily and we lose the precision of polygons. In summary, the contributions of this paper are 1. a proposal for shifting from polygon and pixel labelling to bounding box detection for documents that permit it at the layout detection stage (including a shift regarding evaluation of said results from pixel classification to object detection); 2. a new dataset for historical tabular documents, spanning from the 16 th to the early 20 th century; 3. a new dataset for the segmentation of historical documents such as manuscripts and early printed books (from the 9 th century to the 16 th ) following the Segmonto ontology for segmentation classification labels; 4. models for the detection of content regions according to both the Segmonto guidelines and a tabular approach; 5. a tool, YALTAi, which allows for</p><p>• the conversion of ALTO to YOLOv5 formats and vice versa;</p><p>• accessing a command line interface similar to Kraken interface which injects the region detection of YOLOv5 before using the line serialization of Kraken<ref type="foot" target="#foot_4">3</ref> </p><p>II BACKGROUND AND RELATED WORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>On a printed or manuscript page, conventions lead the organization of content on a page, such that element of a document can easily be processed by a human reader: a letter is usually formed of an opener, a body with one or many paragraphs, and a closer; a printed book usually contains a main body of texts -be it paragraphs or lines and poems -and a running title, a page number and optionally some marginal information such as footnotes. Literary manuscripts follow such an organization, but are generally composed of two or more columns of main body of texts per page, each column is read from left to right so that when Column A is finished we start reading Column B. Such an organization is crucial for the understanding of the content, as it allows the identification of contiguous blocks of main text and blocks of additional information for the human reader.</p><p>Segmonto provides a welcome ontology and syntax for specifying the organization of pages from a layout perspective. <ref type="bibr" target="#b7">Gabay et al. [2021]</ref> define a series of top-level graphical structures that can be found in digitized document from medieval manuscripts up to contemporaneous documents: main bodies of text are qualified as MainZone, marginal text such as footnotes and scribbles as MarginZone, paintings, photos and figures in general are represented by GraphicZone and specific medieval and modern early printed book features such as illuminated capitals fall under the category of DropCapital. These categorizations can be refined at a second level with project-specific vocabulary which can contain categorization such as MainZone:Entry for a dictionary entry or MarginText:Footnote. Documents automatically or manually tagged using this ontology allows for text extraction and data mining at scale without worrying with the noise of running titles or marginal text: preliminary work by <ref type="bibr" target="#b3">Christensen et al. [2022]</ref> shows this possibility.</p><p>In his article "Where is digital philology going"<ref type="foot" target="#foot_5">4</ref> , <ref type="bibr" target="#b2">Camps [2018]</ref> analysed that Handwritten Text Recognition (HTR) can now be placed upstream of philological study, with downstream research taking advantage of the mass of data and multi-scale analysis for different tasks, from quantitative palaeography to computational stylistics. This inclusion of HTR in the workflow of medievalist and philologists in general has profited from the development of easy-to-use, highly available and performant HTR and OCR user interface. Research infrastructure software such as Transkribus <ref type="bibr" target="#b13">[Kahle et al., 2017]</ref> or eScriptorium have taken their place in the humanist's office. <ref type="bibr">Kiessling et al. [2019b]</ref> present Kraken and eScriptorium. Unlike Transkribus, it is built around fully installable and open-source practices, allowing for a take-and-go approach and not making the user prisoner within an ecosystem. Kraken is the computational software for training, evaluation and inference of models for both segmentation and text recognition tasks. Before eScriptorium, it could be used only through its python API or most likely with its command-line interface (CLI). eScriptorium brought a user interface that allows for importing, segmenting, annotating and transcribing document, manually (no prediction provided by Kraken), semiautomatically (prediction provided and manually corrected) or automatically (prediction only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Works</head><p>In HDLA2011, <ref type="bibr" target="#b1">Antonacopoulos et al. [2011]</ref> defines layout analysis as both the actions of identifying "printed regions (Page segmentation) and labelling them according to the type of their content (Region Classification)". Competitions of page segmentation have been run at ICDAR since 2001 and mostly on contemporary documents. 2011 was the first historical document oriented competition but restricted itself to printed material up to the 17 th century. In Antonacopoulos et al. <ref type="bibr">[2009]</ref>'s review of ICDAR2009 Page Segmentation competition (PS2009), ground truth is defined by "isothetic polygons (i.e. a polygon having only horizontal and verti- cal edges) [as] such a representation enables a very accurate and efficient geometric description, especially for complex-shaped regions".</p><p>In the late 2000s and early 2010s, competitions were dominated by difficulties such as binarisation, recognition of separators (whether background as white space or printed such as column borders), combined with different approaches to image analysis. As of ICDAR2017, Convolutional Neural Networks and various other deep learning methods <ref type="bibr" target="#b9">[Hashmi et al., 2021</ref><ref type="bibr" target="#b20">, Pfitzmann et al., 2022]</ref> appeared on this subject, in HBA and HisDoc-Comp-Layout or in unrelated settings. However, <ref type="bibr" target="#b0">Antonacopoulos et al. [2009]</ref> showed that non-deep learning methods still gave good results on complex document layout with DSPH getting the highest score of the competition: <ref type="bibr" target="#b18">Lu and Dooms [2021]</ref> proposes a probabilistic approach to document segmentation and beats the first deep-learning-based method of the competition by 15 points (95.1% vs. 79.6%).</p><p>In parallel with the development of layout analysis research, object detection research has made great strides from still image application to extending performance to real-time detection. Contrary to layout analysis, the object detection task aims at detecting so-called objects in pictures such as dogs, people or boats. It mainly operates with "straight" bounding boxes (with sides of the bounding box being only horizontal and vertical lines) but it also exists with oriented bounding boxes or masks. Unlike layout analysis, it deals with three-dimensional information and as such different scales and different layers of information, from the foreground to the background. One of the main datasets for object detection model evaluation is the Microsoft COCO dataset and its evolutions. Described in <ref type="bibr" target="#b17">Lin et al. [2014]</ref>, COCO contains 91 kinds of objects and 2.5 million instances across 328,000 images.</p><p>In 2015, You Only Look Once (YOLO) was released with its version 1 (YOLOv1). Each YOLO model has had in its core goals to detect quickly and close to real-time instances of objects are at a high success rate. YOLOv1 used 24 convolution layers with two fully connected layers, splitting the picture into multiple cell and predicting for each cell what it could contain <ref type="bibr" target="#b11">[Jiang et al., 2022]</ref>. The use of YOLOv5 <ref type="bibr" target="#b12">[Jocher et al., 2022]</ref> or its predecessors in combination with OCR task is not novel <ref type="bibr" target="#b10">[Huang et al., 2019]</ref>, specifically for text transcription in scenes (e.g., panels or license plate <ref type="bibr" target="#b24">[Raj et al., 2022]</ref>) or highly formatted documents such as receipts  [ <ref type="bibr" target="#b16">Lin et al., 2022]</ref>. Similarly to our work, <ref type="bibr" target="#b19">Ning et al. [2021]</ref> uses YOLOv5 on modern and contemporary documents for table detection but limits its work to table instances with scores on the lower end of the spectrum of state-of-the-art (SOTA) methods but with significantly fewer parameters (240M parameters for the best score vs. 7M for the implemented solution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III METHOD</head><p>We propose to shift the layout analysis task from a pixel categorization or polygon segmentation task to an object detection task, where each region is an instance of a category of zone, mixing the segmentation and classification part of the layout analysis. For the purpose of this paper, we work only with isothetic rectangles which are produced by using the minimal and maximal X and Y of each polygon, which results in a more important area captured by each zone.</p><p>In terms of software and model training, each model is kept independent: YOLOv5 is trained within the YOLOv5 framework and can profit from its ecosystem while Kraken line segmentation is also trained separately. Both models are then used together at inference or evaluation time.</p><p>As the objective is to provide end users with a readily usable analysis of documents, the segmentation of zones should be fed in Kraken before the line detection is serialized, in order to dispatch lines into the various regions detected by YOLOv5(see Figure <ref type="figure" target="#fig_3">4</ref> for the details of the workflow at prediction time). In the YALTAi software library, YOLOv5's output is serialized in the same fashion as Kraken region's segmentation output. Using Kraken own region-line handler<ref type="foot" target="#foot_6">5</ref> , it then distributes lines segmented by Kraken into the YOLOv5 precomputed regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV EXPERIMENTAL SETUP</head><p>The experimental setup is focused on evaluating the gains in terms of accuracy of zone detection or segmentation in historical documents with "complex" layout. We compare YOLOv5 models to Kraken segmentation model using the same datasets (described below) with fix splits<ref type="foot" target="#foot_7">6</ref> and using Mean Average Precision (mAP) as well as Average Precision (AP) for each class of categories that are relevant to the qualitative analysis of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Manuscripts and Early Printed Books</head><p>We built two datasets for the experiment and both Kraken and YOLOv5 are trained on the same rectangle-converted dataset. The first dataset consists of manuscripts and early printed books (YALTAi-MSS-EPB) from the 9 th century up to the 17 th century. It is based on five datasets:</p><p>• CREMMA Medieval <ref type="bibr" target="#b21">[Pinche, 2022]</ref> focused on manuscripts in Old French with various layout and the inclusion of microfilm digitizations (microfilm being black and white pictures). • CREMMA Medieval LAT <ref type="bibr" target="#b6">[Clérice and Vlachou-Efstathiou, 2022]</ref> contains manuscripts in Latin with different types of content, from poetry to medical content, which provides a diversity of layouts. • Eutyches <ref type="bibr">[Vlachou-Efstathiou, 2022]</ref> focused on two manuscripts from the Early Middle Ages. • Two datasets from the Gallicorpora project ( <ref type="bibr">[Pinche et al., 2022a,b]</ref>) containing either incunabula, printed books with gothic scripts or manuscripts from the 15 th centuries, all written in Old or Middle French. On top of these five datasets, we produced an addendum of 593 annotated images for this paper to reach the objective of 1,000 images. This additional dataset provides a higher number of samples (123 different documents) and spans from the Middle Ages to the early modern era (17 th century). These new annotated pages were produced with early versions of the model and corrected by hand on the Roboflow application, thus they are more specific to the way YOLOv5 detects images and less prone to over-estimated area of polygons based on rectangle simplification as they are manually tuned to the images. The whole dataset uses Zone types from Segmonto, thus avoiding the project-specific and vocabulary free levels of subtypes and numbers that come with Segmonto deeper levels.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Tabular dataset</head><p>The tabular dataset was produced using a single source, the Lectaurep Repertoires dataset <ref type="bibr" target="#b25">[Rostaing et al., 2021]</ref>, which served as a basis for only the training and development split. The test set is composed of original data, from various documents, from the 17 th century up to the early 20 th with a single soldier war report. In contrast to the MSS-EPB test set, the test set for tabular datasets falls entirely out of the domain. It presents situations where column borders are neither drawn nor printed, adopting a masonry layout-particularly evident in the soldier war report (refer to Figure <ref type="figure" target="#fig_4">5</ref>)..</p><p>The tabular dataset contains four kinds of zones: Col (Columns), Header, Marginal, Text. The test set was only annotated with Columns and Header, as marginal and text were not clearly defined. The tabular dataset was simplified from the original dataset from <ref type="bibr" target="#b25">Rostaing et al. [2021]</ref>, as it used a known hack to work "as intended" with Kraken. We knowingly put some stress on Kraken on this test. As Kraken relies on pixel classification and less on polygon or bounding box detection, it usually merges together regions of the same kind that are too close to each other. To reduce the effect of these merging errors, the trick learned by the community is to split zones into alternating names, such as ColOdd and ColEven. In the Lectaurep dataset, we found around 16 different ways to describe columns, from Col1 to Col7, the case-different col1-col7 and finally ColPair and ColOdd, which we all reduced to Col (see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Training</head><p>Kraken models were trained according to two different configurations, one with an input resize of 1200 and a second with an input resize of 640, like the recommended YOLOv5 input size<ref type="foot" target="#foot_8">7</ref> . Although Kraken recommends since recently a 1800 resized input, the training, with shuffled input, crashed many times because of the GPU RAM limits (roughly 11 GB). The model was trained for 50 epochs and the best model was selected according to mAP scoring on the development set. Scoring with mAP was produced outside of Kraken as the latter does not provide a best model selection.</p><p>YOLOv5 models were trained with the latest commit (29d79a6) from the 2 nd of July. For both datasets, a model was trained using a YOLOv5x weights for initialization. A second YOLOv5n model was trained for the YALTAi-MSS-EPB dataset to evaluate the impact of the reduction of the size of the models on results: YOLOv5x model is advertised as a 166 MiB model while YOLOv5n weights only 4 MiB. Both models were trained during 50 epochs. The best model for evaluation was chosen according to YOLOv5 appointed best model.</p><p>Models were trained on a Nvidia RTX2080TI with 11 GB of RAM and a TDP of 250 W, on Ubuntu with CUDA 10.2 on drivers 440.118.02. Training for both tools used the same dataset with boundinges box coordinates rather than complex polygon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>The evaluation is done based on mean average precision (mAP) and average precision (AP).</p><p>For fairness of the evaluation and to avoid any discrepancies in calculation framework, we used mean-average-precision==2021.4.26.0 on the converted ALTO or the YOLOv5 text format. mAP is used with an intersection over union threshold of 0.5 (usually noted mAP@.5). Evaluation for Kraken used bounding box converted from the complex polygons it predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results for YALTAi MSS-EPB</head><p>On the YALTAi MSS-EPB, the mAP of Kraken (6.95%) results is largely beaten by the YOLOv5x (47.75%) and the YOLOv5n model (34.63%, see Table <ref type="table" target="#tab_4">4</ref>). This lower results is due in part to lower results for the MainZone and DropCapital recognition, which are the most common objects in all splits (YOLOv5n, the worst-performing YOLOv5 model, doubles the performance of Kraken on MainZone and nearly triples them on DropCapital). On the other hand, Kraken is also unable to beat a 0% AP on any of the less frequent objects such as MarginText, Numbering, QuireMarks or RunningTitle, lowering its capacity to reach a higher mAP. mAP being a macro-average metric, it favours models performing on all classes rather than a few classes. However, the score discrepancy on the MainZone is important enough that it wagers using YOLOv5 instead of Kraken segmenter for extracting main body of texts.</p><p>As for the difference between the nano (YOLOv5n) and the extra-large (YOLOv5x) models, the difference is small enough on the most frequent zone to accept a performance drop for a faster computation time. However, the YOLOv5n is largely beaten on less frequent classes such as MarginTextZone or NumberingZone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results for YALTAi-Table</head><p>The results on YALTAi table are somewhat unfair to the Kraken segmenter but shows the impact of the object detection approach on a segmentation task. As seen in the example output at Figure <ref type="figure">6</ref>, Kraken is unable to distinguish multiple columns and often merges the various detected zones.</p><p>On the other hand, YOLOv5x shows a very high capacity to extend to different kinds of inputs as the input tabular document is not from the LECTAUREP dataset nor is the same in terms of content. At best, the document is also a printed document with various columns filled with manuscript content. In terms of genericity and ability to learn, the test results show that YOLO can learn from very few documents compared to Kraken, as it misses only a few columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Efficiency</head><p>We monitored the GPU usage in terms of memory, time and peak power consumption during the training of all models (see Table <ref type="table" target="#tab_5">5</ref>). On the same system, YOLOv5 has shown a better efficiency than Kraken for each model and metric except for Kraken on the tabular dataset and power consumption. The training time seems to scale with the batch size but we are unable to confirm it, as Kraken does not allow to use batches for training. The inference time was only reported for YOLOv5 models to show the overhead that it would cost per image prediction: YOLOv5 has been designed for quasi-real time prediction on video streams, which explains the very low inference time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI CONCLUSIONS</head><p>Handwritten Text Recognition or Optical Character Recognition are the second step in text acquisition, right after layout analysis. However, in the main open source system used in the humanities, Kraken, the layout analysis under-performs severely on small datasets, specifically for the same-class regions that are close neighbours in the image, as these are often merged. For text extraction, identifying regions is of primordial importance as they can help filter out noise such as running titles or marginal addendum, for both manuscripts and printed books.</p><p>In this paper, we proposed to shift the layout analysis task to an object detection task, shifting in terms of predicted output from polygons to isothetic bounding box in the context of YOLOv5. We show that YOLOv5 severely outperforms Kraken on the same dataset for region segmentation, both in terms of evaluation metrics and GPU efficiency (Peak power usage and RAM).</p><p>The drawback of this approach lies in the bounding box limitations: bounding box limits us to regions that are rectangular and cannot cover complex polygons such as P-or C-shaped ones.</p><p>We used YOLOv5 which limits us to isothetic bounding boxes but note that a YOLOv5 Oriented Bounding Box (YOLOv5-OBB) adaptation exists and could help for documents such as tables for a "perfect" coverage of each column without pre-processing and deskewing.</p><p>Taking advantage of the easy to use, open-sourced and openly licensed python API of Kraken, we are able to often a new package, YALTAi, which simply plugs in the YOLOv5 model before line dispatching across regions. It possess the same segmentation CLI interface as Kraken but adds a simple --yolo parameter that allows for selecting a YOLOv5 model for region segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOFTWARE</head><p>Source code is available at https://github.com/ponteineptique/YALTAi and software can be installed using pip install yaltai.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Prediction on the test set with YOLOv5x models for the Segmonto dataset (three pictures on the left) and the tabular dataset (last picture, columns are in alternating colours for readability). Illustrations are in orange (first picture top), drop capitals are in darker orange, marginal text in green, yellow is the main body of text.</figDesc><graphic coords="4,70.87,228.95,453.55,135.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Typical Handwritten Text Recognition (HTR) workflow. A user uploads a set of pictures from a digitized book, segments the document both at the level of the layout and the lines, corrects the segmentation, provides a transcription or correct an automatic one and then fine-tune or creates models for other pages of the same document or documents of the same kind.</figDesc><graphic coords="6,70.87,70.87,453.51,176.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of polygon in the ground truth when the Kraken prediction is correct on the left. On the right, its simplification into an isothetic rectangle for object detection.</figDesc><graphic coords="7,184.25,70.87,226.76,150.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Workflow and responsibilities at inference time.</figDesc><graphic coords="7,116.22,265.37,362.81,95.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Tabular dataset excerpts. Two images on the left are from the Lectaurep dataset, three images on the right are original data for the test set.</figDesc><graphic coords="9,70.87,70.87,453.55,128.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative and qualitative description of YALTAi MSS EPB. "Books" can be full manuscripts or printed books. Original data are previously unpublished data produced directly using bounding boxes.</figDesc><table><row><cell>Dataset</cell><cell cols="4">"Books" Pages Starting century Ending century</cell><cell cols="2">Type With B&amp;W</cell></row><row><cell>[Pinche, 2022]</cell><cell>13</cell><cell>263</cell><cell>12</cell><cell cols="2">15 Manuscripts</cell><cell>Yes</cell></row><row><cell>[Clérice and Vlachou-Efstathiou, 2022]</cell><cell>7</cell><cell>30</cell><cell>13</cell><cell cols="2">15 Manuscripts</cell><cell>Yes</cell></row><row><cell>[Vlachou-Efstathiou, 2022]</cell><cell>2</cell><cell>129</cell><cell>9</cell><cell cols="2">9 Manuscripts</cell><cell>No</cell></row><row><cell>[Pinche et al., 2022a]</cell><cell>1</cell><cell>20</cell><cell>15</cell><cell cols="2">15 Manuscripts</cell><cell>No</cell></row><row><cell>[Pinche et al., 2022b]</cell><cell>4</cell><cell>80</cell><cell>16</cell><cell>16</cell><cell>Printed</cell><cell>No</cell></row><row><cell>Original Data</cell><cell>123</cell><cell>593</cell><cell>9</cell><cell>17</cell><cell>Mixed</cell><cell>No</cell></row><row><cell>Total</cell><cell cols="2">150 1110</cell><cell>9</cell><cell>17</cell><cell>Mixed</cell><cell>Yes</cell></row></table><note><p><p><p><p><p>Data were mixed semirandomly into 3 split for training (854 images) and development (</p>154</p>), with works being able to be in two or more splits. Test (</p>139</p>) is only composed of different sources. The MainZone bounding box are the largest areas of all zones and just behind the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 2 for the statistical details).</figDesc><table><row><cell></cell><cell cols="6">Train Dev Test Total Average area Median area</cell></row><row><cell>DropCapitalZone</cell><cell cols="4">1537 180 222 1939</cell><cell>0.45</cell><cell>0.26</cell></row><row><cell>MainZone</cell><cell cols="4">1408 253 258 1919</cell><cell>28.86</cell><cell>26.43</cell></row><row><cell>NumberingZone</cell><cell>421</cell><cell>57</cell><cell>76</cell><cell>554</cell><cell>0.18</cell><cell>0.14</cell></row><row><cell>MarginTextZone</cell><cell>396</cell><cell>59</cell><cell>49</cell><cell>504</cell><cell>1.19</cell><cell>0.52</cell></row><row><cell>GraphicZone</cell><cell>289</cell><cell>54</cell><cell>50</cell><cell>393</cell><cell>8.56</cell><cell>4.31</cell></row><row><cell>MusicZone</cell><cell>237</cell><cell>71</cell><cell>0</cell><cell>308</cell><cell>1.22</cell><cell>1.09</cell></row><row><cell>RunningTitleZone</cell><cell>137</cell><cell>25</cell><cell>18</cell><cell>180</cell><cell>0.95</cell><cell>0.84</cell></row><row><cell>QuireMarksZone</cell><cell>65</cell><cell>18</cell><cell>9</cell><cell>92</cell><cell>0.25</cell><cell>0.21</cell></row><row><cell>StampZone</cell><cell>85</cell><cell>5</cell><cell>1</cell><cell>91</cell><cell>1.69</cell><cell>1.14</cell></row><row><cell>DigitizationArtefactZone</cell><cell>1</cell><cell>0</cell><cell>32</cell><cell>33</cell><cell>2.89</cell><cell>2.79</cell></row><row><cell>DamageZone</cell><cell>6</cell><cell>1</cell><cell>14</cell><cell>21</cell><cell>1.50</cell><cell>0.02</cell></row><row><cell>TitlePageZone</cell><cell>4</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell>48.27</cell><cell>63.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Distribution of objects instances across datasets splits after random shuffling of pages for the YALTAi-MSS-EPB dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Table3for the statistical details). Annotation and details about the annotations across splits of the tabular dataset.</figDesc><table><row><cell></cell><cell cols="6">Train Dev Test Total Average area Median area</cell></row><row><cell>Col</cell><cell cols="4">724 105 829 1658</cell><cell>9.32</cell><cell>6.33</cell></row><row><cell>Header</cell><cell>103</cell><cell>15</cell><cell>42</cell><cell>160</cell><cell>6.78</cell><cell>7.10</cell></row><row><cell>Marginal</cell><cell>60</cell><cell>8</cell><cell>0</cell><cell>68</cell><cell>0.70</cell><cell>0.71</cell></row><row><cell>Text</cell><cell>13</cell><cell>5</cell><cell>0</cell><cell>18</cell><cell>0.01</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Scores of Kraken and YOLOv5 best models for the Segmonto MSS and Early Printed Books Dataset. Zones(Main, Graphic, Drop Capital, Margin Text, Numbering, Quire Marks, Running Title and  Stamp)  are the most important zones from the test dataset. Scores for zones are the Average Precision. Both Mean Average Precision and Average Precision are given in percents. Surprisingly, YOLOv5n beats YOLOv5x on RunningTitle but is most often largely outperformed on other zones. Kraken is completely outperformed with YOLOv5x more than doubling its scores on the main body zones, and roughly multiplying it by 7 times for the mean average precision. Figure 6: On the left, scores of Kraken and YOLOv5 best models for the Table Dataset. Col and Header are the only annotated zones in the test set. Scores for zones are the Average Precision. Both Mean Average Precision and Average Precision are given in percents. On the right, two examples of predictions (Kraken on the left, YOLOv5 on the right): we see that Kraken does not separate columns and merges up to four columns while YOLOv5x fails to see only two columns but splits correctly the zones. See 4.1.2 for more explanation on the difficulties of the Kraken segmenter.</figDesc><table><row><cell></cell><cell cols="8">mAP Main Graphic DropCapital MarginText Numbering QuireMarks RunningTitle Stamp</cell></row><row><cell>Kraken</cell><cell>6.98 43.5</cell><cell>16.1</cell><cell>23.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">YOLOv5x 47.75 91.7</cell><cell>48.4</cell><cell>69.2</cell><cell>48.3</cell><cell>75.8</cell><cell>46.3</cell><cell>45.6</cell><cell>100</cell></row><row><cell cols="2">YOLOv5n 34.63 87.0</cell><cell>44.2</cell><cell>54.6</cell><cell>13.9</cell><cell>43.0</cell><cell>25.0</cell><cell>48.1</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Other metrics related to Kraken and YOLOv5. YOLOv5x produces the heaviest model (175mb, 35 times the weight of the Kraken model) but uses at most 77% of the maximum power draw of Kraken. Training time is generally doubled under Kraken but corresponds roughly to the batch size division. Prediction time is per image. For the Segmonto model, the maximum GPU RAM usage is halved for YOLOv5n, while using a batch of 2, but is nearly three times the footprint of Kraken for the Tabledataset. Training and prediction occured on a NVIDIA RTX2080TI with a rated 250W of powers and 11,018MiB of RAM on drivers 440.118.02 and CUDA 10.2.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Journal of Data Mining and Digital Humanities</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>http://jdmdh.episciences.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>While undocumented, this was the approach taken by many projects including LECTAUREP under the advices of the SCRIPTA team who is working on Kraken and eScriptorium.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>https://github.com/TobiasGruening/ARU-Net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>And thus, looking twice at the picture, once from YOLOv5, and a second time from Kraken line segmenter.Journal of Data Mining and Digital Humanities</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>French: "Où va la philologie numérique ?".Journal of Data Mining and Digital Humanities</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>Kraken uses the interpolation of the baseline to find the middle point and then uses the presence of the calculated point inside the region polygon to decide whether it belongs to the region.Journal of Data Mining and Digital Humanities</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>Splits are available in the dataset published with the present article.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8"><p>However, by its very nature, Kraken's algorithm is expected to suffer from lower resolution: comparison is here to provide an insight regarding image processing time and compute footprint in general rather than scores.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was made possible through the funding of the <rs type="funder">DIM MAP</rs>, now called <rs type="funder">DRIM PAMIR</rs>.</p></div>
<div><head>References</head></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Icdar 2009 page segmentation competition</title>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Antonacopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Pletschacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bridson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2009.275</idno>
	</analytic>
	<monogr>
		<title level="m">2009 10th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1370" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Historical document layout analysis competition</title>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Antonacopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Clausner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Pletschacher</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2011.301</idno>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Camps</surname></persName>
		</author>
		<ptr target="https://halshs.archives-ouvertes.fr/halshs-01674953" />
		<title level="m">Où va la philologie numérique ? Fabula LHT (Littérature, histoire, théorie)</title>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gallic(orpor)a: Traitement des sources textuelles en diachronie longue de Gallica</title>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariane</forename><surname>Pinche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Gabay</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-03716534" />
	</analytic>
	<monogr>
		<title level="m">DataLab de la BnF</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">YALTAi: Segmonto Manuscript and Early Printed Book Dataset</title>
		<author>
			<persName><forename type="first">Clérice</forename><surname>Thibault</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6814770</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6814770" />
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">YALTAi: Tabular dataset</title>
		<author>
			<persName><forename type="first">Clérice</forename><surname>Thibault</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6827706</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6827706" />
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cremma Medieval Latin, 6</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Clérice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malamatenia</forename><surname>Vlachou-Efstathiou</surname></persName>
		</author>
		<ptr target="https://github.com/HTR-United/cremma-medieval-lat" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmonto: common vocabulary and practices for analysing the layout of manuscripts (and more)</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariane</forename><surname>Pinche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Jahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Document Analysis and Recognition (ICDAR 2021)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Two-Stage Method for Text Line Detection in Historical Documents</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Grüning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gundram</forename><surname>Leifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Strauß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Labahn</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1802.03345" />
	</analytic>
	<monogr>
		<title level="m">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Castabdetectors: Cascade network for table detection in document images with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName><forename type="first">Azeem</forename><surname>Khurram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hashmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Pagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afzal</forename><surname>Zeshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">214</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A yolo-based table detection method</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinqin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangcai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="813" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A review of yolo algorithm developments</title>
		<author>
			<persName><forename type="first">Peiyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daji</forename><surname>Ergu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2022.01.135</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1877050922001363" />
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<idno type="ISSN">1877-0509</idno>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="1066" to="1073" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ultralytics/yolov5: v6.1 -TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference</title>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jirka</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghye</forename><surname>Nanocode012</surname></persName>
		</author>
		<author>
			<persName><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacong</forename><surname>Taoxie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Abhiram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jebastin</forename><surname>Nadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Laughing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Skalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristi</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Fati</surname></persName>
		</author>
		<author>
			<persName><surname>Mammana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Alexwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Yiwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurentiu</forename><surname>Diaconu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6222936</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6222936" />
		<imprint>
			<date type="published" when="1900-02">1900. February 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transkribus-a service platform for transcription, recognition and retrieval of historical documents</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Kahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Colutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Hackl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Mühlberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BADAM: A public dataset for baseline detection in arabic-script manuscripts</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Kiessling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Stökl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ezra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">Thomas</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352631.3352648</idno>
		<ptr target="https://doi.org/10.1145/3352631.3352648" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, HIP &apos;19</title>
		<meeting>the 5th International Workshop on Historical Document Imaging and Processing, HIP &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Associa-tion for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">eScriptorium: an open source platform for historical document analysis</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Kiessling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Tissot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Stökl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ezra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="19" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic receipt recognition system based on artificial intelligence technology</title>
		<author>
			<persName><forename type="first">Cheng-Jian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Ling</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3390/app12020853</idno>
		<ptr target="https://www.mdpi.com/2076-3417/12/2/853" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<idno type="ISSN">2076-3417</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR, abs/1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic homogeneity for document image segmentation</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Dooms</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107591</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">107591</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MT-YOLOv5: Mobile terminal table detection model based on YOLOv5</title>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-6596/1978/1/012010</idno>
		<ptr target="https://doi.org/10.1088/1742-6596/1978/1/012010" />
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12010</biblScope>
			<date type="published" when="1978-07">1978. jul 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Doclaynet: A large humanannotated dataset for document-layout segmentation</title>
		<author>
			<persName><forename type="first">Birgit</forename><surname>Pfitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Dolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">S</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Staar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3743" to="3751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cremma Medieval</title>
		<author>
			<persName><forename type="first">Ariane</forename><surname>Pinche</surname></persName>
		</author>
		<ptr target="https://github.com/HTR-United/cremma-medieval" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Données HTR manuscrits du 15e siècle, 6 2022a</title>
		<author>
			<persName><forename type="first">Ariane</forename><surname>Pinche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noé</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Christensen</surname></persName>
		</author>
		<ptr target="https://github.com/Gallicorpora/HTR-MSS-15e-Siecle" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Données HTR imprimés gothiques du 16e siècle, 6 2022b</title>
		<author>
			<persName><forename type="first">Ariane</forename><surname>Pinche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malamatenia</forename><surname>Vlachou-Efstathiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Christensen</surname></persName>
		</author>
		<ptr target="https://github.com/Gallicorpora/HTR-imprime-gothique-16e-siecle" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">License plate recognition system using yolov5 and cnn</title>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchika</forename><surname>Malhotra</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICACCS54159.2022.9784966</idno>
	</analytic>
	<monogr>
		<title level="m">2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="372" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Notaires de Paris -Répertoires, ground truth for various Parisian registries of notary deeds</title>
		<author>
			<persName><forename type="first">Aurélia</forename><surname>Rostaing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alix</forename><surname>Chagué</surname></persName>
		</author>
		<ptr target="https://github.com/HTR-United/lectaurep-repertoires" />
	</analytic>
	<monogr>
		<title level="m">French 19th and 20th centuries)</title>
		<imprint>
			<date type="published" when="2021">12 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">41 -Eutyches &quot;de uerbo</title>
		<author>
			<persName><forename type="first">Malamatenia</forename><surname>Vlachou-Efstathiou</surname></persName>
		</author>
		<author>
			<persName><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">O</forename><surname>Lat</surname></persName>
		</author>
		<ptr target="https://github.com/malamatenia/Eutyches" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
