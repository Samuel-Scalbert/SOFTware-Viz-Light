<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A scalable and effective rough set theory-based approach for big data pre-processing</title>
				<funder ref="#_4hus56Q">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-02">2 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zaineb</forename><surname>Chelly Dagdia</surname></persName>
							<email>zaineb.chelly-dagdia@inria.fr</email>
							<idno type="ORCID">0000-0002-2551-6586</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">LORIA</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">Université de Lorraine</orgName>
								<address>
									<postCode>54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<settlement>Aberystwyth</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">LARODEC</orgName>
								<orgName type="institution">Institut Supérieur de Gestion de Tunis</orgName>
								<address>
									<settlement>Tunis</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Zarges</surname></persName>
							<email>c.zarges@aber.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<settlement>Aberystwyth</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaël</forename><surname>Beck</surname></persName>
							<email>beck@lipn.univ-paris13.fr</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Laboratory (LIPN)</orgName>
								<orgName type="institution">University Paris-North</orgName>
								<address>
									<addrLine>13</addrLine>
									<settlement>Villetaneuse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mustapha</forename><surname>Lebbah</surname></persName>
							<email>mustapha.lebbah@lipn.univ-paris13.fr</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Laboratory (LIPN)</orgName>
								<orgName type="institution">University Paris-North</orgName>
								<address>
									<addrLine>13</addrLine>
									<settlement>Villetaneuse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zaineb</forename><surname>Chelly</surname></persName>
							<email>chelly.zaineb@gmail.com</email>
						</author>
						<title level="a" type="main">A scalable and effective rough set theory-based approach for big data pre-processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-02">2 May 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">F4C44A8B38209F3A7897AC75BB39668F</idno>
					<idno type="DOI">10.1007/s10115-020-01467-y</idno>
					<note type="submission">Received: 13 September 2019 / Revised: 16 March 2020 / Accepted: 21 March 2020 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-23T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Big data</term>
					<term>Data pre-processing</term>
					<term>Rough set theory</term>
					<term>Distributed processing</term>
					<term>Scalability</term>
					<term>High-performance computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A big challenge in the knowledge discovery process is to perform data pre-processing, specifically feature selection, on a large amount of data and high dimensional attribute set. A variety of techniques have been proposed in the literature to deal with this challenge with different degrees of success as most of these techniques need further information about the given input data for thresholding, need to specify noise levels or use some feature ranking procedures. To overcome these limitations, rough set theory (RST) can be used to discover the dependency within the data and reduce the number of attributes enclosed in an input data set while using the data alone and requiring no supplementary information. However, when it comes to massive data sets, RST reaches its limits as it is highly computationally expensive. In this paper, we propose a scalable and effective rough set theory-based approach for large-scale data pre-processing, specifically for feature selection, under the Spark framework. In our detailed experiments, data sets with up to 10,000 attributes have been considered, revealing that our proposed solution achieves a good speedup and performs its feature selection task well without sacrificing performance. Thus, making it relevant to big data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a broad variety of domains, data are being gathered and stored at an intense pace due to the Internet and the widespread use of databases <ref type="bibr" target="#b13">[7]</ref>. This ongoing rapid growth of data that led to a new terminology 'big data' has created an immense need for a novel generation of computational techniques, theories and approaches to extract useful information, i.e., knowledge, from these voluminous gathered data. These theories and approaches are the key elements of the emerging domain of knowledge discovery in databases (KDD) <ref type="bibr" target="#b26">[21]</ref>. More precisely, big data arise with many challenges such as in clustering <ref type="bibr" target="#b40">[35]</ref>, in classification <ref type="bibr" target="#b39">[34]</ref>, in mining <ref type="bibr" target="#b43">[38]</ref> but mainly in dimensionality reduction and more precisely in feature selection as this is usually a source of potential data loss <ref type="bibr" target="#b19">[13]</ref>. This motivated researchers to build an efficient and an automated knowledge discovery process with a special focus on its third step, namely data reduction.</p><p>Data reduction is an important point of interest as many real-world applications may have a very large number of features (attributes) <ref type="bibr" target="#b46">[41]</ref>. For instance, among the most practically relevant and high-impact applications are biochemistry, genetics and molecular biology. In these biological sciences, the collected data, e.g., gene expression data, may easily have a number of attributes which is more than 10,000 <ref type="bibr">[1]</ref>. However, not all of these attributes are crucial and needed since many of them can be redundant in the context of some other features or even completely irrelevant and insignificant to the task being handled. Therefore, several important issues arise when learning in such a situation among these are the problems of over-fitting to insignificant aspects of the given data, as well as the computational burden due to the process of several similar attributes that give some redundant information <ref type="bibr" target="#b23">[17]</ref>. These problems may decrease the performance of any learning technique, e.g., a classification algorithm. Hence, to solve these problems, it is an important and significant research direction to automatically look for and only select a small subset of relevant attributes from the initial large set of attributes; that is, to perform feature selection. In fact, by removing the irrelevant and redundant attributes, feature selection is capable of reducing the dimensionality of the input data while speeding up the learning process, simplifying the learned model as well as increasing the performance <ref type="bibr" target="#b15">[9,</ref><ref type="bibr" target="#b23">17]</ref>.</p><p>At an abstract level, to reduce the high dimensionality of data sets, suitable techniques can be applied with respect to the requirements of the future KDD process. The taxonomy of these techniques falls into two main groups namely feature selection techniques and feature extraction techniques <ref type="bibr" target="#b23">[17]</ref>. The main difference between the two approaches is that techniques for feature selection select a subset from the initial features while techniques for feature extraction create new attributes from the initial feature set. More precisely, feature extraction techniques transform the underlying semantic (meaning) of the attributes while feature selection techniques preserve the data set semantics in the process of reduction. In knowledge discovery, feature selection techniques are notably desirable as these ease the interpretability of the output knowledge. In this paper, we mainly focus on the feature selection category for big data pre-processing.</p><p>Technically, feature selection is a challenging process due to the very large search space that reflects the combinatorially large number of all possible feature combinations to select from. This task is becoming more difficult as the total number of attributes is increasing in many big data application domains combined with the increased complexity of those problems. Therefore, to cope with the vast amount of the given data, most of the state-ofthe-art techniques employ some degree of reduction, and thus, an effective feature reduction technique is needed.</p><p>As one of data analysis techniques, rough set theory (RST) <ref type="bibr" target="#b32">[27]</ref>-based approaches have been successfully and widely applied in data mining and knowledge discovery <ref type="bibr" target="#b22">[16]</ref>, and particularly for feature selection <ref type="bibr" target="#b36">[31]</ref>. Nonetheless, in spite of being powerful rough set-based feature selection techniques, most of the classical algorithms are sequential ones, computationally expensive and can only handle non-large data sets. The fact that the RST-based algorithms are computationally expensive and the reason behind the methods' incapacity to handle high dimensional data is explained by the need to first generate all the possible combinations of attributes at once, then process these in turn to finally select the most pertinent and relevant set of attributes.</p><p>Nevertheless, as previously mentioned, since the number of attributes is becoming very large this task becomes more critical and challenging, and at this point the RST-based approaches reach their limits. More precisely, it is unfeasible to generate all the possible attribute combinations at once because of both hardware and memory constraints.</p><p>This leads us to advance in this disjointed field and broaden the application of the theory of rough sets in the domain of data mining and knowledge discovery for big data. This paper proposes a scalable and effective algorithm based on rough sets for large-scale data pre-processing, and specifically for big data feature selection. Based on a distributed implementation design using both Scala and the Apache Spark framework <ref type="bibr" target="#b41">[36]</ref>, our proposed distributed algorithm copes with the RST computational inefficiencies and its restriction to be only applied to non-large data sets. To deeply analyze the proposed distributed approach, experiments on big data sets with up to 10,000 features will be carried out for feature selection and classification. Results demonstrate that our proposed solution achieves a good speedup and performs its feature selection task well without sacrificing performance, making it relevant to big data.</p><p>The rest of this paper is structured as follows. Section 2 presents preliminary information and related work. Section 3 reviews the fundamentals of rough set theory for feature selection. Section 4 formalizes the motivation of this work and introduces our novel distributed algorithm based on rough sets for large-scale data pre-processing. The experimental setup is introduced in Sect. <ref type="bibr" target="#b11">5</ref>. The results of the performance analysis are given in Sect. <ref type="bibr" target="#b12">6</ref>, and the conclusion is given in Sect. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature review</head><p>Feature selection is defined as the process that selects a subset of the most relevant and pertinent attributes from a large input set of original attributes. For example, feature selection is the task of finding key genes (i.e., biomarkers) from the very huge number of candidate genes in biological and biomedical problems <ref type="bibr" target="#b9">[3]</ref>. It is also the task of discovering core indicators (i.e., attributes) to describe the dynamic business environment <ref type="bibr" target="#b30">[25]</ref>, or to select key terms (e.g., words or phrases) in text mining <ref type="bibr" target="#b8">[2]</ref> or to construct essential visual contents (e.g., pixel, color, texture, or shape) in image analysis <ref type="bibr" target="#b21">[15]</ref>.</p><p>In many data mining and machine learning real-world problems, feature selection became a crucial and highly important data pre-processing step due to the abundance of noisy, irrelevant and/or misleading features that are in big data. To cope with this, the usefulness of a feature can be measured by its relevancy as well as its redundancy. In fact, a feature is considered to be relevant if it can predict the decision feature(s); otherwise, it is said to be irrelevant as it provides no useful information with reference to any context. On the other hand, a feature is considered to be redundant if it provides the same piece of information for the currently selected features; this means that it is highly correlated with them. Hence, feature selection must provide beneficial results from big data as it should detect those attributes that present a high correlation with the decision feature(s), but at the same time are uncorrelated with each other.</p><p>In the literature, feature selection techniques can be broadly grouped into two main approaches which are filter approaches and wrapper approaches <ref type="bibr" target="#b15">[9,</ref><ref type="bibr" target="#b23">17]</ref>. The key difference between the two approaches is that wrapper approaches involve a specific learning algorithm, e.g., classification algorithm, when it comes to evaluating the attribute subset. The applied learning algorithm is mainly used as a black box by the wrapper approach to evaluate the quality (i.e., the classification performance) of the selected attribute set. Technically, when an algorithm performs feature selection in an independent way of any learning algorithm, the approach is defined as a filter where the set of the irrelevant features are filtered out before the induction process. Filter approaches tend to be applicable to most real-world domains since they are independent from any specific induction algorithm. On the other side, if the evaluation task is linked or dependent to the task of the learning algorithm then the feature selection approach is a wrapper technique. This approach searches through the attribute subsets space using the training (or validation) accuracy value of a specific induction algorithm as the measure of utility for a candidate subset. Therefore, these approaches may generate subsets that are overly explicit and specific to the used learning algorithm, and hence, any modification in the learning model might render the attribute set suboptimal.</p><p>Each of these two feature selection categories has its advantages and shortcomings where the main distinguishing aspects are the computational speed and the possibility of over-fitting. Overall, in terms of speed of computation, filter algorithms are usually computationally less expensive and more general than the wrapper techniques. Wrappers are computationally expensive and can easily break down when dealing with a very large number of attributes. This is due to the adoption of a learning algorithm in the evaluation process of subsets <ref type="bibr" target="#b29">[24,</ref><ref type="bibr" target="#b31">26]</ref>. In terms of over-fitting, the wrapper techniques have a higher learning capability so are more likely to overfit than filter techniques. It is important to mention that in the literature, some researchers classified feature selection techniques into three separate categories, namely the wrapper techniques, the embedded techniques and the filter techniques <ref type="bibr" target="#b29">[24]</ref>. The embedded approaches tend to fuse feature selection and the learning approach into a single process. For large-scaled data sets having large number of features, the filter methods are usually a good option. Focusing on this category is the main scope of this paper.</p><p>Meanwhile, in the context of big data, it is worth mentioning that a detailed study was conducted in <ref type="bibr" target="#b12">[6]</ref> where authors performed a deep analysis of the scalability of the state-ofthe-art feature selection techniques that belong to the filter, the embedded and the wrapper techniques. In <ref type="bibr" target="#b12">[6]</ref>, it was demonstrated that the state-of-the-art feature selection techniques will obviously have scalability issues when dealing with big data. Authors have proved that the existent techniques will be inadequate for handling a high number of attributes in terms of training time and/or effectiveness in selecting the relevant set of features. Thus, the adaptation of feature selection techniques for big data problems seems essential and it may require the redesign of these algorithms and their incorporation in parallel and distributed environments/frameworks. Among the possible alternatives is the MapReduce paradigm <ref type="bibr" target="#b16">[10]</ref> which was introduced by Google and which offers a robust and efficient framework to deal with big data analysis. Several recent works have been concentrated on parallelizing and distributing machine learning techniques using the MapReduce paradigm <ref type="bibr" target="#b45">[40,</ref><ref type="bibr" target="#b48">43,</ref><ref type="bibr" target="#b49">44]</ref>. Recently, a set of new and more flexible paradigms have been proposed aiming at extending the standard MapReduce approach, mainly Apache Spark<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b41">[36]</ref> which has been applied with success over a number of data mining and machine learning real-world problems <ref type="bibr" target="#b41">[36]</ref>. Further details and descriptions of such distributed processing frameworks will be given in Sect. 4.1.</p><p>With the aim of choosing the most relevant and pertinent subset of features, a variety of feature reduction techniques were proposed within the Apache Spark framework to deal with big data in a distributed way. Among these are several feature extraction methods such as nn-gram, principal component analysis, discrete cosine transform, tokenizer, Polynomi-alExpansion, ElementwiseProduct, etc., and very few feature selection techniques which are the VectorSlicer, the RFormula and the ChiSqSelector. To further expand this restricted research, i.e., the development of parallel feature selection methods, lately, some other feature selection techniques were proposed in the literature which are based on evolutionary algorithms <ref type="bibr" target="#b35">[30]</ref>. Specifically, the evolutionary algorithms were implemented based on the MapReduce paradigm to obtain subsets of features from big data sets. <ref type="foot" target="#foot_1">2</ref> These include a generic implementation of greedy information theoretic feature selection methods <ref type="foot" target="#foot_2">3</ref> which are based on the common theoretic framework presented in <ref type="bibr" target="#b34">[29]</ref>, and an improved implementation of the classical minimum Redundancy and Maximum Relevance feature selection method <ref type="bibr" target="#b34">[29]</ref>. This implementation includes several optimizations such as cache marginal probabilities, accumulation of redundancy (greedy approach) and a data-access by columns. <ref type="foot" target="#foot_3">4</ref> Nevertheless, most of these techniques suffer from some shortcomings. For instance, they usually require the user or expert to deal with the algorithms' parameterisation, noise levels specification, where some other techniques simply order the attributes set and let the user choose his/her own subset. There are some other feature selection techniques that require the user to indicate how many attributes should be selected, or they must give a threshold that determines when the algorithm should end, which are all counted as significant drawbacks. All of these require users to make a decision based on their own (possibly subjective) perception. To overcome the shortcomings of the state-of-the-art techniques, it seemed to be crucial to look for a filter approach that does not require any external or supplementary information to function properly. Rough set theory (RST) can be used as such a technique <ref type="bibr" target="#b44">[39]</ref>.</p><p>The use of rough set theory in data mining and knowledge discovery, specifically for feature selection, has proved to be very successful in many application domains such as in classification <ref type="bibr" target="#b27">[22]</ref>, clustering <ref type="bibr" target="#b28">[23]</ref> and in supply chain <ref type="bibr" target="#b11">[5]</ref>. This success is explained by the several aspects of the theory in dealing with data. For example, the theory is able to analyze the facts hidden in data, does not need any supplementary information about the given data such as thresholds or expert knowledge on a particular domain and is also capable to find a minimal knowledge representation <ref type="bibr" target="#b17">[11]</ref>. This is achieved by making use of the granularity structure of the provided data only.</p><p>Although algorithms based on rough sets have been widely used as efficient filter feature selectors, most of the classical rough set algorithms are sequential ones, computationally expensive and can only deal with non-large data sets. The prohibitive complexity of these algorithms comes from the search for an optimal attribute subset through the computation of an exponential number of candidate subsets. Although it is an exhaustive method, this is quite impractical for most data sets specifically for big data as it becomes clearly unmanageable to build the set of all possible combinations of features.</p><p>In order to overcome these weaknesses, a set of parallel and distributed rough set methods has been proposed in the literature to ensure feature selection but in different contexts. For example, some of these distributed methods adopt some evolutionary algorithms, such as the work proposed in <ref type="bibr" target="#b18">[12]</ref>, where authors defined a hierarchical MapReduce implementation of a parallel genetic algorithm for determining the minimum rough set reduct, i.e., the set of the selected features. Within another context, the context of limited labeled big data, in <ref type="bibr" target="#b37">[32]</ref>, authors introduced a theoretic framework called local rough set and developed a series of corresponding concept approximation and attribute reduction algorithms with linear time complexity, which can efficiently and effectively work in limited labeled big data. In the context of distributed decision information systems, i.e., several separate data sets dealing with different contents/topics but concerning the same data items, in <ref type="bibr" target="#b24">[19]</ref>, authors proposed a distributed definition of rough sets to deal with the reduction of these information systems.</p><p>In this paper, and in contrast to the state-of-the-art methods, we mainly focus on the formalization of rough set theory in a distributed manner by using its granular concepts only, and without making use of any heuristics, e.g., evolutionary algorithms. We also focus on a single information system, i.e., a single big data set, which covers a single content/topic and which is characterized by a full and complete labeled data. Within this focus, and in the literature, a first attempt presenting a parallel rough set model was given in <ref type="bibr" target="#b14">[8]</ref>. The main idea in <ref type="bibr" target="#b14">[8]</ref> is to split the given big data set into several partitions, each with a smaller number of features which are all then processed in a parallel way. This is to minimize the computational effort of the RST computations when dealing with a very large number of features particularly. However, it is important to mention that the scalability of <ref type="bibr" target="#b14">[8]</ref> was only validated in terms of sizeup and scaleup with a change in the standard metrics definitions (the standard definitions are given in Sect. 6.2). Actually, the used definition of these two metrics was based on the number of features per partition instead of the standard definition where the evaluation has to be based on the total number of features in the database used.</p><p>In this paper, we propose a redesign of rough set theory for feature selection by giving a better definition of the work presented in <ref type="bibr" target="#b14">[8]</ref>, specifically when it comes to the validation of the method (Sect. 6). Our work, which is an extension of <ref type="bibr" target="#b14">[8]</ref>, is based on a distributed partitioning procedure, within a Spark/MapReduce paradigm, that makes our proposed solution scalable and effective in dealing with big data. For the validation of our method, and in contrast to <ref type="bibr" target="#b14">[8]</ref>, we believe that using the overall number of attributes is a much more natural setup as it will give insights into the performance depending on the input data set rather than the partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rough sets for feature selection</head><p>Rough set theory (RST) <ref type="bibr" target="#b32">[27,</ref><ref type="bibr">28]</ref> is a formal approximation of the conventional set theory that supports approximations in decision making. This approach can extract knowledge from a problem domain in a concise way and retain the information content while reducing the involved amount of data <ref type="bibr" target="#b44">[39]</ref>. This section focuses mainly on highlighting the fundamentals of RST for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>In rough set theory, the training data set is called an information table or an information system. It is represented by a table where rows represent objects or instances and columns represent attributes or features. The information table can be defined as a tuple S = (U , A), where U = {u 1 , u 2 , . . . , u N } is a non-empty finite set of N instances (or objects), called universe, and A is a non-empty set of (n + k) attributes. The feature set A = C ∪ D can be partitioned into two subsets, namely the conditional feature set C = {a 1 , a 2 , . . . , a n } consisting of n conditional attributes or predictors and the decision attribute D = {d 1 , d 2 , . . . , d k } consisting of k decision attributes or output variables. Each feature a ∈ A is described with a set of possible values V a named the domain of a.</p><p>For each non-empty subset of attributes P ⊂ C, a binary relation called P-indiscernibility relation, which is the central concept of rough set theory, is defined as follows:</p><formula xml:id="formula_0">IND(P) = {(u 1 , u 2 ) ∈ U × U : ∀a ∈ P, a(u 1 ) = a(u 2 )} . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where a(u i ) refers to the value of attribute a for the instance u i . This means if (u The rough set approximates a concept or a target set of objects X ⊆ U using the equivalence classes induced using P as follows:</p><formula xml:id="formula_2">P(X ) = {u: [u] P ⊆ X }.</formula><p>(2)</p><formula xml:id="formula_3">P(X ) = {u: [u] P ∩ X = ∅}.<label>(3)</label></formula><p>where P(X ) and P(X ) denote the P-lower (certainly classified as members of X ) and Pupper (possibly classified as members of X ) approximations of X , respectively. The notation ∩ denotes the intersection operation.</p><p>The concept that defines the set of instances that are not certainly, but can possibly be classified in a specific way is named the boundary region and is defined as the difference between the two approximations. X is a crisp set if the boundary region is an empty set, i.e., accurate approximation, P(X ) = P(X ); otherwise, it is a rough set.</p><p>To compare subsets of attributes, a dependency measure is defined. For instance, the dependency measure of an attribute subset Q on another attribute subset P is given as:</p><formula xml:id="formula_4">γ P (Q) = |POS P (Q)| |U | . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where 0 ≤ γ P (Q) ≤ 1, ∪ denotes the union operation, || denotes the set cardinality, and POS P (Q) is defined as:</p><formula xml:id="formula_6">POS P (Q) = X ∈[u] Q P(X ). (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>P O S P (Q) is the positive region of Q with respect to P and is the set of all elements of U that can be uniquely classified to blocks of the partition [u] Q , by means of P. The closer γ P (Q) is to 1, the more Q depends on P.</p><p>Based on these basics, RST defines two important concepts for feature selection which are the Core and the Reduct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reduction process</head><p>The theory of rough sets aims at finding the smallest subset of the conditional attribute set in a way that the resulting reduced database remains consistent with respect to the decision attribute. A database is considered to be consistent in case where for every set of objects, having identical feature values, the corresponding decision features are the same. To achieve this, the theory defines the Reduct concept and the Core concept.</p><p>Formally, in an information table, the unnecessary attributes can be categorized into either irrelevant features or into redundant features. The point is to define an heuristic that defines a measure to evaluate the necessity of a feature. Nevertheless, it is not easy to define an heuristic based on these qualitative definitions of irrelevance and redundancy. Therefore, authors in <ref type="bibr" target="#b25">[20]</ref> defined strong relevance and weak relevance of an attribute based on the probability of the target concept occurrence given this attribute. The set of the strong relevant attributes presents the indispensable features in the sense that they cannot be removed from the information table without causing a loss of the prediction accuracy. On the other hand, the set of the weak relevant features can in some cases contribute to the prediction accuracy. Based on these definitions, both of the strong and the weak relevance concepts can provide good basics upon which the description of the importance of each feature can be defined. In the rough set terminology, the set of strong relevant attributes can be mapped to the Core concept while the Reduct concept defines a mixture of all strong relevant attributes and some weak relevant attributes.</p><p>To define these key concept, RST sets the following formalizations: A subset R ⊆ C is said to be a reduct of C in the case where</p><formula xml:id="formula_8">γ R (D) = γ C (D) (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>and there is no</p><formula xml:id="formula_10">R ⊂ R such that γ R (D) = γ R (D).</formula><p>Based on this formula, the Reduct can be defined as the minimal set of selected features that preserve the same dependency degree as the whole set of features.</p><p>In practice, from the given information table, it is possible that the theory generates a set of reducts: RED F C (D). In this situation, any reduct in RED F C (D) can be selected to describe the original information table.</p><p>The theory also defines the Core concept which is the set of features that are enclosed in all reducts. The Core concept is defined as</p><formula xml:id="formula_11">CORE C (D) = RED F C (D). (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>More precisely, the Core is defined as the set of features that cannot be omitted from the information table without inducing a collapse of the equivalence class structure. Thus, the Core is the most important subset of attributes, since none of its elements can be removed without affecting the classification power of attributes. This means that all the features which are in the Core are indispensable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parallel computing frameworks and the MapReduce programming model</head><p>In this section, we highlight the main solutions for big data processing. We, also, give a description of the MapReduce paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parallel computing frameworks</head><p>With the dramatic increase of the amount of data, it has become crucial to implement a new set of technologies and tools that permit improved decision making and insight discovery. In this context, different techniques <ref type="bibr" target="#b38">[33]</ref> have been developed to handle high dimensional data sets where most of these proposed tools are based on distributed processing, e.g., the Message Passing Interface (MPI) programming paradigm <ref type="bibr" target="#b42">[37]</ref>.</p><p>The encountered challenges in this concern are essentially linked to the access to the given big data, to the transparency of the development process of the software with respect to its prerequisites, as well as to the available programming paradigms <ref type="bibr" target="#b20">[14]</ref>. For example, standard techniques require that all the given data should be loaded into the main machine's memory. This obviously presents a technical issue in big data since the data, which is given as input, is usually stored in different locations causing an intensive communication in the network as well as some supplementary input and output costs. It is true that it is possible to afford this, but it is also important to mention that it will be crucial to afford an intensively large main memory to be able to retain all the pre-loaded given data for computing and processing purposes.</p><p>To overcome these serious limitations, a new set of highly efficient and fault-tolerant parallel frameworks has been developed and set in the market. These distributed frameworks can be categorized with respect to the nature or type of the data they are able to process. Actually, there are some frameworks that can only process batch data. Within this schema, the parallel processing system functions over a high dimensional and static data set. At a later level of the distributed processing, the system returns the output result(s) when all the process of computations is successfully achieved. Among the well-known open-source distributed processing frameworks dedicated for batch processing, we mention Hadoop. <ref type="foot" target="#foot_4">5</ref> Hadoop is based on simple programming paradigms that allow a highly scalable and reliable parallel processing of high-dimensional data sets. The framework offers a cost-effective solution to store and process different types of data such as structured, semi-structured and unstructured data without any specific format specifications. Technically, Hadoop works on top of the Hadoop distributed file system (HDFS) which duplicates the input data files in various storage machines (nodes). In this manner, the framework facilitates a fast transfer rate of the data among nodes set in the cluster and allows the system to operate without any interruption if one or a number of nodes fail. MapReduce is the core of the Hadoop framework. This paradigm offers an intensive scalability over a large number of nodes within a Hadoop cluster. The programming details of MapReduce as well as its basic concepts will be given in Sect. 4.2.</p><p>On the other hand, there are some other distributed frameworks that can only deal with streaming data. Within these frameworks' design, the distributed calculations are performed over data (to each individual data item) at the time it enters the parallel framework. Apache Storm <ref type="foot" target="#foot_5">6</ref> and Apache Samza<ref type="foot" target="#foot_6">7</ref> are among the most popular stream processing frameworks.</p><p>A third category of distributed frameworks can be highlighted which is considered as hybrid systems. This is because these frameworks are capable of processing not only batch data but also stream data. In these frameworks' designs, similar or some linked elements can be used for both types of data. This makes the diverse processing requirements of the hybrid systems much easier and simpler. Among the well-known streaming processing parallel frameworks, we mention Apache Spark<ref type="foot" target="#foot_7">8</ref> and Apache Flink. <ref type="foot" target="#foot_8">9</ref>In this conducted research, we focus on Apache Spark. The distributed open source framework was initially developed in the UC Berkeley AMPLab for big data processing. Apache Spark is characterized by its capability of improving the system's effectiveness-which is achieved via the use of intensive memory-,its efficiency, and its high transparency for users. These characteristics allow to perform parallel processing of diverse application domains in a simple and easy way. More precisely and in comparison to Hadoop, in Hadoop MapReduce multiple jobs would be adjusted together to build a data pipeline. In this process, and in every level of that built pipeline, MapReduce will have to read the data from the disk and then write it back to the disk again. This process was obviously ineffective as it had to read all the data and write it from and back to the disk at each level of the process. To deal with this issue, Apache Spark comes into play. Based on the same MapReduce paradigm, the Spark framework could offer an immediate 10 times increase in the system's performance. This is explained by the non-necessity to store the given data back to the disk at every stage of the process as all activities remain in the memory <ref type="bibr" target="#b41">[36]</ref>. Spark affords a much faster data process in contrast to transferring it through needless Hadoop MapReduce mechanisms. Adding to this specificity, the key concept that Spark offers is a resilient distributed data set (RDD), which is a set of elements that are distributed across the nodes of the used cluster that can be operated on in a parallel way. Indeed, Spark has a number of high-level libraries for stream processing, machine learning and graph processing, e.g., <ref type="bibr">MLlib [18]</ref>. The choice of this specific framework to design our proposed algorithm based on rough sets for big data feature selection is essentially based on several reasons which are as follows: (1) to offer a general solution based on a hybrid parallel framework, (2) Apache Spark provides high-speed benefits with a trade-off in the usage of high memory, (3) Spark is one of the well-known and certified distributed frameworks and also a mature hybrid system specifically when comparing it to some other frameworks in the market. These are considered as more niche in terms of their usage but more importantly they are still in their initial periods of adoption. <ref type="foot" target="#foot_9">10</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The MapReduce paradigm</head><p>MapReduce <ref type="bibr" target="#b16">[10]</ref> is one of the most popular processing techniques and program models for distributed computing to deal with big data. It was proposed by Google in 2004 and designed to easily scale data processing over multiple computing nodes. The MapReduce paradigm is composed of two main tasks/phases, namely the map phase and the reduce phase. At an abstract level, the map process takes as input a set of data and transforms it into a different set where each element is represented in the form of a tuple key/value pair, producing some intermediate results. Then, the reduce process collects the output from the map task as an input and combines these given key/value tuples into a smaller set of pairs to generate the final output. A representation of the MapReduce framework is given in Fig. <ref type="figure">1</ref>.</p><p>Technically, the MapReduce paradigm is based on a specific data structure which is the (key, value) pair. More precisely, during the map phase, on each split of the data the map function gets a unique (key, value) tuple as an input and generates a set of intermediate (key , value ) pairs as output. This is represented as follows: map(key, value) → {(key , value ), . . .}. <ref type="bibr" target="#b14">(8)</ref> After that, the MapReduce paradigm assembles all the intermediate (key , value ) pairs by key via the shuffling phase. Finally, the reduce function takes the aggregated (key , value ) pairs and generates a new (key , value ) pair as output. This is defined as: reduce(key , {value , . . .}) → (key , value ).</p><p>(</p><formula xml:id="formula_13">)<label>9</label></formula><p>Fig. <ref type="figure">1</ref> The process of the MapReduce framework</p><p>As discussed, a variety of open source parallel computing frameworks are proposed in the market, and in this section, we have highlighted the well-known ones. However, it is important to mention that choosing a particular distributed framework is always dependent to the type or kind of the given data that the system will process. The choice also depends on how time bound the specifications of the users are, and on the types of output results that users are looking for. In this paper, we mainly focused on the use of Apache Spark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The rough set distributed algorithm for big data feature selection</head><p>In this section, we will introduce our developed parallel rough set-based algorithm, that we name 'Sp-RST,' for big data pre-processing and specifically for feature selection. Sp-RST has a distributed architecture based on Apache Spark for a distributed and in-memory computation task. First, we will highlight the main motivation for developing the distributed Sp-RST algorithm by identifying the computational inefficiencies of the classical rough set theory which limit its application to small data sets only. Secondly, we will elucidate our Sp-RST solution as an efficient approach capable of performing big data feature selection without sacrificing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Motivation and problem statement</head><p>Rough set theory for feature selection is an exhaustive search as the theory needs to compute every possible combination of attributes. The number of possible attribute subsets with m attributes from a set of <ref type="bibr" target="#b23">[17]</ref>. Thus, the total number of feature subsets to generate is N i=1 N i = 2 N -1. For example, for N = 30 we have roughly 1 billion combinations. This constraint prevents us to use high-dimensional data sets as the number of feature subsets is growing exponentially in the total number of features N . Moreover, hardware constraints, specifically memory consumption, do not allow us to 123 store a high number of entries. This is because the system has to store the entire training data set in memory, together with all the supplementary data computations as well as the generated results. All of this data can be so big that its size can easily exceed the available RAM memory. These are the main motivations for our proposed Sp-RST solution, which makes use of parallelization.</p><formula xml:id="formula_14">N total attributes is N m = N ! m!(N -m)!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The proposed solution</head><p>To overcome the standard RST inadequacy to perform feature selection in the context of big data, we propose our distributed Sp-RST solution. Technically, to handle a large set of data it is crucial to store all the given data set in a parallel framework and perform computations in a distributed way. Based on these requirements, we first partition the overall rough set feature selection process into a set of smaller and basic tasks that each can be processed independently. After that, we combine the generated intermediate outputs to finally build the sought result, i.e., the reduct set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">General model formalization</head><p>For feature selection, our learning problem aims to select a set of highly discriminating attributes from the initial large-scale input data set. The input base refers to the data stored in the distributed file system (DFS). To perform distributed tasks on the given DFS, a resilient distributed data set (RDD) is built. The latter can be formalized as a given information table that we name T RDD . T RDD is defined via a universe U = {x 1 , x 2 , . . . , x N }, which refers to the set of data instances (items), a large conditional feature set C = {c 1 , c 2 , . . . , c V } that includes all the features of the T RDD information table and finally via a decision feature D of the given learning problem. D refers to the label (also called class) of each T RDD data item and is defined as follows: D = {d 1 , d 2 , . . . , d W }. C presents the conditional attribute pool from where the most significant attributes will be selected.</p><p>As explained in Sect. 5.1, the classical RST cannot deal with a very large number of features, which is defined as C in the T RDD information table. Thus, to ensure the scalability of our proposed algorithm when dealing with a large number of attributes, Sp-RST first partitions the input T RDD information table (the big data set) into a set of m data blocks based on splits from the conditional feature set C, i.e., m smaller data sets with a fewer number of features instead of using a single data block (T RDD ) with an unmanageable C number of features that we note as T RDD (C). The key idea is to generate m smaller data sets that we name T RDD (i) , where i ∈ {1, . . . , m}, from the big T RDD data set, where each T RDD (i) is defined via a manageable number of features r , where r</p><formula xml:id="formula_15">≪ C = {c 1 , c 2 , . . . , c V } and r ∈ {1, . . . , V }.</formula><p>The definition of the parameter r will be further explained in what follows. We note the resulting data block as T RDD (i) (C r ). This leads to the following formalization:</p><formula xml:id="formula_16">T RDD = m i=1 T RDD (i) (C r )</formula><p>, where r ∈ {1, . . . , V }. As mentioned above, r defines the number of attributes that will be considered to build every T RDD (i) data block. Based on this, every T RDD (i) is built using r random attributes which are selected from C. Each T RDD (i) is constructed based on r distinct features as there are no common attributes between all the built T RDD (i) . This leads to the following formalization: ∀T RDD (i) : {c r } = m i=1 T RDD (i) . Figure <ref type="figure" target="#fig_2">2</ref> presents this data partitioning phase.</p><p>With respect to the parallel implementation design, the distributed Sp-RST algorithm will be applied to every T RDD (i) (C r ) while gathering all the intermediate results from the distinct m created partitions; rather than being applied to the complete T RDD that encloses the whole set Fig. <ref type="figure" target="#fig_2">2</ref> The process of data partitioning C of conditional features. Based on this design, we can ensure that the algorithm can perform its feature selection task on a computable number of attributes and therefore overcome the standard rough set computational inefficiencies. The pseudocode of our proposed distributed Sp-RST solution is highlighted in Algorithm 1.</p><p>To further guarantee the Sp-RST feature selection performance while avoiding any critical information loss, to evolve the algorithm and to refine it, Sp-RST runs over N iterations on the T RDD m data blocks, i.e., N iterations on all the m built T RDD (i) (C r ). Through all these N iterations, Sp-RST will first randomly build the m distinct T RDD (i) (C r ) as explained above. Once this is achieved and for each partition, the algorithm's distributed tasks defined in Algorithm 1 (lines 5-10) will be performed. As noticed, line 1 in Algorithm 1 that defines the initial Sp-RST parallel job is performed outside the loop iteration. This process calculates the indiscernibility relation IND(D) of the decision class D. The main reason for this implementation is that this process is totally separated from the m created partitions. This is because the output is tied to the label of the data instances and not on the attribute set.</p><p>Out from the iteration loop (line 12), the outcome of each created partition can be either only one reduct RED i (D) (C r ) or a set (a family) of reducts RED F i (D) (C r ). As previously highlighted in Sect. 3, any reduct among the RED F i (D) (C r ) reducts can be selected to describe the T RDD (i) (C r ) information table. Therefore, in case where Sp-RST generates a single reduct for a specific T RDD (i) (C r ) partition, the final output of this attribute selection phase is the set of features defined in RED i (D) (C r ). These attributes represent the most informative features among the C r features and generate a new reduced T RDD (i) defined as: T RDD (i) (R E D). The latter reduced base guarantees nearly the same data quality as its corresponding T RDD (i) (C r ) which is based on the full attribute set C r . In the other case where Sp-RST generates multiple reducts, the algorithm performs a random selection of a single reduct among the generated </p><formula xml:id="formula_17">Reduct m = m i=1 RED i (D) (C r ) 14:</formula><p>end for 15: end for 16:</p><formula xml:id="formula_18">Reduct = N n=1 Reduct m 17: return (Reduct) family of reducts RED F i (D) (C r ) to describe the corresponding T RDD (i) (C r ).</formula><p>This random selection is supported by the RST fundamentals and is explained by the same level of importance of all the reducts defined in RED F i (D) (C r ). More precisely, any reduct included in the family of reducts RED F i (D) (C r ) can be selected to replace the T RDD (i) (C r ) attributes. At this level, the output of every i data block is RED i (D) (C r ) which refers to the selected set of features. Nevertheless, since every T RDD (i) is described using r distinct attributes and with respect to T RDD = m i=1 T RDD (i) (C r ), a union operator on the generated selected attributes is needed to represent the original T RDD . This is defined as Reduct m = m i=1 RED i (D) (C r ) (Algorithm 1, lines <ref type="bibr" target="#b18">[12]</ref><ref type="bibr" target="#b19">[13]</ref><ref type="bibr" target="#b20">[14]</ref>. As previously highlighted, Sp-RST will perform its distributed tasks over the N iterations generating N Reduct m . Therefore, finally, an intersection operator applied on all the obtained Reduct m is required. This is defined as Reduct = N n=1 Reduct m . Sp-RST could diminish the dimensionality of the original data set from T RDD (C) to T RDD (Reduct) by removing irrelevant and redundant features at each computation level. Sp-RST could also simplify the learned model, speed up the overall learning process, and increase the performance of an algorithm, e.g., a classification algorithm, as will be discussed in the experimental setup section (Sect. 6). Figure <ref type="figure" target="#fig_1">3</ref> illustrates the global functioning of Sp-RST. In what follows, we will elucidate the different Sp-RST elementary distributed tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Algorithmic details</head><p>As previously highlighted, the elementary Sp-RST distributed tasks will be executed on every T RDD (i) partition defined by its C r features (T  To achieve this task, Sp-RST processes a first map transformation operation taking the data in its format of (id i of x i , List of the features of x i , Class d i of x i ) and transforming it to a key, value pair: Class d i of x i , List of id i of x i . Based on this transformation, the decision class d i defines the key of the generated output and the data items identifiers id i of x i of the T RDD define the values. After that, the f old ByK ey() <ref type="foot" target="#foot_10">11</ref>  After that and within a specific partition i, where i ∈ {1, 2, . . . , m} and m is the number of partitions, the algorithm generates the AllComb (C r ) RDD which reflects all the possible combinations of the C r set of attributes. This is based on transforming the C r RDD to the AllComb (C r ) RDD using the f latmap() <ref type="foot" target="#foot_11">12</ref> transformation operation and by using the combinations() operation. This is shown in Algorithm 3. In its third distributed job, Sp-RST calculates the indiscernibility relation IND(AllComb (C r ) ) for every created combination, i.e., the indiscernibility relation of every element in the output of Algorithm 3, and that we name AllComb (C r ) i . In this task and as described in Algorithm 4, the algorithm aims at collecting all the identifiers id i of the data items x i that have identical values of the combination of attributes which are extracted from AllComb (C r ) . To do so, a first map operation is applied taking the data in its format of (id i of x i , List of the features of x i , Class d i of x i ) and transforming it to a key, value pair: (AllComb (C r ) i , List of the features of x i ), List of id i of x i . Based on this transformation, the combination of features and their vector of features define the key and the identifiers id i of the data items x i define the value. After that, the f old ByK ey() operation is applied to merge all values of each key in the transformed RDD output, i.e., all the identifiers id i of the data items x i that have the same combination of features with their corresponding vector of features (AllComb (C r ) i , List of the features of x i ). This is to represent the sought IND(AllComb (C r ) ). At its third step, Sp-RST prepares the set of features that will be selected in the coming steps.</p><formula xml:id="formula_19">RDD (i) (C r )),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Generate AllComb</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Calculate IND(AllComb (C r ) )</head><p>Inputs:</p><formula xml:id="formula_20">T RDD i , AllComb (C r ) Output: IND(AllComb (C r ) )</formula><p>1: Map the T RDD i based on its format (id i of x i , List of the features of x i , Class d i of x i ) and generate the new format as a key-value pair (AllComb (C r ) i , List of the features of x i ), List of id i of x i 2: Merge the values of each generated key using the f old ByK ey()</p><formula xml:id="formula_21">operation 3: Return AllComb (C r ) : IND(AllComb (C r ) )</formula><p>In a next stage, Sp-RST computes the dependency degrees γ (AllComb (C r ) ) of each attribute combination as described in Algorithm 5. For this task, the distributed job requires three input parameters which are the calculated indiscernibility relations IND(D), the IND(AllComb (C r ) ) and the set of all attribute combinations AllComb (C r ) .</p><p>For every element AllComb (C r ) i in AllComb (C r ) , and using the intersection() transformation, the job tests first if the intersection of every</p><formula xml:id="formula_22">IND(d i ) of IND(d) with each element IND(AllComb (C r )) i in IND(AllComb (C r )</formula><p>) holds all the elements in the latter parameter. This process refers to the calculation of the lower approximation as detailed in Sect. 3. We name the length of the resulting intersection as LengthIntersect. If the condition is satisfied then a score, which is equal to the length of the elements resulting from the generated intersection, i.e., LengthIntersect, is assigned, else a 0 value is given.</p><p>After that a reduce function is applied over the different IND(D) elements together with a sum() function applied on the calculated scores which are based on the elements having the same IND(d i ). This operation is followed by a second reduce function which is applied over the different IND(AllComb (C r ) ) elements together with a sum() function applied on the previous calculated results which are indeed based on the elements having the same</p><formula xml:id="formula_23">AllComb (C r ) i .</formula><p>The latter output refers to the dependency degrees: γ (AllComb (C r ) ). This distributed job generates two outputs namely the set of dependency degrees γ (AllComb (C r ) ) of the attribute combinations AllComb (C r ) as well as their associated sizes Size (AllComb (Cr ) ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 5 Generate DE P(AllComb (C r ) )</head><formula xml:id="formula_24">Inputs: AllComb (C r ) , IND(D), IND(AllComb (C r ) ) Outputs: γ (AllComb (C r ) ), Size (AllComb (Cr ) ) 1: for each element AllComb (C r ) i in AllComb (C r ) do 2:</formula><p>for each element</p><formula xml:id="formula_25">IND(d i ) in IND(D) do 3: for each element IND(AllComb (C r ) ) i in IND(AllComb (C r ) ) do 4:</formula><p>Apply the intersection() transformation over</p><formula xml:id="formula_26">IND(d i ) and IND(AllComb (C r ) ) i 5:</formula><p>Get the length of the resulting intersection that we name as Length I ntersect 6:</p><formula xml:id="formula_27">if Length I ntersect = length of IND(AllComb (C r )) i then Score = Length I ntersect 7:</formula><p>else Score = 0 8:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End if 9:</head><p>End for 10:</p><p>Apply Once all the dependencies are calculated, in Algorithm 6, Sp-RST looks for the maximum value of the dependency among all the computed γ (AllComb (C r ) ) using the max() function operated on the given RDD input and which is referred to as RDD[AllComb (C r ) , Size (AllComb (Cr ) ) , γ (AllComb (C r ) )]. Specifically, the max() function will be applied on the third argument of the given RDD, i.e., γ (AllComb (C r ) ). Let us recall that based on the RST preliminaries (seen in Sect. 3), the maximum dependency refers to not only the dependency of the whole attribute set (C r ) describing the T RDD i (C r ) but also to the dependency of all the possible attribute combinations sat-123 isfying the following constraint: γ (AllComb (C r ) ) = γ (C r ). The maximum dependency Max Dependency reflects the baseline value for the feature selection task.</p><p>In a next step, Sp-RST performs a filtering process using the f ilter() function to only keep the set of all combinations which have the same dependency degrees, as the already selected dependency baseline value (Max Dependency), i.e., γ (AllComb (C r ) ) = Max Dependency. This is described in Algorithm 7. In fact, through these computations, the algorithm removes in each level the unnecessary attributes that may negatively influence the performance of any learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 7</head><p>Filter DE P max (AllComb (C r ) )</p><formula xml:id="formula_28">Inputs: RDD[AllComb (C r ) , Size (AllComb (Cr ) ) , γ (AllComb (C r ) )], Max Dependency Outputs: Filtered-RDD[AllComb (C r ) , Size (AllComb (Cr ) ) , γ (AllComb (C r ) )]</formula><p>1: Apply the filter() function on the input RDD in a way to select all combinations having a dependency that is equal to Max Dependency:</p><formula xml:id="formula_29">γ (AllComb (C r ) ) = Max Dependency 2: Return the filter RDD: Filtered-RDD[AllComb (C r ) , Size (AllComb (Cr ) ) , γ (AllComb (C r ) )]</formula><p>At a final stage, and using the results generated from the previous step, which is the input of Algorithm 8, Sp-RST applies first the min() operator to look for the minimum number of features among all the Size (AllComb (Cr ) ) ; specifically, the min() operator will be applied to the second argument of the given RDD. Once determined, a result that we name min N bF, the algorithm applies a f ilter() method to only keep the set of combinations having the same minimum number of features as min N bF. This is achieved by satisfying the full reduct constraints highlighted in Sect. 3: <ref type="bibr">)</ref>. Every combination that satisfies this constraint is evaluated as a possible minimum reduct set. The features defining the reduct set describe all concepts in the initial T RDD i (C r ) training data set.</p><formula xml:id="formula_30">γ (AllComb (C r ) ) = γ (C r ) while there is no AllComb (C r ) ⊂ AllComb (C r ) such that γ (AllComb (C r ) ) = γ (AllComb (C r )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 8</head><p>Filter N bF min (DE P max (AllComb (C r ) ))</p><formula xml:id="formula_31">Input: Filtered-RDD[AllComb (C r ) , Size (AllComb (Cr ) ) , γ (AllComb (C r ) )]</formula><p>Output: Reduct 1: Apply the min() function on the input filtered RDD second argument: Size (AllComb (Cr ) ) to get min N bF 2: Apply a f ilter function on the input RDD while satisfying the condition Size (AllComb (Cr ) ) = min N bF 3: Return Reduct = List of selected C r</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sp-RST: a working example</head><p>We apply Sp-RST to an example of an information table, T RDD (C), which is presented in Table <ref type="table" target="#tab_2">1</ref>. By assuming that the considered T RDD (C) is a big data set, the information table is defined via a universe U = {x 1 , x 2 , . . . , x 5 } which refers to the set of data instances (items), a large conditional feature set C = {Headache, Muscle-pain, Temperature} that includes all the features of the T RDD (C) information table and finally via a decision feature Flu of the given learning problem. Flu refers to the label (or class) of each T RDD (C) data item and is defined as follows: Flu = {yes, no}. C presents the conditional attribute pool from where the most significant attributes will be selected.   <ref type="table">2</ref> and<ref type="table">3:</ref> -From Split 1:</p><formula xml:id="formula_32">-yes, x 0 -yes, x 1 -no, x 2 -From Split 2: -no, x 3 -yes, x 4 -yes, x 5</formula><p>After that, and by applying Algorithm 2, line 2, we get the following output which refers to the indiscernibility relation of the class IND(Flu):</p><formula xml:id="formula_33">-yes, {x 0 , x 1 , x 4 , x 5 } -no, {x 2 , x 3 }</formula><p>In this example, we assume that we have two partitions m = 2. For the first partition, m = 1, a random number r = 2 is selected to build the first T RDD i=1 (C r ). For the second partition, m = 2, a random number r = 1 is selected to build the first T RDD i=2 (C r ). Based on these assumptions, the following partitions and splits based on Apache Spark are obtained (Tables <ref type="table" target="#tab_3">4,</ref><ref type="table">5</ref>, 6, 7).</p><p>Based on the first partition m = 1, and by applying Algorithm 3, which aims to generate all the AllComb (C r ) possible combinations of the C r set of attributes, the output from both Apache Spark splits is the following:</p><p>-Muscle-pain -Temperature -Muscle-pain, temperature</p><p>In its third distributed job, Sp-RST calculates the indiscernibility relation IND(AllComb (C r ) ) for every created combination, i.e., the indiscernibility relation of every element in the output of the previous step (Algorithm 3). By applying Algorithm 4 and based on both Apache Spark splits, the output is the following:</p><p>-From m = 1-Split 1:</p><formula xml:id="formula_34">-Muscle-pain, {x 0 }, {x 1 , x 2 } -Temperature, {x 0 }, {x 1 , x 2 } -Muscle-pain, Temperature, {x 0 }, {x 1 , x 2 } -From m = 1-Split 2: -Muscle-pain, {x 3 , x 4 , x 5 } -Temperature, {x 3 }, {x 4 }, {x 5 } -Muscle-pain, Temperature, {x 3 }, {x 4 }, {x 5 }</formula><p>In a next stage, and by using the previous output as well as IND(Flu), Sp-RST computes the dependency degrees γ (AllComb (C r ) ) of each attribute combination as described in Algorithm 5. This distributed job generates two outputs namely the set of dependency degrees γ (AllComb (C r ) ) of the attribute combinations AllComb (C r ) as well as their associated sizes Size (AllComb (Cr ) ) . The output from both splits for m = 1 is the following:</p><p>-Muscle-pain, 1, 1 -Temperature, 4, 1 -Muscle-pain, temperature, 4, 2 Once all the dependencies are calculated, in Algorithm 6, Sp-RST looks for the maximum value of the dependency among all the computed γ (AllComb (C r ) ). The maximum dependency reflects the baseline value for the feature selection task. The output is the following:</p><p>-4</p><p>In a next step, Sp-RST performs a filtering process to only keep the set of all combinations, which have the same dependency degrees, as the already selected dependency baseline value (Max Dependency = 4), i.e., γ (AllComb (C r ) ) = Max Dependency = 4. By applying Algorithm 7, the following output is obtained:</p><p>-Temperature, 4, 1 -Headache, Temperature, 4, 2</p><p>In fact, through these computations, the algorithm removes in each level the unnecessary attributes that may negatively influence the performance of any learning algorithm.</p><p>At a final stage, and using the results generated from the previous step and by applying Algorithm 8, Sp-RST looks for the minimum number of features among all the Size (AllComb (Cr ) ) . Once determined (min N bF = 1), the algorithm only keeps the set of combinations having the same minimum number of features as min N bF. The filtered selected features define the reduct set and describe all concepts in the initial T RDD i (C r ) training data set. The output of Algorithm 8 and which presents the Reduct for m = 1 is the following:</p><p>-Temperature Based on these calculations, for m = 1, Sp-RST reduced the T DD i=1 (C r =2 ) to Reduct m=1 = {T emperature}.</p><p>The same calculations will be applied to m = 2, and the output is Reduct m=2 = {H eadache} (as the data is composed of a single feature).</p><p>At this stage, different reducts are generated from the different m partitions. With respect to Algorithm 1, lines 12-14, a union of the obtained results is required to represent the initial big information table T RDD (C), i.e., Table <ref type="table" target="#tab_2">1</ref>. The final output is Reduct = {H eadache, T emperature}.</p><p>In this example, we presented a single iteration of Sp-RST, i.e., N = 1. Therefore, line 16 on Algorithm 1 will not be covered in this example.</p><p>Sp-RST could reduce the big information To validate the effectiveness of Sp-RST, we require a data set with a large number of attributes that is also defined by a large number of data instances. The Amazon Commerce reviews data set from the UCI machine learning repository <ref type="bibr" target="#b10">[4]</ref> fulfills this requirement. The Amazon data set was initially build from several customer reviews on the Amazon commerce Web site. The base was constructed based on the identification of the most active users, aiming at performing authorship identification. The database enclosed a total of 1500 data instances which are described using 10,000 features (linguistic style such punctuation, length of words, sentences, etc.) and 50 distinct classes (referring to authors). The Amazon data items are identically distributed across the data set classes, i.e., for each class there are 30 items.</p><p>We demonstrate the scalability of our approach by considering subsets of this data set in terms of attributes. To be more precise, we have created five additional data sets by randomly choosing 1000, 2000, 4000, 6000, and 8000 out of the original 10,000 attributes. We use these sets to evaluate our proposed method as discussed in Sect. 6.2 and refer to them as Amazon1000, Amazon2000, …, Amazon10,000 in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation metrics</head><p>To evaluate the scalability of the parallel Sp-RST, we consider the standard metrics which are the speedup, the scaleup, and the sizeup from literature <ref type="bibr" target="#b47">[42]</ref>. These are defined as follows:</p><p>-For the speedup, we keep the size of the data set constant (where size is measured by the number of features, i.e., we use the original data set with 10,000 features) and increase the number of nodes. For a system with m nodes, the speedup is defined as: Speedup(m) = runtime on one node runtime on m nodes An ideal parallel algorithm has linear speedup: The algorithm using m nodes solves the problem in the order of m times faster than the same algorithm using a single node. However, this is difficult to achieve in practice due to startup and communication cost as well as interference and skew <ref type="bibr" target="#b47">[42]</ref> which may lead to a sub-linear speedup. -The sizeup keeps the number of nodes constant and measures how much the runtime increases as the data set is increased by a factor of m:</p><p>Sizeup(m) = runtime for data set of size m • s runtime for baseline data set of size s To measure the sizeup, we use the smaller databases described in Sect. 6.1. We use 1000 features as a baseline and consider 2000, 4000, 6000, 8000, and 10,000 features, respectively. A parallel algorithm with a linear sizeup has a very good sizeup performance: Considering a problem that is m times larger than a baseline problem, the algorithm requires in the order of m times more runtime for the larger problem.</p><p>-The scaleup evaluates the ability to increase the number of nodes and the size of the data set simultaneously:</p><p>Scaleup(m) = runtime for data set of size s on 1 node runtime for data set of size s • m on m nodes Again, we use the sub-data set with 1000 features as a baseline. Here, a scaleup of 1 implies 'linear' scaleup, which similarly to linear speedup is difficult to achieve.</p><p>As previously highlighted in Sect. 2, a preliminary version of our proposed solution was introduced in <ref type="bibr" target="#b14">[8]</ref> as an attempt to deal with feature selection in the big data context. Yet, let us recall that in <ref type="bibr" target="#b14">[8]</ref>, both of the sizeup and scaleup were measured based on the number of features per partition, and hence, they are based on a modified definition of the standard metrics which are detailed above. However, we think that using the overall number of attributes is a much more natural setup as it will give insights into the performance depending on the input data set rather than the partitions, i.e., the proper definitions of the metrics are adopted in this paper.</p><p>To demonstrate that our distributed Sp-RST solution performs its feature selection task well without sacrificing performance, we perform model evaluation using a Naive Bayes and a random forest classifier. For the evaluation, we use the standard measures which are the precision, the recall, the accuracy and F1 score as well as the runtime (measured in seconds), to compare the quality of the feature set selected by Sp-RST with other feature selection methods as described in Sect. 6.3. The metrics definitions are as follows (where TP: True positive, TN: True negative, FP: False positive, and FN: False negative):</p><p>-Precision: measures the ratio of correctly predicted positive observations to the total predicted positive observations, and is defined as: Precision = TP TP + FP -Recall: measures the ratio of correctly predicted positive observations to all observations in the actual class-yes, and is defined as: Recall = TP TP + FN -Accuracy: measures the ratio of correctly predicted observation to the total observations, and is defined as follows:</p><p>Accuracy = TP + TN TP + FN + TN + FP -F1 score: is the weighted average of Precision and Recall. F1 score is defined as follows:</p><formula xml:id="formula_35">F1 score = 2 *</formula><p>Recall * Precision Recall + Precision We remark that Sp-RST is a stochastic algorithm. For Sp-RST, the process of randomization is applied twice. Once when partitioning the data set into m data blocks and second during the selection of one reduct among the generated set (family) of reducts. To diminish 123 the effect of the first randomization process, we perform several iterations of the main part of the algorithm (Algorithm 1) and we only keep the set of attributes that are selected in all iterations. The second randomization process is already justified and supported by the fundamentals of the theory of rough sets as presented in Sect. 3. We, therefore, conduct a deep analysis of the stability of the attribute sets which were selected by performing several runs of Algorithm 1 and then report averages and standard deviations whenever appropriate.</p><p>To investigate the significance of any noticed variation in the classification performance when random forest and Naive Bayes are applied to the initial data set and to the reduced set generated by Sp-RST and other feature selection techniques, we perform Wilcoxon signed rank tests with Bonferroni correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental environment</head><p>In the following, we conduct a detailed study of various parameters of Sp-RST with the aim to analyze how these can affect the system's runtime as well as the stability of the attribute selection task. We then apply a Naive Bayes and a random forest classifier on the original data set and the reduced data sets produced by Sp-RST and other feature selection techniques. We use the scikit-learn random forest implementation 13 with the following parameters: n_estimators = 1000, n_jobs = -1, and oob_score = True. A Stratified 10-Folds cross-validator 14 is used for all our conducted experiments. Moreover, we use the Naive Bayes implementation from Weka 3.8.2. 15  The Sp-RST algorithm is implemented in Scala 2.11 within the Spark 2.1.1 framework. Our experiments for Sp-RST are performed on Grid5000, 16 a large-scale testbed for experimentdriven research. Within this testbed, we used dual 8 core Intel Xeon E5-2630v3 CPUs and 128 GB memory. Since the study does not require a scalable version of the two classifiers, these experiments are run on a standard laptop configuration with Intel(R) Core(TM) i7-7500U CPU, 16 GB RAM, 64-bit, Windows-10.</p><p>Preliminary results revealed that a maximum of 10 features per partition is the limit that can be processed by Sp-RST. We therefore perform experiments using 4, 5, 8, and 10 features per partition in Algorithm 1. We run all settings on 1, 2, 4, 8, 16, and 32 nodes on Grid5000. When considering scalability, we set the number of iterations in Algorithm 1 to 10 (based on preliminary experiments). However, we perform an additional analysis of the feature selection process across different iterations in Sect. 7.1.</p><p>To ensure a fair comparison, we restrict our comparison with other feature selection methods to filter techniques. These methods include both, attribute and subset evaluation methods. For subset evaluation, we use a 'Best First' greedy search method. For attribute evaluation, we need to either provide a threshold or a number of features to be selected. We set the number of features to be selected to a value comparable with Sp-RST, i.e., the average number of features selected for each parameter setting considered and additionally use 0 as a threshold. We determine the sets of features selected by these methods and then perform model evaluation with a Naive Bayes and a random forest classifier as discussed previously. Subset selection: 13 http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html. 14 http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html. 15 http://weka.sourceforge.net/doc.dev/weka/classifiers/bayes/NaiveBayes.html. 16 http://www.grid5000.fr.</p><p>-CfsSubsetEval considers the individual predictive ability of each feature along with the degree of redundancy between them -ConsistencySubsetEval considers the level of consistency in the class values when the training instances are projected onto the subset of attributes Attribute selection:</p><p>-Sum squares ratio, which measures the ratio of between-groups to within-groups sum of squares. -Chi squared, which computes the value of the chi-squared statistic with respect to the class. -Gain ratio, which measures the gain ratio with respect to the class.</p><p>-Information gain, which measures the information gain with respect to the class.</p><p>-Correlation, which measures the correlation (Pearson's) between it and the class.</p><p>-CV, which first creates a ranking of attributes based on the Variation value, then divides into two groups, last using Verification method to select the best group. -ReliefF, which repeatedly samples an instance and considers the value of the given attribute for the nearest instance of the same and different class. -Significance, which computes the probabilistic significance as a two-way function (attribute-classes and classes-attribute association). -Symmetrical uncertainty, which evaluates the symmetrical uncertainty with respect to the class.</p><p>For sum squares ratio, we have used the version implemented in Smile; <ref type="foot" target="#foot_12">17</ref> while for the other three techniques, we have used the implementation provided in Weka 3.8.2.<ref type="foot" target="#foot_13">18</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion of results</head><p>We first examine the feature selection process over several iterations, which is a crucial parameter of Sp-RST (Sect. 7.1). Afterwards, we analyze the stability (Sect. 7.2) and scalability (Sect. 7.3) of our proposed feature selection approach. Finally, we compare its performance with other state-of-the-art feature selection techniques (Sect. 7.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Number of iterations in Sp-RST</head><p>We first have a closer look at one of the parameters of Sp-RST, i.e., the number of iterations (N in Algorithm 1). We perform four independent runs (or repetitions) of Sp-RST with 1, 2, …, 20 iterations, and for each iteration record the elected features as well as the elapsed time. We plot the average and standard deviation of the number of remaining features after each iteration over these four runs and for different parameter settings in Fig. <ref type="figure">4</ref>.</p><p>We also perform runs of Sp-RST with 1, 2, …, 20 iterations on different numbers of nodes, i.e., 1, 4, 8, and 16 nodes. The corresponding runtimes split by the number of the nodes are shown in Fig. <ref type="figure">5</ref>. Here, the average and standard deviation are taken across all runs with the same number of iterations.</p><p>From Fig. <ref type="figure">4</ref>, we observe that independently of the number of iterations, there is a clear ordering with respect to the number of selected features: The smaller the number of features   per partition, the fewer features are selected by Sp-RST. The very small standard deviation (&lt; 40) is hardly visible in the graphs and clearly demonstrates the stability of Sp-RST with respect to the number of features selected in an iteration. Recall that Sp-RST returns the intersection of the reducts from all iterations performed. Thus, the number of selected features strictly decreases with the number of iterations. As discussed before, the runtime for the rough set component of our methods grows exponentially with respect to the number of features per partition. Thus, the runtime behavior in Fig. <ref type="figure">5</ref> is not surprising and clearly demonstrates that the number of features per partition should not grow too large. We also see that the runtime per iteration is quite stable so that the overall runtime grows linearly with the number of iterations. Based on these experiments, we have decided to use a medium number of iterations for the remainder of our analysis, namely 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Stability of feature selection</head><p>To validate the stability of the feature selection of Sp-RST, we have a closer look at the concrete features selected. We perform two sets of experiments. First, we look at the features selected by Sp-RST with a single iteration (N = 1). Second, we consider our standard parameter setting of 10 iterations (N = 10) and perform several independent runs of Algorithm 1. We particularly look at two extreme cases: the number of features that are always selected and the number of features that are never selected over a given number of these runs or iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Iterations</head><p>We first consider features selected during a single iteration (N = 1). We run 10, 20, 40, and 80 independent runs of Sp-RST with 1 iteration and for each run record the set of selected features. For each feature, we count the number of times it has been selected and depict the results in Fig. <ref type="figure" target="#fig_5">6</ref>. Table <ref type="table" target="#tab_6">9</ref> additionally shows the mean, standard deviation (SD), min and max of the number of features selected in a single iteration (over 80 runs performed independently).</p><p>From Fig. <ref type="figure" target="#fig_5">6</ref> and Table <ref type="table" target="#tab_6">9</ref>, we see that the number of features selected in a single iteration is very stable. However, depending on the number of features per partition, only about 45% (4, 5 features), 55% (8 features), and 70% (10 features) of these features are selected in all 10 iterations and already for 20 iterations all features will have been selected at least once. The latter also holds for 40 and 80 iterations. This demonstrates that the set of 'core' features (defined is Sect. 3.2) that are reliably selected is much smaller than the set of features selected in a single iteration. This observation is the main motivation for using several iterations and only returning features that are always selected as the result of Sp-RST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Complete algorithm</head><p>To confirm that using the intersection of several iterations improves the stability of the feature selection, we now consider the complete algorithm with 10 iterations. We run 6 independent repetitions of Sp-RST with 10 iterations and plot the number of features selected 0, 1, …, 6 times in these 6 runs in Fig. <ref type="figure">7</ref>.  From Fig. <ref type="figure">7</ref>, it is obvious that most features are either always or never selected. This demonstrates that Sp-RST reliably selects the same features, or in other words is able to identify the most relevant (always selected) and least relevant (never selected) features. As before, we provide the number of features selected in a run of Sp-RST in Table <ref type="table" target="#tab_7">10</ref> and observe that the number of features selected is again very stable.</p><p>We remark that it is not a weakness of our proposed method that the actual features selected from one run to another differ. As discussed earlier in Sect. 3.2, there can be more than one reduct, a family of reducts, and selecting an arbitrary one among these is appropriate. Only core features appear in all the generated reducts set. The results presented in Fig. <ref type="figure">7</ref> support that our method is able to identify core features as well as features that are not in any of the resulting reducts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Scalability</head><p>We measure the scalability of Sp-RST based on its speedup (Fig. <ref type="figure">8</ref>), sizeup (Fig. <ref type="figure" target="#fig_16">10</ref>) and scaleup (Fig. <ref type="figure" target="#fig_2">12</ref>) as discussed in Sect. 6.2 and additionally plot the measured runtimes by the number of nodes and the different data sets (Figs. <ref type="bibr" target="#b15">9,</ref><ref type="bibr" target="#b17">11)</ref>. We see that the runtime increases considerably with the number of features per partition and decreases with the number of nodes used (Fig. <ref type="figure">9</ref>). However, in the latter case, increasing the number of nodes from 1 to 2 123  or 4 has a much larger effect than increasing it further (Fig. <ref type="figure">9</ref>). Moreover, it increases with the size of the database (Fig. <ref type="figure">11</ref>), where size is measured in terms of the number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Nodes</head><p>In terms of speedup (Fig. <ref type="figure">8</ref>), we see that the speedup for our smallest data set (Ama-zon1000) with 8 or 10 features per partition is approximately linear. However, in general and in particular for larger databases, the speedup is sub-linear. It is better for settings with more features per partition (with 8 and 10 features always being the best parameter settings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Nodes</head><p>Runtime (in seconds) (f) Amazon10000 This implies that having more partitions is not beneficial with respect to the parallel running time-even though the runtime for a single partition grows exponentially in the number of its features. This indicates that settings with 4 and 5 features per partition generate too small sub-databases that generate unnecessary large overhead due to high communication cost.</p><p>Figure <ref type="figure" target="#fig_16">10</ref> shows that Sp-RST has a very good sizeup performance; however, the more nodes are used, the more important the parameterization becomes. We see that 4 or 5 features per partition yield generally better sizeup than 8 or 10 features. While 4, 5 as well as 8 and 10 features show very similar sizeup performance. For the latter, the sizeup deteriorates for m ≥ 6. This is in stark contrast to the speedup results discussed earlier where we have observed better speedups for more features per partition. Thus, depending on the size of the database in terms of features and the number of nodes available different parameter settings will be more appropriate.</p><p>Figure <ref type="figure" target="#fig_2">12</ref> shows the scaleup for 1, 2, 4, and 8 nodes and the corresponding data sets Amazon1000, Amazon2000, Amazon4000, and Amazon8000. For up to 4 nodes, the scaleup is close to 1 for most parameter settings; however, it drops below 1 for 8 nodes. Therefore, we can conclude that using a very large number of nodes does not necessarily yield much better runtimes while we obtain large improvements with moderate parallelization. This is in line with the speedup results in Fig. <ref type="figure">8</ref> where the speedup was close to linear for most parameter settings for up to 4 or 8 nodes and only deteriorates quickly with more nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Comparison with other feature selection techniques</head><p>To demonstrate that our method is suitable with respect to classification, i.e., to demonstrate that Sp-RST performs its feature selection task well without sacrificing performance, we perform model evaluation and investigate the influence of Sp-RST on the classification of a Naive Bayes (Sect. 7.4.1) and a random forest (Sect. 7.4.2) classifier and compare the results with the original data set with 10,000 features and other feature selection techniques (see Sect. 6.3).</p><p>To account for the stochasticity of Sp-RST, we obtain six different feature sets for each parameter setting in independent runs of Sp-RST. For all other methods, we obtain a unique data set. We run the two classifiers on all these data sets and combine results for data sets based on the same parameter setting in Sp-RST. We present average and standard deviation of 10 independent runs for each considered data set and visualize the results as boxplots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Naive Bayes</head><p>From Fig. <ref type="figure" target="#fig_1">13</ref> and Table <ref type="table" target="#tab_8">11</ref> in "Appendix A", we can see that the classification results strictly improve when the number of features per partition in Sp-RST is increased. Moreover, they only slightly worsen in comparison with the original data set. For example, the accuracy for 4 features is 0.5634, for 10 features 0.5934 and for the original data set 0.6099.</p><p>Results differ widely depending on parameterisation for the other feature selection techniques as shown in Figs. <ref type="figure">14</ref> and<ref type="figure">15</ref> and<ref type="figure">Table 11 ("Appendix A"</ref>). This demonstrates that the other techniques tested are much more sensitive to suitable parameter settings than Sp-RST. As noted before, parameterisation can be a very difficult task to perform, thus demonstrating a clear strength of our method.</p><p>Recall that the parameter to determine the number of features to be selected was set in a way that mirrors the number of features selected by Sp-RST, namely the average number of features selected in the four different parameter settings (see <ref type="bibr">Table 10)</ref>. Additionally, we used a standard threshold value of 0. Looking more closely into the accuracy results for the best parameter setting of Sp-RST, i.e., 10 features, we see that it outperforms all parameterisations of CV and Consistency, but is outperformed by all parameterisations of Cfs, Correlation, ReliefF and Sum Squares Ratio. For the remaining methods, only the parameterisation with  the largest number of features (6171) outperforms Sp-RST (with the exception of significance which also obtains good results for a threshold of 0). It should be noted that the overall best accuracy is obtained by Sum Squares Ratio with 4117 features (0.6556) while the worst accuracy is obtained CV with 3205 features (0.4877). Recall that the accuracy of Sp-RST ranges from 0.5634 to 0.5934. To further validate these conclusions, pairwise Wilcoxon rank sum tests with Bonferroni correction were executed (see <ref type="bibr">Tables 14,</ref><ref type="bibr" target="#b21">15,</ref><ref type="bibr" target="#b22">16,</ref><ref type="bibr" target="#b23">17</ref> in "Appendix B"). We observe that the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(f) 32 Nodes</head><p>Fig. <ref type="figure">11</ref> Runtimes for different numbers of nodes majority of observed differences are statistically significant at a confidence level of 0.05, but there are some notable exceptions where statistical significance could not be confirmed by the test. These include Sp-RST with 8 and 10 features Sp-RST with 10 features in comparison with Chi-Squared, Gain Ratio, Info Gain, Significance and Symmetrical Uncertainty with parameters 5566 and 6171. We remark that these are exactly the methods that only outperformed Sp-RST <ref type="bibr" target="#b16">(10)</ref> for the parameter setting of 6171, but were worse for the other settings of the number of parameters to be selected.  While we have only provided a detailed for the accuracy metric, identical observations can be made for the other three metrics (precision, recall, and F1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Random forest</head><p>The overall classification performance of the random forest classifier is much better than the performance of the Naive Bayes classifier. To be more precise, the accuracy ranges from 0.8121 (Significance with 6171 features) to 0.6483 (Consistency). The accuracy of Sp-RST is between 0.7733 and 0.7938 and again strictly increasing with the number of features per partition, thus comparable to the best performing methods. Detailed results can be found in Figs. <ref type="bibr" target="#b22">16</ref>, 17 and 18 and Table <ref type="table" target="#tab_10">12</ref> in "Appendix A". We see that the overall ranking of the methods tested is very similar to the Naive Bayes results and that Sp-RST outperforms/is outperformed by roughly the same methods and parameterisations. The main exception is that for methods where only parameter 6171 performed better in the case of Naive Bayes, now also parameter 5566 produces slightly better results. However, the overall differences are much smaller than in the case of the Naive Bayes classifier.   <ref type="table" target="#tab_17">18,</ref><ref type="table" target="#tab_18">19</ref>, 20, 21 in "Appendix B"). While most results for 4, 5 and 8 features per partition are statistically significant at confidence level 0.05, for quite a few comparisons with SP-RST <ref type="bibr" target="#b16">(10)</ref> no statistical significance could be observed. This particularly holds for medium parameters such as 4117 and 5566 features, while in the case of the random forest classifier the parameter setting 6171 produces statistically different results in terms of accuracy. Recall that this was different for the Naive Bayes classifier. While we have only discussed the accuracy results in detail, identical observations can be made for the other three metrics shown in the graphs and tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3">Runtime comparison</head><p>We have provided a detailed analysis of the runtime for Sp-RST in Sect. 7.3. It should be noted that all other methods have negligible runtime for the feature selection part, and thus, there is a clear trade-off between runtime and feature selection quality. Based on the comparable classification results, we argue that the additional runtime is worthwhile, particularly if the reduced feature set has the potential to be used repeatedly in different applications. We summarize the runtime for the classification in Table <ref type="table" target="#tab_12">13</ref> and depict boxplots in Figs. <ref type="figure">19</ref> and<ref type="figure" target="#fig_22">20</ref>.</p><p>Besides these results, it is important to recall that as proved in <ref type="bibr" target="#b12">[6]</ref>, these existent methods are inadequate to cope with a higher number of features as they will have scalability problems. Hence, based on our detailed analysis and based on the results obtained from <ref type="bibr" target="#b12">[6]</ref>, we can conclude that our proposed Sp-RST solution is a scalable and effective method for large-scale data pre-processing. Thus, it is relevant to big data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and future work</head><p>In this paper, we have introduced a parallelized filter feature selection technique based on rough set theory, called Sp-RST. We have presented a comprehensive experimental study to investigate parameter settings and to demonstrate the stability and scalability of our proposed method. Moreover, we have compared Sp-RST with other commonly used filter techniques. We have used model evaluation using a Naive Bayes and a random forest classifier to evaluate the quality of the feature set selected by Sp-RST and the other methods considered.</p><p>Our experiments show that the proposed method effectively performs feature selection in a founded way without sacrificing performance. In terms of scalability, using a moderate number of nodes yields a considerably improvement of the runtime and good speedup performance. However, the improvement quickly stagnates if 8 or more nodes are used. Thus, improving the speedup, sizeup and scaleup performance for more nodes is subject to future work.</p><p>Our results show that Sp-RST is competitive or better against other methods on the Amazon data set and only induces very small information loss: For the best Sp-RST parameter setting, the classification accuracy for Naive Bayes is 6.22% smaller than for the best comparator (Sum Squares), while for random forest it is only 1.83% (Signficance). In comparison with the original data set, we lose only 1.65% for Naive Bayes and 0.88% for random forest. Moreover, we have demonstrated that Sp-RST is able to reliably identify the most and least important features in the data set, which can be an important aspect when interpreting the feature selection results from an application perspective. Improving the overall classification ratio of Sp-RST is subject to future work. Particularly, we plan to investigate the performance and behavior of Sp-RST on other larger data sets to further our understanding of its working principles.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>except for the first step, Algorithm 1-line 1, which deals with the calculation of the indiscernibility relation for the decision class D: IND(D). Sp-RST performs seven main distributed jobs to generate the final output, i.e., Reduct. Sp-RST stars first of all by computing the indiscernibility relation for the decision class D = {d 1 , d 2 , . . . , d W }. We define the indiscernibility relation as IND(D): IND(d i ), where i ∈ {1, 2, . . . , W }. Sp-RST will calculate IND(D) for each decision class d i by associating</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 The global functioning of Sp-RST</figDesc><graphic coords="16,49.59,55.85,340.36,275.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2</head><label>2</label><figDesc>transformation operation is applied to merge all values of each key in the transformed RDD output. This is to represent the sought IND(D): IND(d i ). The pseudo-code related to this distributed job is highlighted in Algorithm 2. Calculate IND(D) Input: T RDD Output: IND(D): [d i , List of id i of x i ] 1: Map the T RDD based on its format (id i of x i , List of the features of x i , Class d i of x i ) and generate the new format as a key-value pair Class d i of x i , List of id i of x i 2: Merge the values of each generated key using the f old ByK ey() operation 3: Return IND(D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(C r ) Input: C r Output: AllComb (C r ) 1: Generate the AllComb (C r ) RDD by applying the f latmap() function and the combinations() operation on each element in C r 2: Return AllComb (C r )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a reduce function over IND(D) based on a sum() function on the calculated scores which are indeed based on the elements having the same IND(d i ) 11: End for 12: Apply a reduce function over AllComb (C r ) based on a sum() function on the calculated results of Step (10) which are indeed based on the elements having the same AllComb (C r ) i 13: End for 14: Return AllComb (C r ) : γ (AllComb (C r ) ), Size (AllComb (Cr ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 6</head><label>6</label><figDesc>Select DE P max (AllComb (C r ) ) Input: RDD[AllComb (C r ) , Size (AllComb (Cr ) ) , γ (AllComb (C r ) )] Output: Max Dependency 1: Apply the max() function on the third argument of the given RDD: γ (AllComb (C r ) ) 2: Return Max Dependency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>set of conditional features C, Sp-RST starts first of all by computing the indiscernibility relation for the decision class Flu. We define the indiscernibility relation as IND(Flu): IND(Flu i ). Sp-RST will calculate IND(Flu) for each decision class Flu i by associating the same T RDD (C) data items (instances) that are expressed in the universe U and that belong to the same decision class Flu i . Based on the Apache Spark framework and by applying Algorithm 2, line 1, we get the following outputs from the different Apache Spark data splits which are presented in Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig. Number of features selected depending on the number of iterations executed in a run of Sp-RST (average and standard deviation over 4 runs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.</head><label></label><figDesc>Fig. Runtime of Sp-RST depending on the number of iterations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Number of features selected i times in i iterations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig.</head><label></label><figDesc>Fig. Speedup for the six data sets discussed in Sect. 6.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig.</head><label></label><figDesc>Fig. Runtimes (in seconds) for the six data sets discussed in Sect. 6.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Sizeup for different numbers of nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Fig. 12 Scaleup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 13 Fig. 14 Fig. 16</head><label>131416</label><figDesc>Fig. 13 Naive Bayes classification results for different feature selection techniques</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 17 Fig. 19</head><label>1719</label><figDesc>Fig. 17Random forest classification results for other feature selection techniques (Part 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 20</head><label>20</label><figDesc>Fig. 20 Runtime of the random forest classifier for the results of different feature selection techniques</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,49.59,55.58,340.36,184.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="14,49.59,55.67,340.36,243.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 , u 2 ) ∈ IND(P), then u 1 is indistinguishable (indiscernible) from u 2 by the attributes P. This relation is reflexive, symmetric and transitive. The induced set of equivalence classes is denoted as [u] P where u ∈ U , and it partitions U into different blocks denoted as U /P.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Toy data set</figDesc><table><row><cell></cell><cell>Patient ID</cell><cell>Headache</cell><cell>Muscle-pain</cell><cell>Temperature</cell><cell>Flu</cell></row><row><cell></cell><cell>x 0</cell><cell>Yes</cell><cell>Yes</cell><cell>Very-high</cell><cell>Yes</cell></row><row><cell></cell><cell>x 1</cell><cell>Yes</cell><cell>No</cell><cell>High</cell><cell>Yes</cell></row><row><cell></cell><cell>x 2</cell><cell>Yes</cell><cell>No</cell><cell>High</cell><cell>No</cell></row><row><cell></cell><cell>x 3</cell><cell>No</cell><cell>Yes</cell><cell>Normal</cell><cell>No</cell></row><row><cell></cell><cell>x 4</cell><cell>No</cell><cell>Yes</cell><cell>High</cell><cell>Yes</cell></row><row><cell></cell><cell>x 5</cell><cell>No</cell><cell>Yes</cell><cell>Very-high</cell><cell>Yes</cell></row><row><cell>Table 2 Toy data set-split 1</cell><cell>Patient ID</cell><cell>Headache</cell><cell>Muscle-pain</cell><cell>Temperature</cell><cell>Flu</cell></row><row><cell></cell><cell>x 0</cell><cell>Yes</cell><cell>Yes</cell><cell>Very-high</cell><cell>Yes</cell></row><row><cell></cell><cell>x 1</cell><cell>Yes</cell><cell>No</cell><cell>High</cell><cell>Yes</cell></row><row><cell></cell><cell>x 2</cell><cell>Yes</cell><cell>No</cell><cell>High</cell><cell>No</cell></row><row><cell>Table 3 Toy data set-split 2</cell><cell>Patient ID</cell><cell>Headache</cell><cell>Muscle-pain</cell><cell>Temperature</cell><cell>Flu</cell></row><row><cell></cell><cell>x 3</cell><cell>No</cell><cell>Yes</cell><cell>Normal</cell><cell>No</cell></row><row><cell></cell><cell>x 4</cell><cell>No</cell><cell>Yes</cell><cell>High</cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Partition m = 1-split 1</figDesc><table><row><cell></cell><cell>Patient ID</cell><cell>Muscle-pain</cell><cell>Temperature</cell><cell>Flu</cell></row><row><cell></cell><cell>x 0</cell><cell>Yes</cell><cell>Very-high</cell><cell>Yes</cell></row><row><cell></cell><cell>x 1</cell><cell>No</cell><cell>High</cell><cell>Yes</cell></row><row><cell></cell><cell>x 2</cell><cell>No</cell><cell>High</cell><cell>No</cell></row><row><cell>Table 5 Partition m = 1-split 2</cell><cell>Patient ID</cell><cell>Muscle-pain</cell><cell>Temperature</cell><cell>Flu</cell></row><row><cell></cell><cell>x 3</cell><cell>Yes</cell><cell>Normal</cell><cell>No</cell></row><row><cell></cell><cell>x 4</cell><cell>Yes</cell><cell>High</cell><cell>Yes</cell></row><row><cell></cell><cell>x 5</cell><cell>Yes</cell><cell>Very-high</cell><cell>Yes</cell></row><row><cell>Table 6 Partition m = 2-split 1</cell><cell>Patient ID</cell><cell>Headache</cell><cell></cell><cell>Flu</cell></row><row><cell></cell><cell>x 0</cell><cell>Yes</cell><cell></cell><cell>Yes</cell></row><row><cell></cell><cell>x 1</cell><cell>Yes</cell><cell></cell><cell>Yes</cell></row><row><cell></cell><cell>x 2</cell><cell>Yes</cell><cell></cell><cell>No</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7</head><label>7</label><figDesc>Partition m = 2-split 2</figDesc><table><row><cell>Patient ID</cell><cell>Headache</cell><cell>Flu</cell></row><row><cell>x 3</cell><cell>No</cell><cell>No</cell></row><row><cell>x 4</cell><cell>No</cell><cell>Yes</cell></row><row><cell>x 5</cell><cell>No</cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 . 123 Table 8</head><label>81238</label><figDesc></figDesc><table><row><cell>Reduct</cell><cell>Patient ID</cell><cell>Headache</cell><cell>Temperature</cell><cell>Flu</cell></row><row><cell></cell><cell>x 0</cell><cell>Yes</cell><cell>Very-high</cell><cell>Yes</cell></row><row><cell></cell><cell>x 1</cell><cell>Yes</cell><cell>High</cell><cell>Yes</cell></row><row><cell></cell><cell>x 2</cell><cell>Yes</cell><cell>High</cell><cell>No</cell></row><row><cell></cell><cell>x 3</cell><cell>No</cell><cell>Normal</cell><cell>No</cell></row><row><cell></cell><cell>x 4</cell><cell>No</cell><cell>High</cell><cell>Yes</cell></row><row><cell></cell><cell>x 5</cell><cell>No</cell><cell>Very-high</cell><cell>Yes</cell></row><row><cell>6 Experimental setup</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6.1 Benchmark</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>table presented in Table 1 from T RDD (C) to T RDD (Reduct). The output is presented in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9</head><label>9</label><figDesc>Number of features selected in a single iteration Number of features selected 0, 1, …, 6 times in six independent runs of Sp-RST</figDesc><table><row><cell>#features per partition</cell><cell>4</cell><cell>5</cell><cell>8</cell><cell>10</cell></row><row><cell>Mean (80 iterations)</cell><cell>7278.15</cell><cell>7875.2</cell><cell>8731.725</cell><cell>8994.25</cell></row><row><cell>SD (80 iterations)</cell><cell>13.5629</cell><cell>12.5329</cell><cell>5.1484</cell><cell>2.7212</cell></row><row><cell>Min (80 iterations)</cell><cell>7247</cell><cell>7841</cell><cell>8721</cell><cell>8985</cell></row><row><cell>max (80 iterations)</cell><cell>7313</cell><cell>7909</cell><cell>8744</cell><cell>9000</cell></row><row><cell>Always (10 iterations)</cell><cell>3216</cell><cell>4074</cell><cell>5541</cell><cell>6214</cell></row><row><cell>Never (10 iterations)</cell><cell>31</cell><cell>9</cell><cell>2</cell><cell>0</cell></row><row><cell>Always (20 iterations)</cell><cell>2521</cell><cell>3498</cell><cell>4928</cell><cell>5545</cell></row><row><cell>Never (20 iterations)</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Always (40 iterations)</cell><cell>1914</cell><cell>3020</cell><cell>4477</cell><cell>5070</cell></row><row><cell>Never (40 iterations)</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Always (80 iterations)</cell><cell>1396</cell><cell>2528</cell><cell>4176</cell><cell>4693</cell></row><row><cell>Never (80 iterations)</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>123</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10</head><label>10</label><figDesc>Number of features selected in a run of Sp-RST</figDesc><table><row><cell>#features per partition</cell><cell>4</cell><cell>5</cell><cell>8</cell><cell>10</cell></row><row><cell>Mean</cell><cell>3205.333</cell><cell>4117</cell><cell>5566</cell><cell>6171.333</cell></row><row><cell>SD</cell><cell>24.3530</cell><cell>18.9526</cell><cell>23.8914</cell><cell>26.0128</cell></row><row><cell>Min</cell><cell>3178</cell><cell>4093</cell><cell>5542</cell><cell>6131</cell></row><row><cell>Max</cell><cell>3245</cell><cell>4138</cell><cell>5598</cell><cell>6197</cell></row><row><cell>Always</cell><cell>1561</cell><cell>2780</cell><cell>4287</cell><cell>4851</cell></row><row><cell>Never</cell><cell>5216</cell><cell>4463</cell><cell>2833</cell><cell>2225</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11</head><label>11</label><figDesc>Naive Bayes classification results for different feature selection techniques</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>Original</cell><cell>10,000</cell><cell>0.6099</cell><cell>0.6451</cell><cell>0.6098</cell><cell>0.6118</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0080)</cell><cell>(0.0082)</cell><cell>(0.0080)</cell><cell>(0.0087)</cell></row><row><cell>CV (t = 0)</cell><cell>4149</cell><cell>0.5152</cell><cell>0.5703</cell><cell>0.5152</cell><cell>0.5213</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0067)</cell><cell>(0.0082)</cell><cell>(0.0068)</cell><cell>(0.0067)</cell></row><row><cell>CV (3205)</cell><cell>3205</cell><cell>0.4877</cell><cell>0.5387</cell><cell>0.4878</cell><cell>0.4925</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0090)</cell><cell>(0.0094)</cell><cell>(0.0089)</cell><cell>(0.0094)</cell></row><row><cell>CV (4117)</cell><cell>4117</cell><cell>0.5153</cell><cell>0.5701</cell><cell>0.5153</cell><cell>0.5213</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0074)</cell><cell>(0.0093)</cell><cell>(0.0075)</cell><cell>(0.0074)</cell></row><row><cell>CV (5566)</cell><cell>5566</cell><cell>0.5431</cell><cell>0.5906</cell><cell>0.5432</cell><cell>0.5475</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0069)</cell><cell>(0.0075)</cell><cell>(0.0070)</cell><cell>(0.0067)</cell></row><row><cell>CV (6171)</cell><cell>6171</cell><cell>0.5477</cell><cell>0.5933</cell><cell>0.5478</cell><cell>0.5522</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0072)</cell><cell>(0.0082)</cell><cell>(0.0072)</cell><cell>(0.0072)</cell></row><row><cell>Cfs (Greedy)</cell><cell>41</cell><cell>0.6083</cell><cell>0.6156</cell><cell>0.6084</cell><cell>0.6053</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0039)</cell><cell>(0.0049)</cell><cell>(0.0039)</cell><cell>(0.0043)</cell></row><row><cell>Chi squared (t = 0)</cell><cell>113</cell><cell>0.5771</cell><cell>0.5870</cell><cell>0.5771</cell><cell>0.5705</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0039)</cell><cell>(0.0055)</cell><cell>(0.0038)</cell><cell>(0.0043)</cell></row><row><cell>Chi squared (3205)</cell><cell>3205</cell><cell>0.5103</cell><cell>0.5340</cell><cell>0.5102</cell><cell>0.5123</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0071)</cell><cell>(0.0058)</cell><cell>(0.0059)</cell></row><row><cell>Chi squared (4117)</cell><cell>4117</cell><cell>0.5595</cell><cell>0.5823</cell><cell>0.5596</cell><cell>0.5580</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0051)</cell><cell>(0.0058)</cell><cell>(0.0053)</cell></row><row><cell>Chi squared (5566)</cell><cell>5566</cell><cell>0.5863</cell><cell>0.6190</cell><cell>0.5865</cell><cell>0.5872</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0038)</cell><cell>(0.0047)</cell><cell>(0.0037)</cell><cell>(0.0041)</cell></row><row><cell>Chi squared (6171)</cell><cell>6171</cell><cell>0.5965</cell><cell>0.6295</cell><cell>0.5966</cell><cell>0.5976</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0042)</cell><cell>(0.0041)</cell><cell>(0.0042)</cell><cell>(0.0042)</cell></row><row><cell>Consistency (Greedy)</cell><cell>30</cell><cell>0.5443</cell><cell>0.5514</cell><cell>0.5443</cell><cell>0.5391</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0044)</cell><cell>(0.0050)</cell><cell>(0.0045)</cell><cell>(0.0044)</cell></row><row><cell>Correlation (t = 0)</cell><cell>10,000</cell><cell>0.6147</cell><cell>0.6512</cell><cell>0.6147</cell><cell>0.6170</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0059)</cell><cell>(0.0063)</cell><cell>(0.0059)</cell><cell>(0.0069)</cell></row><row><cell>Correlation (3205)</cell><cell>3205</cell><cell>0.6232</cell><cell>0.6588</cell><cell>0.6231</cell><cell>0.6266</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0048)</cell><cell>(0.0065)</cell><cell>(0.0048)</cell><cell>(0.0056)</cell></row><row><cell>Correlation (4117)</cell><cell>4117</cell><cell>0.6207</cell><cell>0.6570</cell><cell>0.6208</cell><cell>0.6234</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0049)</cell><cell>(0.0077)</cell><cell>(0.0049)</cell><cell>(0.0058)</cell></row><row><cell>Correlation (5566)</cell><cell>5566</cell><cell>0.6145</cell><cell>0.6519</cell><cell>0.6143</cell><cell>0.6176</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0061)</cell><cell>(0.0079)</cell><cell>(0.0061)</cell><cell>(0.0070)</cell></row><row><cell>Correlation (6171)</cell><cell>6171</cell><cell>0.6187</cell><cell>0.6574</cell><cell>0.6189</cell><cell>0.6225</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0063)</cell><cell>(0.0065)</cell><cell>(0.0063)</cell><cell>(0.0064)</cell></row><row><cell>gain ratio (t = 0)</cell><cell>113</cell><cell>0.5771</cell><cell>0.5870</cell><cell>0.5771</cell><cell>0.5705</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0039)</cell><cell>(0.0055)</cell><cell>(0.0038)</cell><cell>(0.0043)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>continued</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>gain ratio (3205)</cell><cell>3205</cell><cell>0.5103</cell><cell>0.5340</cell><cell>0.5102</cell><cell>0.5123</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0071)</cell><cell>(0.0058)</cell><cell>(0.0059)</cell></row><row><cell>gain ratio (4117)</cell><cell>4117</cell><cell>0.5595</cell><cell>0.5823</cell><cell>0.5596</cell><cell>0.5580</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0051)</cell><cell>(0.0058)</cell><cell>(0.0053)</cell></row><row><cell>gain ratio (5566)</cell><cell>5566</cell><cell>0.5863</cell><cell>0.6190</cell><cell>0.5865</cell><cell>0.5872</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0038)</cell><cell>(0.0047)</cell><cell>(0.0037)</cell><cell>(0.0041)</cell></row><row><cell>gain ratio (6171)</cell><cell>6171</cell><cell>0.5965</cell><cell>0.6295</cell><cell>0.5966</cell><cell>0.5976</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0042)</cell><cell>(0.0041)</cell><cell>(0.0042)</cell><cell>(0.0042)</cell></row><row><cell>Info gain (t = 0)</cell><cell>113</cell><cell>0.5771</cell><cell>0.5870</cell><cell>0.5771</cell><cell>0.5705</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0039)</cell><cell>(0.0055)</cell><cell>(0.0038)</cell><cell>(0.0043)</cell></row><row><cell>Info gain (3205)</cell><cell>3205</cell><cell>0.5103</cell><cell>0.5340</cell><cell>0.5102</cell><cell>0.5123</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0071)</cell><cell>(0.0058)</cell><cell>(0.0059)</cell></row><row><cell>Info gain (4117)</cell><cell>4117</cell><cell>0.5595</cell><cell>0.5823</cell><cell>0.5596</cell><cell>0.5580</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0051)</cell><cell>(0.0058)</cell><cell>(0.0053)</cell></row><row><cell>Info gain (5566)</cell><cell>5566</cell><cell>0.5863</cell><cell>0.6190</cell><cell>0.5865</cell><cell>0.5872</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0038)</cell><cell>(0.0047)</cell><cell>(0.0037)</cell><cell>(0.0041)</cell></row><row><cell>Info gain (6171)</cell><cell>6171</cell><cell>0.5965</cell><cell>0.6294</cell><cell>0.5966</cell><cell>0.5976</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0042)</cell><cell>(0.0041)</cell><cell>(0.0042)</cell><cell>(0.0042)</cell></row><row><cell>ReliefF (t = 0)</cell><cell>7957</cell><cell>0.6087</cell><cell>0.6476</cell><cell>0.6087</cell><cell>0.6120</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0061)</cell><cell>(0.0069)</cell><cell>(0.0060)</cell><cell>(0.0067)</cell></row><row><cell>ReliefF (3205)</cell><cell>3205</cell><cell>0.6008</cell><cell>0.6367</cell><cell>0.6007</cell><cell>0.6044</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0076)</cell><cell>(0.0074)</cell><cell>(0.0076)</cell><cell>(0.0075)</cell></row><row><cell>ReliefF (4117)</cell><cell>4117</cell><cell>0.5972</cell><cell>0.6353</cell><cell>0.5973</cell><cell>0.6010</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0060)</cell><cell>(0.0068)</cell><cell>(0.0061)</cell><cell>(0.0065)</cell></row><row><cell>ReliefF (5566)</cell><cell>5566</cell><cell>0.6087</cell><cell>0.6497</cell><cell>0.6086</cell><cell>0.6121</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0062)</cell><cell>(0.0088)</cell><cell>(0.0062)</cell><cell>(0.0069)</cell></row><row><cell>ReliefF (6171)</cell><cell>6171</cell><cell>0.6093</cell><cell>0.6479</cell><cell>0.6093</cell><cell>0.6132</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0053)</cell><cell>(0.0076)</cell><cell>(0.0055)</cell><cell>(0.0065)</cell></row><row><cell>Significance (t = 0)</cell><cell>10,000</cell><cell>0.6139</cell><cell>0.6475</cell><cell>0.6138</cell><cell>0.6151</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0065)</cell><cell>(0.0065)</cell><cell>(0.0066)</cell><cell>(0.0068)</cell></row><row><cell>Significance (3205)</cell><cell>3205</cell><cell>0.5103</cell><cell>0.5340</cell><cell>0.5102</cell><cell>0.5123</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0071)</cell><cell>(0.0058)</cell><cell>(0.0059)</cell></row><row><cell>Significance (4117)</cell><cell>4117</cell><cell>0.5595</cell><cell>0.5823</cell><cell>0.5596</cell><cell>0.5580</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0057)</cell><cell>(0.0051)</cell><cell>(0.0058)</cell><cell>(0.0053)</cell></row><row><cell>Significance (5566)</cell><cell>5566</cell><cell>0.5863</cell><cell>0.6190</cell><cell>0.5865</cell><cell>0.5872</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0038)</cell><cell>(0.0047)</cell><cell>(0.0037)</cell><cell>(0.0041)</cell></row><row><cell>Significance (6171)</cell><cell>6171</cell><cell>0.5965</cell><cell>0.6295</cell><cell>0.5966</cell><cell>0.5976</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0042)</cell><cell>(0.0041)</cell><cell>(0.0042)</cell><cell>(0.0042)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12</head><label>12</label><figDesc>Random forest classification results for different feature selection techniques</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>Original</cell><cell>10,000</cell><cell>0.8026</cell><cell>0.8174</cell><cell>0.8026</cell><cell>0.7963</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0038)</cell><cell>(0.0042)</cell><cell>(0.0038)</cell><cell>(0.0044)</cell></row><row><cell>CV (t = 0)</cell><cell>4149</cell><cell>0.6855</cell><cell>0.7264</cell><cell>0.6855</cell><cell>0.6766</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0036)</cell><cell>(0.0070)</cell><cell>(0.0036)</cell><cell>(0.0040)</cell></row><row><cell>CV (3205)</cell><cell>3205</cell><cell>0.6503</cell><cell>0.7029</cell><cell>0.6503</cell><cell>0.6402</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0045)</cell><cell>(0.0060)</cell><cell>(0.0045)</cell><cell>(0.0052)</cell></row><row><cell>CV (4117)</cell><cell>4117</cell><cell>0.6789</cell><cell>0.7224</cell><cell>0.6789</cell><cell>0.6698</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0042)</cell><cell>(0.0079)</cell><cell>(0.0042)</cell><cell>(0.0043)</cell></row><row><cell>CV (5566)</cell><cell>5566</cell><cell>0.7337</cell><cell>0.7621</cell><cell>0.7337</cell><cell>0.7265</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0031)</cell><cell>(0.0033)</cell><cell>(0.0031)</cell><cell>(0.0038)</cell></row><row><cell>CV (6171)</cell><cell>6171</cell><cell>0.7451</cell><cell>0.7740</cell><cell>0.7451</cell><cell>0.7381</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0050)</cell><cell>(0.0051)</cell><cell>(0.0050)</cell><cell>(0.0050)</cell></row><row><cell>Cfs (Greedy)</cell><cell>41</cell><cell>0.6881</cell><cell>0.6881</cell><cell>0.6881</cell><cell>0.6786</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0019)</cell><cell>(0.0027)</cell><cell>(0.0019)</cell><cell>(0.0022)</cell></row><row><cell>Chi squared (t = 0)</cell><cell>113</cell><cell>0.7107</cell><cell>0.7171</cell><cell>0.7107</cell><cell>0.7070</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0029)</cell><cell>(0.0033)</cell><cell>(0.0029)</cell><cell>(0.0029)</cell></row><row><cell>Chi squared (3205)</cell><cell>3205</cell><cell>0.7607</cell><cell>0.7717</cell><cell>0.7607</cell><cell>0.7513</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0029)</cell><cell>(0.0042)</cell><cell>(0.0029)</cell><cell>(0.0029)</cell></row><row><cell>Chi squared (4117)</cell><cell>4117</cell><cell>0.7927</cell><cell>0.8054</cell><cell>0.7927</cell><cell>0.7834</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0036)</cell><cell>(0.0038)</cell><cell>(0.0036)</cell><cell>(0.0039)</cell></row><row><cell>Chi squared (5566)</cell><cell>5566</cell><cell>0.8002</cell><cell>0.8145</cell><cell>0.8002</cell><cell>0.7919</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0038)</cell><cell>(0.0044)</cell><cell>(0.0038)</cell><cell>(0.0041)</cell></row><row><cell>Chi squared (6171)</cell><cell>6171</cell><cell>0.8073</cell><cell>0.8225</cell><cell>0.8073</cell><cell>0.7998</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0058)</cell><cell>(0.0059)</cell><cell>(0.0058)</cell><cell>(0.0062)</cell></row><row><cell>Consistency (Greedy)</cell><cell>30</cell><cell>0.6483</cell><cell>0.6489</cell><cell>0.6483</cell><cell>0.6383</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0033)</cell><cell>(0.0047)</cell><cell>(0.0033)</cell><cell>(0.0035)</cell></row><row><cell>Correlation (t = 0)</cell><cell>10,000</cell><cell>0.8016</cell><cell>0.8178</cell><cell>0.8016</cell><cell>0.7949</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0064)</cell><cell>(0.0081)</cell><cell>(0.0064)</cell><cell>(0.0068)</cell></row><row><cell>Correlation (3205)</cell><cell>3205</cell><cell>0.7930</cell><cell>0.8073</cell><cell>0.7930</cell><cell>0.7860</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0032)</cell><cell>(0.0049)</cell><cell>(0.0032)</cell><cell>(0.0036)</cell></row><row><cell>Correlation (4117)</cell><cell>4117</cell><cell>0.7979</cell><cell>0.8134</cell><cell>0.7979</cell><cell>0.7909</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0054)</cell><cell>(0.0054)</cell><cell>(0.0054)</cell><cell>(0.0060)</cell></row><row><cell>Correlation (5566)</cell><cell>5566</cell><cell>0.8017</cell><cell>0.8169</cell><cell>0.8017</cell><cell>0.7949</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0040)</cell><cell>(0.0041)</cell><cell>(0.0040)</cell><cell>(0.0041)</cell></row><row><cell>Correlation (6171)</cell><cell>6171</cell><cell>0.8029</cell><cell>0.8186</cell><cell>0.8029</cell><cell>0.7966</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0049)</cell><cell>(0.0041)</cell><cell>(0.0049)</cell><cell>(0.0054)</cell></row><row><cell>Gain ratio (t = 0)</cell><cell>113</cell><cell>0.7122</cell><cell>0.7178</cell><cell>0.7122</cell><cell>0.7083</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0024)</cell><cell>(0.0029)</cell><cell>(0.0024)</cell><cell>(0.0027)</cell></row><row><cell>Gain ratio (3205)</cell><cell>3205</cell><cell>0.7586</cell><cell>0.7681</cell><cell>0.7586</cell><cell>0.7487</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0054)</cell><cell>(0.0064)</cell><cell>(0.0054)</cell><cell>(0.0055)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>continued</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>#Features</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell>SpRST (10)</cell><cell>6171.333</cell><cell>0.7938</cell><cell>0.8107</cell><cell>0.7938</cell><cell>0.7870</cell></row><row><cell></cell><cell>(26.01282)</cell><cell>(0.0063)</cell><cell>(0.0063)</cell><cell>(0.0063)</cell><cell>(0.0069)</cell></row><row><cell>SumSquaresRatio (3205)</cell><cell>3205</cell><cell>0.8111</cell><cell>0.8239</cell><cell>0.8111</cell><cell>0.8051</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0052)</cell><cell>(0.0055)</cell><cell>(0.0052)</cell><cell>(0.0055)</cell></row><row><cell>SumSquaresRatio (4117)</cell><cell>4117</cell><cell>0.8091</cell><cell>0.8212</cell><cell>0.8091</cell><cell>0.8034</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0035)</cell><cell>(0.0037)</cell><cell>(0.0035)</cell><cell>(0.0038)</cell></row><row><cell>SumSquaresRatio (5566)</cell><cell>5566</cell><cell>0.8066</cell><cell>0.8213</cell><cell>0.8066</cell><cell>0.8002</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0032)</cell><cell>(0.0059)</cell><cell>(0.0032)</cell><cell>(0.0033)</cell></row><row><cell>SumSquaresRatio (6171)</cell><cell>6171</cell><cell>0.8045</cell><cell>0.8203</cell><cell>0.8045</cell><cell>0.7982</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0044)</cell><cell>(0.0048)</cell><cell>(0.0044)</cell><cell>(0.0048)</cell></row><row><cell>Symmetricaluncert (t = 0)</cell><cell>113</cell><cell>0.7117</cell><cell>0.7182</cell><cell>0.7117</cell><cell>0.7079</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0044)</cell><cell>(0.0038)</cell><cell>(0.0044)</cell><cell>(0.0044)</cell></row><row><cell>Symmetricaluncert (3205)</cell><cell>3205</cell><cell>0.7593</cell><cell>0.7713</cell><cell>0.7593</cell><cell>0.7498</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0030)</cell><cell>(0.0033)</cell><cell>(0.0030)</cell><cell>(0.0037)</cell></row><row><cell>Symmetricaluncert (4117)</cell><cell>4117</cell><cell>0.7930</cell><cell>0.8055</cell><cell>0.7930</cell><cell>0.7839</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0042)</cell><cell>(0.0054)</cell><cell>(0.0042)</cell><cell>(0.0047)</cell></row><row><cell>Symmetricaluncert (5566)</cell><cell>5566</cell><cell>0.8021</cell><cell>0.8170</cell><cell>0.8021</cell><cell>0.7936</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0048)</cell><cell>(0.0047)</cell><cell>(0.0048)</cell><cell>(0.0052)</cell></row><row><cell>Symmetricaluncert (6171)</cell><cell>6171</cell><cell>0.8089</cell><cell>0.8242</cell><cell>0.8089</cell><cell>0.8011</cell></row><row><cell></cell><cell>(-)</cell><cell>(0.0042)</cell><cell>(0.0054)</cell><cell>(0.0042)</cell><cell>(0.0048)</cell></row><row><cell cols="2">Averages and standard deviation in brackets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13</head><label>13</label><figDesc>Summary of runtimes (in seconds)</figDesc><table><row><cell>Method</cell><cell>#Features</cell><cell>Selection</cell><cell>Random forest</cell><cell>Naive Bayes</cell></row><row><cell>Original</cell><cell>10,000</cell><cell>0</cell><cell>478.1466</cell><cell>154.3220</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(29.3932)</cell><cell>(13.4778)</cell></row><row><cell>CV (t = 0)</cell><cell>4149</cell><cell>49</cell><cell>184.2093</cell><cell>59.7090</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(8.0784)</cell><cell>(5.3565)</cell></row><row><cell>CV (3205)</cell><cell>3205</cell><cell>46</cell><cell>240.1020</cell><cell>44.5060</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(6.9240)</cell><cell>(3.9978)</cell></row><row><cell>CV (4117)</cell><cell>4117</cell><cell>47</cell><cell>199.5176</cell><cell>58.9020</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(36.3792)</cell><cell>(6.2692)</cell></row><row><cell>CV (5566)</cell><cell>5566</cell><cell>48</cell><cell>200.4283</cell><cell>78.9990</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(3.1720)</cell><cell>(9.0758)</cell></row><row><cell>CV (6171)</cell><cell>6171</cell><cell>48</cell><cell>258.3198</cell><cell>86.9790</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(40.7532)</cell><cell>(8.5235)</cell></row><row><cell>Cfs (Greedy)</cell><cell>41</cell><cell>296</cell><cell>72.0946</cell><cell>1.1660</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(7.3634)</cell><cell>(0.1060)</cell></row><row><cell>Chi squared (t = 0)</cell><cell>113</cell><cell>2</cell><cell>91.0451</cell><cell>2.0520</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(6.9113)</cell><cell>(0.2946)</cell></row><row><cell>Chi squared (3205)</cell><cell>3205</cell><cell>3</cell><cell>197.3493</cell><cell>50.5860</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(33.7688)</cell><cell>(16.9048)</cell></row><row><cell>Chi squared (4117)</cell><cell>4117</cell><cell>2</cell><cell>258.7080</cell><cell>59.6300</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(21.4711)</cell><cell>(5.5790)</cell></row><row><cell>Chi squared (5566)</cell><cell>5566</cell><cell>2</cell><cell>218.7398</cell><cell>77.6480</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(7.0253)</cell><cell>(4.9175)</cell></row><row><cell>Chi squared (6171)</cell><cell>6171</cell><cell>3</cell><cell>233.1519</cell><cell>84.5110</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(13.3455)</cell><cell>(4.3186)</cell></row><row><cell>Consistency (Greedy)</cell><cell>30</cell><cell>608</cell><cell>55.0971</cell><cell>0.9240</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(5.3676)</cell><cell>(0.0534)</cell></row><row><cell>Correlation (t = 0)</cell><cell>10,000</cell><cell>10</cell><cell>339.7830</cell><cell>135.5330</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(20.4477)</cell><cell>(6.5574)</cell></row><row><cell>Correlation (3205)</cell><cell>3205</cell><cell>8</cell><cell>222.5547</cell><cell>39.6600</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(8.3334)</cell><cell>(2.3442)</cell></row><row><cell>Correlation (4117)</cell><cell>4117</cell><cell>8</cell><cell>409.2935</cell><cell>53.0690</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(266.9731)</cell><cell>(2.9146)</cell></row><row><cell>Correlation (5566)</cell><cell>5566</cell><cell>9</cell><cell>242.3133</cell><cell>74.3500</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(13.5045)</cell><cell>(4.3644)</cell></row><row><cell>Correlation (6171)</cell><cell>6171</cell><cell>8</cell><cell>277.2288</cell><cell>81.8020</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(9.0772)</cell><cell>(4.5509)</cell></row><row><cell>Gain ratio (t = 0)</cell><cell>113</cell><cell>7</cell><cell>88.3953</cell><cell>1.9850</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(7.3338)</cell><cell>(0.1276)</cell></row><row><cell>Gain ratio (3205)</cell><cell>3205</cell><cell>8</cell><cell>211.9993</cell><cell>44.6560</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(8.9143)</cell><cell>(1.6366)</cell></row><row><cell>Gain ratio (4117)</cell><cell>4117</cell><cell>9</cell><cell>207.8572</cell><cell>58.0990</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(33.3359)</cell><cell>(2.7974)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell>continued</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>#Features</cell><cell>Selection</cell><cell>Random forest</cell><cell>Naive Bayes</cell></row><row><cell>Gain ratio (5566)</cell><cell>5566</cell><cell>8</cell><cell>229.9701</cell><cell>77.2070</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(10.6783)</cell><cell>(3.2169)</cell></row><row><cell>Gain ratio (6171)</cell><cell>6171</cell><cell>9</cell><cell>236.6279</cell><cell>88.3710</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(10.8751)</cell><cell>(7.0199)</cell></row><row><cell>Info gain (t = 0)</cell><cell>113</cell><cell>7</cell><cell>81.1204</cell><cell>2.0050</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(2.7119)</cell><cell>(0.1910)</cell></row><row><cell>Info gain (3205)</cell><cell>3205</cell><cell>8</cell><cell>203.8654</cell><cell>45.8720</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(5.2012)</cell><cell>(3.2261)</cell></row><row><cell>Info gain (4117)</cell><cell>4117</cell><cell>8</cell><cell>234.0230</cell><cell>58.9890</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(31.3471)</cell><cell>(2.7353)</cell></row><row><cell>Info gain (5566)</cell><cell>5566</cell><cell>9</cell><cell>200.8629</cell><cell>78.1260</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(17.7479)</cell><cell>(3.4878)</cell></row><row><cell>Info gain (6171)</cell><cell>6171</cell><cell>8</cell><cell>227.2942</cell><cell>85.6750</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(3.2503)</cell><cell>(5.4645)</cell></row><row><cell>ReliefF (t = 0)</cell><cell>7957</cell><cell>1</cell><cell>327.6763</cell><cell>106.4700</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(63.6214)</cell><cell>(6.8651)</cell></row><row><cell>ReliefF (3205)</cell><cell>3205</cell><cell>8</cell><cell>199.1945</cell><cell>41.4520</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(6.3894)</cell><cell>(2.0377)</cell></row><row><cell>ReliefF (4117)</cell><cell>4117</cell><cell>8</cell><cell>225.2873</cell><cell>53.7170</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(6.5349)</cell><cell>(3.5077)</cell></row><row><cell>ReliefF (5566)</cell><cell>5566</cell><cell>9</cell><cell>253.8634</cell><cell>73.7800</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(8.2094)</cell><cell>(5.1618)</cell></row><row><cell>ReliefF (6171)</cell><cell>6171</cell><cell>9</cell><cell>321.8576</cell><cell>80.0170</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(87.1975)</cell><cell>(3.2470)</cell></row><row><cell>Significance (t = 0)</cell><cell>10,000</cell><cell>3</cell><cell>345.5262</cell><cell>135.7860</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(47.0985)</cell><cell>(5.4555)</cell></row><row><cell>Significance (3205)</cell><cell>3205</cell><cell>3</cell><cell>150.2230</cell><cell>45.2610</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(4.9146)</cell><cell>(2.3983)</cell></row><row><cell>Significance (4117)</cell><cell>4117</cell><cell>2</cell><cell>161.6598</cell><cell>58.9500</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(8.0198)</cell><cell>(3.0718)</cell></row><row><cell>Significance (5566)</cell><cell>5566</cell><cell>3</cell><cell>326.4685</cell><cell>76.0120</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(14.4817)</cell><cell>(3.0610)</cell></row><row><cell>Significance (6171)</cell><cell>6171</cell><cell>3</cell><cell>227.0810</cell><cell>84.6230</cell></row><row><cell></cell><cell>(-)</cell><cell>(-)</cell><cell>(43.6380)</cell><cell>(4.6004)</cell></row><row><cell>SpRST (4)</cell><cell></cell><cell>1 node: 51.98853</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2 nodes: 51</cell><cell></cell><cell></cell></row><row><cell></cell><cell>3205.333</cell><cell>4 nodes: 25.86191</cell><cell>239.5910</cell><cell>39.8933</cell></row><row><cell></cell><cell>(24.35296)</cell><cell>8 nodes: 20.81937</cell><cell>(97.0897)</cell><cell>(1.2833)</cell></row><row><cell></cell><cell></cell><cell>16 nodes: 18.0587</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>32 nodes: 16.60878</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15</head><label>15</label><figDesc>Results of Wilcoxon rank sum tests with Bonferroni correction for Naive Bayes and the recall metric</figDesc><table><row><cell></cell><cell>SpRST (4)</cell><cell>SpRST (5)</cell><cell>SpRST (8)</cell><cell>SpRST (10)</cell></row><row><cell>SpRST (5)</cell><cell>1.7e-0 9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (8)</cell><cell>&lt; 2e-16</cell><cell>5.9e-15</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (10)</cell><cell>&lt; 2e-16</cell><cell>&lt; 2e-16</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>Original (-)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0032</cell><cell>0.0181</cell></row><row><cell>Cfs (Greedy)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0020</cell><cell>0.0035</cell></row><row><cell>Chi squared (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (4117)</cell><cell>1.0000</cell><cell>0.0204</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (5566)</cell><cell>0.0018</cell><cell>0.0044</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (6171)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (t = 0)</cell><cell>0.0042</cell><cell>1.0000</cell><cell>0.0383</cell><cell>0.0038</cell></row><row><cell>Consistency (Greedy)</cell><cell>0.0025</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Correlation (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Correlation (4117)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Correlation (5566)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0020</cell><cell>0.0022</cell></row><row><cell>Correlation (6171)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0019</cell></row><row><cell>Correlation (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (4117)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (5566)</cell><cell>0.0025</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (6171)</cell><cell>0.0178</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16</head><label>16</label><figDesc>Results of Wilcoxon rank sum tests with Bonferroni correction for Naive Bayes and the precision metric</figDesc><table><row><cell></cell><cell>SpRST (4)</cell><cell>SpRST (5)</cell><cell>SpRST (8)</cell><cell>SpRST (10)</cell></row><row><cell>SpRST (5)</cell><cell>3.3e-0 7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (8)</cell><cell>&lt; 2e-16</cell><cell>2.2e-13</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (10)</cell><cell>&lt; 2e-16</cell><cell>1.1e-14</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>Original (-)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0540</cell><cell>0.1821</cell></row><row><cell>Cfs (Greedy)</cell><cell>0.6941</cell><cell>1.0000</cell><cell>0.0060</cell><cell>0.0031</cell></row><row><cell>Chi squared (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (4117)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (5566)</cell><cell>0.0122</cell><cell>1.0000</cell><cell>0.0787</cell><cell>0.0194</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17</head><label>17</label><figDesc>Results of Wilcoxon rank sum tests with Bonferroni correction for Naive Bayes and the F1 metric</figDesc><table><row><cell></cell><cell>SpRST (4)</cell><cell>SpRST (5)</cell><cell>SpRST (8)</cell><cell>SpRST (10)</cell></row><row><cell>SpRST (5)</cell><cell>6e-0 7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (8)</cell><cell>&lt; 2e-16</cell><cell>1e-14</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (10)</cell><cell>&lt; 2e-16</cell><cell>&lt; 2e-16</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>Original (-)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0074</cell><cell>0.0562</cell></row><row><cell>Cfs (Greedy)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0270</cell><cell>1.0000</cell></row><row><cell>Chi squared (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (4117)</cell><cell>0.5275</cell><cell>0.0032</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (5566)</cell><cell>0.0018</cell><cell>0.1024</cell><cell>1.0000</cell><cell>0.5425</cell></row><row><cell>Chi squared (6171)</cell><cell>0.0018</cell><cell>0.0019</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (t = 0)</cell><cell>1.0000</cell><cell>1.0000</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Consistency (Greedy)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Correlation (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Correlation (4117)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Correlation (5566)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0021</cell><cell>0.0033</cell></row><row><cell>Correlation (6171)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0020</cell></row><row><cell>Correlation (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0019</cell><cell>0.0022</cell></row><row><cell>CV (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (4117)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (5566)</cell><cell>0.0024</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (6171)</cell><cell>0.0399</cell><cell>0.0019</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Gain ratio (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Gain ratio (4117)</cell><cell>0.5275</cell><cell>0.0032</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Gain ratio (5566)</cell><cell>0.0018</cell><cell>0.1024</cell><cell>1.0000</cell><cell>0.5425</cell></row><row><cell>Gain ratio (6171)</cell><cell>0.0018</cell><cell>0.0019</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Gain ratio (t = 0)</cell><cell>1.0000</cell><cell>1.0000</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Info gain (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Info gain (4117)</cell><cell>0.5275</cell><cell>0.0032</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Info gain (5566)</cell><cell>0.0018</cell><cell>0.1024</cell><cell>1.0000</cell><cell>0.5425</cell></row><row><cell>Info gain (6171)</cell><cell>0.0018</cell><cell>0.0019</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Info gain (t = 0)</cell><cell>1.0000</cell><cell>1.0000</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>ReliefF (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.8417</cell><cell>1.0000</cell></row><row><cell>ReliefF (4117)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>ReliefF (5566)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0058</cell><cell>0.0326</cell></row><row><cell>ReliefF (6171)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0033</cell><cell>0.0094</cell></row><row><cell>ReliefF (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0058</cell><cell>0.0230</cell></row><row><cell>Significance (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Significance (4117)</cell><cell>0.5275</cell><cell>0.0032</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Significance (5566)</cell><cell>0.0018</cell><cell>0.1024</cell><cell>1.0000</cell><cell>0.5425</cell></row><row><cell>Significance (6171)</cell><cell>0.0018</cell><cell>0.0019</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Significance (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0023</cell><cell>0.0033</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18</head><label>18</label><figDesc>Results of Wilcoxon rank sum tests with Bonferroni correction for random forest and the accuracy metric</figDesc><table><row><cell></cell><cell>SpRST (4)</cell><cell>SpRST (5)</cell><cell>SpRST (8)</cell><cell>SpRST (10)</cell></row><row><cell>SpRST (05)</cell><cell>2.2e-0 7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (08)</cell><cell>&lt; 2e-16</cell><cell>2.5e-09</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (10)</cell><cell>&lt; 2e-16</cell><cell>4.1e-10</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>Original (-)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.1252</cell><cell>0.5138</cell></row><row><cell>Cfs (Greedy)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (3205)</cell><cell>0.0061</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (4117)</cell><cell>0.0020</cell><cell>0.0897</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (5566)</cell><cell>0.0018</cell><cell>0.0024</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (6171)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0113</cell><cell>0.0216</cell></row><row><cell>Chi squared (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Consistency (Greedy)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Correlation (3205)</cell><cell>0.0022</cell><cell>0.0616</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Correlation (4117)</cell><cell>0.0018</cell><cell>0.0058</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Correlation (5566)</cell><cell>0.0018</cell><cell>0.0024</cell><cell>0.4050</cell><cell>1.0000</cell></row><row><cell>Correlation (6171)</cell><cell>0.0018</cell><cell>0.0020</cell><cell>0.2470</cell><cell>0.7423</cell></row><row><cell>Correlation (t = 0)</cell><cell>0.0018</cell><cell>0.0031</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>CV (3205)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (4117)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (5566)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (6171)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>CV (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Gain ratio (3205)</cell><cell>0.0100</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Gain ratio (4117)</cell><cell>0.0024</cell><cell>0.1245</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Gain ratio (5566)</cell><cell>0.0018</cell><cell>0.0022</cell><cell>0.3406</cell><cell>1.0000</cell></row><row><cell>Gain ratio (6171)</cell><cell>0.0018</cell><cell>0.0020</cell><cell>0.0361</cell><cell>0.0803</cell></row><row><cell>Gain ratio (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19</head><label>19</label><figDesc>Results of Wilcoxon rank sum tests with Bonferroni correction for Random forest and the recall metric</figDesc><table><row><cell></cell><cell>SpRST (4)</cell><cell>SpRST (5)</cell><cell>SpRST (8)</cell><cell>SpRST (10)</cell></row><row><cell>SpRST (05)</cell><cell>2.2e-07</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (08)</cell><cell>&lt; 2e-16</cell><cell>2.5e-09</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (10)</cell><cell>&lt; 2e-16</cell><cell>4.1e-10</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>Original (-)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.1252</cell><cell>0.5138</cell></row><row><cell>Cfs (Greedy)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (3205)</cell><cell>0.0061</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row><row><cell>Chi squared (4117)</cell><cell>0.0020</cell><cell>0.0897</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (5566)</cell><cell>0.0018</cell><cell>0.0024</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (6171)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0113</cell><cell>0.0216</cell></row><row><cell>Chi squared (t = 0)</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell><cell>0.0018</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 20</head><label>20</label><figDesc>Results of Wilcoxon rank sum tests with Bonferroni correction for random forest and the precision metric</figDesc><table><row><cell></cell><cell>SpRST (4)</cell><cell>SpRST (5)</cell><cell>SpRST (8)</cell><cell>SpRST (10)</cell></row><row><cell>SpRST (05)</cell><cell>1.1e-0 7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (08)</cell><cell>&lt; 2e-16</cell><cell>1.1e-09</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (10)</cell><cell>&lt; 2e-16</cell><cell>8.5e-11</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>Original (-)</cell><cell>0.0019</cell><cell>0.0034</cell><cell>0.3327</cell><cell>1.0000</cell></row><row><cell>Cfs (Greedy)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Chi squared (3205)</cell><cell>0.0031</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Chi squared (4117)</cell><cell>0.0029</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (5566)</cell><cell>0.0019</cell><cell>0.0110</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (6171)</cell><cell>0.0019</cell><cell>0.0024</cell><cell>0.0289</cell><cell>0.1315</cell></row><row><cell>Chi squared (t = 0)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Consistency (Greedy)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Correlation (3205)</cell><cell>0.0024</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Correlation (4117)</cell><cell>0.0019</cell><cell>0.0094</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Correlation (5566)</cell><cell>0.0019</cell><cell>0.0034</cell><cell>0.7525</cell><cell>1.0000</cell></row><row><cell>Correlation (6171)</cell><cell>0.0019</cell><cell>0.0029</cell><cell>0.0783</cell><cell>1.0000</cell></row><row><cell>Correlation (t = 0)</cell><cell>0.0019</cell><cell>0.0052</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>CV (3205)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (4117)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (5566)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (6171)</cell><cell>0.0079</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (t = 0)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Gain ratio (3205)</cell><cell>0.0034</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Gain ratio (4117)</cell><cell>0.0024</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Gain ratio (5566)</cell><cell>0.0019</cell><cell>0.0102</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Gain ratio (6171)</cell><cell>0.0019</cell><cell>0.0031</cell><cell>0.1415</cell><cell>0.4696</cell></row><row><cell>Gain ratio (t = 0)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Info gain (3205)</cell><cell>0.0067</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Info gain (4117)</cell><cell>0.0024</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Info gain (5566)</cell><cell>0.0019</cell><cell>0.0067</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Info gain (6171)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0247</cell></row><row><cell>Info gain (t = 0)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>ReliefF (3205)</cell><cell>0.0022</cell><cell>0.2696</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>ReliefF (4117)</cell><cell>0.0019</cell><cell>0.0120</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>ReliefF (5566)</cell><cell>0.0019</cell><cell>0.0086</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>ReliefF (6171)</cell><cell>0.0019</cell><cell>0.0057</cell><cell>0.5027</cell><cell>1.0000</cell></row><row><cell>ReliefF (t = 0)</cell><cell>0.0019</cell><cell>0.0022</cell><cell>0.0073</cell><cell>0.1223</cell></row><row><cell>Significance (3205)</cell><cell>0.0022</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Significance (4117)</cell><cell>0.0022</cell><cell>0.2696</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Significance (5566)</cell><cell>0.0019</cell><cell>0.0031</cell><cell>0.1636</cell><cell>1.0000</cell></row><row><cell>Significance (6171)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Significance (t = 0)</cell><cell>0.0019</cell><cell>0.0153</cell><cell>1.0000</cell><cell>1.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 21</head><label>21</label><figDesc>Results of Wilcoxon rank sum tests with Bonferroni correction for random forest and the F1 metric</figDesc><table><row><cell></cell><cell>SpRST (4)</cell><cell>SpRST (5)</cell><cell>SpRST (8)</cell><cell>SpRST (10)</cell></row><row><cell>SpRST (05)</cell><cell>9.7e-0 7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (08)</cell><cell>&lt; 2e-16</cell><cell>9.0e-10</cell><cell>-</cell><cell>-</cell></row><row><cell>SpRST (10)</cell><cell>&lt; 2e-16</cell><cell>2.6e-10</cell><cell>1.0000</cell><cell>-</cell></row><row><cell>Original (-)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.1415</cell><cell>0.9173</cell></row><row><cell>Cfs (Greedy)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Chi squared (3205)</cell><cell>0.0048</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Chi squared (4117)</cell><cell>0.0031</cell><cell>0.8589</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (5566)</cell><cell>0.0019</cell><cell>0.0026</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Chi squared (6171)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0228</cell><cell>0.0980</cell></row><row><cell>Chi squared (t = 0)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Consistency (Greedy)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>Correlation (3205)</cell><cell>0.0022</cell><cell>0.0460</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Correlation (4117)</cell><cell>0.0019</cell><cell>0.0052</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>Correlation (5566)</cell><cell>0.0019</cell><cell>0.0024</cell><cell>0.4095</cell><cell>1.0000</cell></row><row><cell>Correlation (6171)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.2513</cell><cell>1.0000</cell></row><row><cell>Correlation (t = 0)</cell><cell>0.0019</cell><cell>0.0031</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell>CV (3205)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (4117)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (5566)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (6171)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row><row><cell>CV (t = 0)</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0019</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://spark.apache.org/docs/2.2.0/ml-features.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/triguero/MR-EFS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/sramirez/spark-infotheoretic-feature-selection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/sramirez/fast-mRMR.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://hadoop.apache.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://storm.apache.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://samza.apache.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://spark.apache.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://flink.apache.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://www.digitalocean.com/community/tutorials/hadoop-storm-samza-spark-and-flink-big-dataframeworks-compared.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>https://spark.apache.org/docs/0.7.3/api/core/spark/PairRDDFunctions.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>https://spark.apache.org/docs/latest/rdd-programming-guide.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_12"><p>https://haifengl.github.io/smile/feature.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_13"><p>https://www.cs.waikato.ac.nz/~ml/weka.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work is part of a project that has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Skłodowska-Curie Grant</rs> Agreement No. <rs type="grantNumber">702527</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4hus56Q">
					<idno type="grant-number">702527</idno>
					<orgName type="grant-name">Marie Skłodowska-Curie Grant</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Detailed classification and runtime results</head><p>See <ref type="bibr">Tables 11,</ref><ref type="bibr" target="#b18">12</ref> and 13.</p><p>123   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">InfoGainAttributeEval-threshold0 InfoGainAttributeEval-num3205 InfoGainAttributeEval-num4117 InfoGainAttributeEval-num5566 InfoGainAttributeEval-num6171</title>
		<idno>CorrelationAttributeEval-num3205 CorrelationAttributeEval-num4117 CorrelationAttributeEval-num5566 CorrelationAttributeEval-num6171 GainRatioAttributeEval-threshold0 GainRatioAttributeEval-num3205 GainRatioAttributeEval-num4117 GainRatioAttributeEval-num5566 GainRatioAttributeEval-num6171</idno>
	</analytic>
	<monogr>
		<title level="j">Method Accuracy (a) Accuracy</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">00 0</biblScope>
			<biblScope unit="page" from="50" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">ConsistencySubsetEval-BestFirstGreedy CorrelationAttributeEval-threshold0</note>
	<note>00 Original CVAttributeEval-threshold0</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">InfoGainAttributeEval-threshold0 InfoGainAttributeEval-num3205 InfoGainAttributeEval-num4117 InfoGainAttributeEval-num5566 InfoGainAttributeEval-num6171</title>
		<idno>CorrelationAttributeEval-num3205 CorrelationAttributeEval-num4117 CorrelationAttributeEval-num5566 CorrelationAttributeEval-num6171 GainRatioAttributeEval-threshold0 GainRatioAttributeEval-num3205 GainRatioAttributeEval-num4117 GainRatioAttributeEval-num5566 GainRatioAttributeEval-num6171</idno>
	</analytic>
	<monogr>
		<title level="j">Method Recall (b) Recall 0</title>
		<imprint>
			<biblScope unit="issue">00 0</biblScope>
			<biblScope unit="page" from="50" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">ConsistencySubsetEval-BestFirstGreedy CorrelationAttributeEval-threshold0</note>
	<note>00 Original CVAttributeEval-threshold0</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">InfoGainAttributeEval-threshold0 InfoGainAttributeEval-num3205 InfoGainAttributeEval-num4117 InfoGainAttributeEval-num5566 InfoGainAttributeEval-num6171</title>
		<idno>CorrelationAttributeEval-num3205 CorrelationAttributeEval-num4117 CorrelationAttributeEval-num5566 CorrelationAttributeEval-num6171 GainRatioAttributeEval-threshold0 GainRatioAttributeEval-num3205 GainRatioAttributeEval-num4117 GainRatioAttributeEval-num5566 GainRatioAttributeEval-num6171</idno>
	</analytic>
	<monogr>
		<title level="j">Method Precision (c) Precision</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">00 0</biblScope>
			<biblScope unit="page" from="50" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">ConsistencySubsetEval-BestFirstGreedy CorrelationAttributeEval-threshold0</note>
	<note>00 Original CVAttributeEval-threshold0</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">InfoGainAttributeEval-threshold0 InfoGainAttributeEval-num3205 InfoGainAttributeEval-num4117 InfoGainAttributeEval-num5566 InfoGainAttributeEval-num6171</title>
		<idno>CorrelationAttributeEval-num3205 CorrelationAttributeEval-num4117 CorrelationAttributeEval-num5566 CorrelationAttributeEval-num6171 GainRatioAttributeEval-threshold0 GainRatioAttributeEval-num3205 GainRatioAttributeEval-num4117 GainRatioAttributeEval-num5566 GainRatioAttributeEval-num6171</idno>
	</analytic>
	<monogr>
		<title level="j">Method Accuracy (a) Accuracy</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">00 0</biblScope>
			<biblScope unit="page" from="50" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">ConsistencySubsetEval-BestFirstGreedy CorrelationAttributeEval-threshold0</note>
	<note>00 Original CVAttributeEval-threshold0</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">InfoGainAttributeEval-threshold0 InfoGainAttributeEval-num3205 InfoGainAttributeEval-num4117 InfoGainAttributeEval-num5566 InfoGainAttributeEval-num6171</title>
		<idno>CorrelationAttributeEval-num3205 CorrelationAttributeEval-num4117 CorrelationAttributeEval-num5566 CorrelationAttributeEval-num6171 GainRatioAttributeEval-threshold0 GainRatioAttributeEval-num3205 GainRatioAttributeEval-num4117 GainRatioAttributeEval-num5566 GainRatioAttributeEval-num6171</idno>
	</analytic>
	<monogr>
		<title level="j">Method Recall (b) Recall 0</title>
		<imprint>
			<biblScope unit="issue">00 0</biblScope>
			<biblScope unit="page" from="50" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">ConsistencySubsetEval-BestFirstGreedy CorrelationAttributeEval-threshold0</note>
	<note>00 Original CVAttributeEval-threshold0</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">InfoGainAttributeEval-threshold0 InfoGainAttributeEval-num3205 InfoGainAttributeEval-num4117 InfoGainAttributeEval-num5566 InfoGainAttributeEval-num6171</title>
		<idno>CorrelationAttributeEval-num3205 CorrelationAttributeEval-num4117 CorrelationAttributeEval-num5566 CorrelationAttributeEval-num6171 GainRatioAttributeEval-threshold0 GainRatioAttributeEval-num3205 GainRatioAttributeEval-num4117 GainRatioAttributeEval-num5566 GainRatioAttributeEval-num6171</idno>
	</analytic>
	<monogr>
		<title level="j">Method Precision (c) Precision</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">00 0</biblScope>
			<biblScope unit="page" from="50" to="50" />
		</imprint>
	</monogr>
	<note type="report_type">ConsistencySubsetEval-BestFirstGreedy CorrelationAttributeEval-threshold0</note>
	<note>00 Original CVAttributeEval-threshold0</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<idno>ReliefFAttributeEval-num4117 ReliefFAttributeEval-num5566</idno>
		<title level="m">SymmetricalUncertAttributeEval-num6171 Method Runtime (in seconds) 0 250 500 750 1000 Original SpRST-P1000 SpRST-P1250 SpRST-P2000 SpRST-P2500 Method Runtime (in seconds) Original CVAttributeEval-threshold0 CVAttributeEval-num3205 CVAttributeEval-num4117 CVAttributeEval-num5566 CVAttributeEval-num6171 CfsSubsetEval-BestFirstGreedy ChiSquaredAttributeEval-threshold0</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="50" to="100" />
		</imprint>
	</monogr>
	<note type="report_type">GainRatioAttributeEval-threshold0</note>
	<note>InfoGainAttributeEval-num6171 Method Runtime (in seconds) Original ReliefFAttributeEval-threshold0 ReliefFAttributeEval-num3205</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data mining methods for omics and knowledge of crude medicinal plants toward big data biology</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Afendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Darusman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kibinge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Horai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Altaf-Ul-Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Struct Biotechnol J</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text feature selection using ant colony optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Aghdam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghasem-Aghaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Basiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6843" to="6853" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhanced feature selection for biomarker discovery in LC-MS data using GP</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary computation (CEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="584" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Newman</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/~mlearn/MLRepository.html" />
		<title level="m">UCI machine learning repository</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating sustainability into supplier selection with grey system and rough set methodologies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Produ Econ</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="252" to="264" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the scalability of feature selection methods on high-dimensional data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bolón-Canedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rego-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peteiro-Barral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonso-Betanzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guijarro-Berdiñas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sánchez-Maroño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Inf Syst</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="395" to="442" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Big data: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Netw Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="209" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A distributed rough set theory based algorithm for an efficient big data pre-processing under the spark framework</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Dagdia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zarges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lebbah</surname></persName>
		</author>
		<idno type="DOI">10.1109/BigData.2017.8258008</idno>
		<ptr target="https://doi.org/10.1109/BigData.2017.8258008" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on big data, BigData 2017</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-11">2017. December 11-14, 2017</date>
			<biblScope unit="page" from="911" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature selection for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell Data Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MapReduce: a flexible data processing tool</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rough set data analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Düntsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gediga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encycl Comput Sci Technol</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="281" to="301" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards scalable rough set based attribute subset selection for intrusion detection using parallel genetic algorithm in MapReduce</title>
		<author>
			<persName><forename type="first">Esm</forename><surname>El-Alfy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Alshammari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simul Model Pract Theory</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mining big data: current status, and forecast to the future</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM sIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Big data with cloud computing: an insight on the computing environment, MapReduce, and programming frameworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bawakid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benítez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip Rev Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="409" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-adaptive differential evolution for feature selection in hyperspectral image data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1969" to="1977" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data mining and rough set theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Grzymala-Busse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ziarko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="108" to="109" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003-03">2003. Mar</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rough sets in distributed decision information systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="13" to="22" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Irrelevant features and the subset selection problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning: proceedings of the eleventh international conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Discovering knowledge in data: an introduction to data mining</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Larose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised rough set classification using GAs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lingras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Intell Inf Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="228" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rough set clustering for web mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lingras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FUZZ-IEEE&apos;02. Proceedings of the 2002 IEEE international conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1039" to="1044" />
		</imprint>
	</monogr>
	<note>Fuzzy systems</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature selection: an ever evolving frontier in data mining</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Setiono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Feature selection in data mining</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Manipulating data and dimension reduction methods: feature selection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational complexity</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1790" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rough sets: theoretical aspects of reasoning about data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pawlak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">9</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rudiments of rough sets</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pawlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Sci</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evolutionary feature selection for big data classification: a mapreduce approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramírez-Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1155/2015/246139</idno>
		<ptr target="https://doi.org/10.1155/2015/246139" />
	</analytic>
	<monogr>
		<title level="j">Math Probl Eng</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Positive approximation: an accelerator for attribute reduction in rough set theory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="597" to="618" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Local rough set: a solution to rough data analysis in big data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Approx Reason</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="38" to="63" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of large scale data management approaches in cloud environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alomari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun Surv Tutor</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="336" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable time series classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable density-based clustering with quality guarantees using random projections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="972" to="1005" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large scale distributed data science using apache spark</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2323" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">MPI-the complete reference: the MPI core</title>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A distributed approach for graph mining in massive networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Talukder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mini Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1024" to="1052" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dimensionality reduction based on rough set theory: a review</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thangavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pethalakshmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discovering outlying aspects in large datasets</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1520" to="1555" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data mining with big data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="107" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A fast parallel clustering algorithm for large spatial databases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jäger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High performance data mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="263" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Classification of high-dimensional evolving data streams via a resource-efficient online ensemble</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1242" to="1265" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiple bayesian discriminant functions for highdimensional massive data classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="465" to="501" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Publisher&apos;s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">and data analytics, evolutionary algorithms and artificial immune systems</title>
		<author>
			<persName><forename type="first">Zaineb</forename><surname>Chelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dagdia</forename></persName>
		</author>
		<idno>MSCA-IF-2015- EF, Project No. 702527</idno>
	</analytic>
	<monogr>
		<title level="m">the ACM-W Award, the Marie Sklodowska Curie Individual European Fellowship and the Best Reviewer Award (iCDEc 2018)</title>
		<meeting><address><addrLine>Tunisia; UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010 and 2014. 03/2017-02/2019</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Research in Computer Science and Automation (Inria) ; ISG-Campus) ; Aberystwyth University</orgName>
		</respStmt>
	</monogr>
	<note>Research and Development Technical Project Manager. big data, and uncertainty theories. She has a strong publication record. She was awarded the Young Researcher First Price (IEEE EHB&apos;2013. She also acts as a Marie Sklodowska Curie Ambassador, selected as a Female Scientist Role Model, and selected to be among the 200 Heidelberg-Laureate-Forum (HLF) most qualified young researchers</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christine</forename><surname>Zarges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Wales, UK; Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science at Aberystwyth University ; She graduated with a PhD in Computer Science from TU Dortmund</orgName>
		</respStmt>
	</monogr>
	<note>Her current main research interests include machine learning, optimization and heuristic search methods</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
